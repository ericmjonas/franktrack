Automatically generated by Mendeley 1.6
Any changes to this file will be lost if it is regenerated by Mendeley.

@misc{picloud,
title = {{PiCloud : Cloud Computing Simplified}},
url = {http://www.picloud.com}
}
@article{Liang2007,
address = {New York, New York, USA},
author = {Liang, Percy and Jordan, Michael I. and Taskar, Ben},
doi = {10.1145/1273496.1273565},
file = {:Users/jonas/Documents/Mendeley Desktop/Liang, Jordan, Taskar - 2007 - A permutation-augmented sampler for DP mixture models.pdf:pdf},
isbn = {9781595937933},
journal = {Proceedings of the 24th international conference on Machine learning - ICML '07},
keywords = {augmentation,dpmm,mcmc,splitmerge},
mendeley-tags = {augmentation,dpmm,mcmc,splitmerge},
pages = {545--552},
publisher = {ACM Press},
title = {{A permutation-augmented sampler for DP mixture models}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273565},
year = {2007}
}
@article{Jain2004,
abstract = {This article proposes a split-mergeMarkov chain algorithmto address the problemof inef cient sampling for conjugate Dirichlet process mixture models. Traditional Markov chain Monte Carlo methods for Bayesian mixture models, such as Gibbs sampling, can become trapped in isolated modes corresponding to an inappropriate clustering of data points. This article describes a Metropolis-Hastingsprocedure that can escape such local modesby splittingormergingmixturecomponents.Our algorithmemploysa newtechnique in which an appropriate proposal for splitting or merging components is obtained by using a restrictedGibbs sampling scan.We demonstrateempirically that ourmethod outperforms the Gibbs sampler in situationswhere two or more components are similar in structure.},
author = {Jain, Sonia and Neal, Radford M},
doi = {10.1198/1061860043001},
file = {:Users/jonas/Documents/Mendeley Desktop/Jain, Neal - 2004 - A Split-Merge Markov chain Monte Carlo Procedure for the Dirichlet Process Mixture Model(2).pdf:pdf},
isbn = {1061860043001},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {conjugate,crp,dpmm,gibbs,gibbs sampler,latent class analysis,mcmc,metropolis,metropolis-hastings algorithm},
mendeley-tags = {conjugate,crp,dpmm,gibbs,mcmc},
month = mar,
number = {1},
pages = {158--182},
title = {{A Split-Merge Markov chain Monte Carlo Procedure for the Dirichlet Process Mixture Model}},
url = {http://pubs.amstat.org/doi/abs/10.1198/1061860043001},
volume = {13},
year = {2004}
}
@misc{github,
title = {{Github}},
url = {http://www.github.com}
}
@article{Bejjanki2011a,
abstract = {Extensive training on simple tasks such as fine orientation discrimination results in large improvements in performance, a form of learning known as perceptual learning. Previous models have argued that perceptual learning is due to either sharpening and amplification of tuning curves in early visual areas or to improved probabilistic inference in later visual areas (at the decision stage). However, early theories are inconsistent with the conclusions of psychophysical experiments manipulating external noise, whereas late theories cannot explain the changes in neural responses that have been reported in cortical areas V1 and V4. Here we show that we can capture both the neurophysiological and behavioral aspects of perceptual learning by altering only the feedforward connectivity in a recurrent network of spiking neurons so as to improve probabilistic inference in early visual areas. The resulting network shows modest changes in tuning curves, in line with neurophysiological reports, along with a marked reduction in the amplitude of pairwise noise correlations.},
author = {Bejjanki, Vikranth R and Beck, Jeffrey M and Lu, Zhong-Lin and Pouget, Alexandre},
doi = {10.1038/nn.2796},
file = {:Users/jonas/Documents/Mendeley Desktop/Bejjanki et al. - 2011 - Perceptual learning as improved probabilistic inference in early sensory areas.pdf:pdf},
issn = {1546-1726},
journal = {Nature neuroscience},
keywords = {Animals,Cerebral Cortex,Cerebral Cortex: physiology,Computer Simulation,Discrimination (Psychology),Discrimination (Psychology): physiology,Humans,Learning,Learning: physiology,Models, Neurological,Neural Networks (Computer),Neural Pathways,Neural Pathways: physiology,Neurons,Neurons: physiology,Noise,Orientation,Orientation: physiology,Probability,Psychophysics,Thalamus,Thalamus: physiology,Visual Cortex,Visual Cortex: cytology,Visual Cortex: physiology,Visual Fields,Visual Fields: physiology,Visual Perception,Visual Perception: physiology},
month = may,
number = {5},
pages = {642--8},
pmid = {21460833},
publisher = {Nature Publishing Group},
title = {{Perceptual learning as improved probabilistic inference in early sensory areas.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21460833},
volume = {14},
year = {2011}
}
@article{Levin2009,
author = {Levin, a. and Weiss, Y. and Durand, F. and Freeman, W.T.},
doi = {10.1109/CVPR.2009.5206815},
file = {:Users/jonas/Documents/Mendeley Desktop/deconvLevinEtalCVPR09.pdf:pdf},
isbn = {978-1-4244-3992-8},
journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
keywords = {deconvolution},
mendeley-tags = {deconvolution},
month = jun,
number = {2},
pages = {1964--1971},
publisher = {Ieee},
title = {{Understanding and evaluating blind deconvolution algorithms}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206815},
year = {2009}
}
@article{Johansen2012,
author = {Johansen, Adam M and Whiteley, Nick and Doucet, Arnaud},
file = {:Users/jonas/Documents/Mendeley Desktop/20120711.pdf:pdf},
keywords = {RBPF},
mendeley-tags = {RBPF},
pages = {1--17},
title = {{Exact Approximation of Rao-Blackwellized Particle Filters [TALK]}},
year = {2012}
}
@article{VanGael2008,
address = {New York, New York, USA},
author = {{Van Gael}, Jurgen and Saatci, Yunus and Teh, Yee Whye and Ghahramani, Zoubin},
doi = {10.1145/1390156.1390293},
file = {:Users/jonas/Documents/Mendeley Desktop/Van Gael et al. - 2008 - Beam sampling for the infinite hidden Markov model.pdf:pdf},
isbn = {9781605582054},
journal = {Proceedings of the 25th international conference on Machine learning - ICML '08},
keywords = {beam,hdphmm,mcmc},
mendeley-tags = {beam,hdphmm,mcmc},
pages = {1088--1095},
publisher = {ACM Press},
title = {{Beam sampling for the infinite hidden Markov model}},
url = {http://portal.acm.org/citation.cfm?doid=1390156.1390293},
year = {2008}
}
@article{Jain2004a,
author = {Jain, Sonia and Neal, R.M.},
file = {:Users/jonas/Documents/Mendeley Desktop/Jain, Neal - 2004 - A split-merge Markov chain Monte Carlo procedure for the Dirichlet process mixture model.pdf:pdf},
institution = {Department Of Statistics, University of Toronto},
journal = {Journal of Computational and Graphical Statistics},
keywords = {conjugacy,dpmm,mcmc,splitmerge},
mendeley-tags = {conjugacy,dpmm,mcmc,splitmerge},
number = {1},
pages = {158--182},
publisher = {ASA},
title = {{A split-merge Markov chain Monte Carlo procedure for the Dirichlet process mixture model}},
url = {http://pubs.amstat.org/doi/pdf/10.1198/1061860043001},
volume = {13},
year = {2004}
}
@article{Doucet2006,
annote = {Canonical Block Sampling Paper

        
Seriously, just compares ESS to show it's better. Also uses ridiculously-specific proposal distributions, such as the EKF (which, ok, fine, maybe it's finally time to learn). 

        

      },
author = {Doucet, Arnaud and Briers, Mark and S\'{e}n\'{e}cal, St\'{e}phane},
doi = {10.1198/106186006X142744},
file = {:Users/jonas/Documents/Mendeley Desktop/Efficient Block Sampling Strategies for Sequential Monte Carlo Methods.pdf:pdf},
isbn = {106186006X},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {block sequential monte carlo,carlo,importance sampling,markov chain monte,optimal filtering,particle filtering,state-space models},
month = sep,
number = {3},
pages = {693--711},
title = {{Efficient Block Sampling Strategies for Sequential Monte Carlo Methods}},
url = {http://www.tandfonline.com/doi/abs/10.1198/106186006X142744},
volume = {15},
year = {2006}
}
@article{Barber2010a,
author = {Barber, David and Cemgil, A.},
doi = {10.1109/MSP.2010.938028},
file = {:Users/jonas/Documents/Mendeley Desktop/Barber, Cemgil - 2010 - Graphical Models for Time-Series.pdf:pdf},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
keywords = {graphical models,time series},
mendeley-tags = {graphical models,time series},
month = nov,
number = {November},
pages = {18--28},
title = {{Graphical Models for Time-Series}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5563116},
year = {2010}
}
@article{Fresia2010a,
author = {Fresia, Maria and Perez-Cruz, Fernando and Poor, H. and Verdu, Sergio},
doi = {10.1109/MSP.2010.938080},
file = {:Users/jonas/Documents/Mendeley Desktop/Fresia et al. - 2010 - Joint Source and Channel Coding.pdf:pdf},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
keywords = {coding,graphical models},
mendeley-tags = {coding,graphical models},
month = nov,
number = {November},
pages = {104--113},
title = {{Joint Source and Channel Coding}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5563107},
year = {2010}
}
@techreport{Walsh2004,
abstract = {Lecture notes for EEB 581},
author = {Walsh, B},
booktitle = {Notes},
file = {:Users/jonas/Documents/Mendeley Desktop/Walsh - 2004 - Markov Chain Monte Carlo and Gibbs Sampling.pdf:pdf},
keywords = {mcmc,tutorial},
mendeley-tags = {mcmc,tutorial},
number = {April},
title = {{Markov Chain Monte Carlo and Gibbs Sampling}},
year = {2004}
}
@article{Maceachern2007,
author = {Maceachern, Steven N},
doi = {10.1214/07-BA219B},
file = {:Users/jonas/Documents/Mendeley Desktop/Maceachern - 2007 - Comment on Article by Jain and Neal.pdf:pdf},
issn = {1931-6690},
journal = {Bayesian Analysis},
keywords = {conjugate,dpmm,mcmc,nonconjugate,splitmerge},
mendeley-tags = {conjugate,dpmm,mcmc,nonconjugate,splitmerge},
number = {3},
pages = {479--482},
title = {{Comment on Article by Jain and Neal}},
url = {http://ba.stat.cmu.edu/journal/2007/vol02/issue03/robert.pdf},
volume = {2},
year = {2007}
}
@article{Germain2006,
address = {New York, New York, USA},
author = {Germain, Guillaume},
doi = {10.1145/1159789.1159795},
file = {:Users/jonas/Documents/Mendeley Desktop//Germain - 2006 - Concurrency oriented programming in termite scheme.pdf:pdf;:Users/jonas/Documents/Mendeley Desktop//Germain - 2006 - Concurrency oriented programming in termite scheme(2).pdf:pdf;:Users/jonas/Documents/Mendeley Desktop//Germain - 2006 - Concurrency oriented programming in termite scheme(3).pdf:pdf},
isbn = {1595934901},
journal = {Proceedings of the 2006 ACM SIGPLAN workshop on Erlang - ERLANG '06},
keywords = {contin-,distributed computing,erlang,lisp,parallel,scheme},
mendeley-tags = {parallel,scheme},
pages = {20},
publisher = {ACM Press},
title = {{Concurrency oriented programming in termite scheme}},
url = {http://portal.acm.org/citation.cfm?doid=1159789.1159795},
year = {2006}
}
@article{Jain,
abstract = {The inferential problem of associating data to mixture components is dif- ficult when components are nearby or overlapping. We introduce a new split-merge Markov chain Monte Carlo technique that efficiently classifies observations by splitting and merging mixture components of a nonconjugate Dirichlet process mixture model. Our method, which is a Metropolis-Hastings procedure with split-merge proposals, sam- ples clusters of observations simultaneously rather than incrementally assigning observa- tions to mixture components. Split-merge moves are produced by exploiting properties of a restricted Gibbs sampling scan. A simulation study compares the new split-merge technique to a nonconjugate version of Gibbs sampling and an incremental Metropolis- Hastings technique. The results demonstrate the improved performance of the new sampler.},
author = {Jain, Sonia and Neal, R.M.},
file = {:Users/jonas/Documents/Mendeley Desktop/Jain, Neal - Unknown - Splitting and merging components of a nonconjugate Dirichlet process mixture model.pdf:pdf},
journal = {Bayesian Analysis},
keywords = {Bayesian model,Markov chain Monte Carlo,dpmm,mcmc,nonconjugate,nonconjugate prior,split-merge moves,splitmerge},
mendeley-tags = {dpmm,mcmc,nonconjugate,splitmerge},
number = {3},
pages = {445--472},
title = {{Splitting and merging components of a nonconjugate Dirichlet process mixture model}},
url = {http://ba.stat.cmu.edu/journal/2007/vol02/issue03/jain.pdf},
volume = {2}
}
@article{Lefebvre2004,
author = {Lefebvre, Tine and Bruyninckx, Herman and {De Schutter}, Joris},
doi = {10.1080/00207170410001704998},
file = {:Users/jonas/Documents/Mendeley Desktop/00207170410001704998.pdf:pdf},
issn = {0020-7179},
journal = {International Journal of Control},
keywords = {ekf,kalman filter,tutorial,ukf},
mendeley-tags = {ekf,kalman filter,tutorial,ukf},
month = may,
number = {7},
pages = {639--653},
title = {{Kalman filters for non-linear systems: a comparison of performance}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00207170410001704998},
volume = {77},
year = {2004}
}
@article{Geisler2002a,
abstract = {In recent years, there has been much interest in characterizing statistical properties of natural stimuli in order to better understand the design of perceptual systems. A fruitful approach has been to compare the processing of natural stimuli in real perceptual systems with that of ideal observers derived within the framework of Bayesian statistical decision theory. While this form of optimization theory has provided a deeper understanding of the information contained in natural stimuli as well as of the computational principles employed in perceptual systems, it does not directly consider the process of natural selection, which is ultimately responsible for design. Here we propose a formal framework for analysing how the statistics of natural stimuli and the process of natural selection interact to determine the design of perceptual systems. The framework consists of two complementary components. The first is a maximum fitness ideal observer, a standard Bayesian ideal observer with a utility function appropriate for natural selection. The second component is a formal version of natural selection based upon Bayesian statistical decision theory. Maximum fitness ideal observers and Bayesian natural selection are demonstrated in several examples. We suggest that the Bayesian approach is appropriate not only for the study of perceptual systems but also for the study of many other systems in biology.},
author = {Geisler, Wilson S and Diehl, Randy L},
doi = {10.1098/rstb.2001.1055},
file = {:Users/jonas/Documents/Mendeley Desktop/Geisler, Diehl - 2002 - Bayesian natural selection and the evolution of perceptual systems.pdf:pdf},
issn = {0962-8436},
journal = {Philosophical transactions of the Royal Society of London. Series B, Biological sciences},
keywords = {Alleles,Animals,Bayes Theorem,Biological Evolution,Computer Simulation,Environment,Mutation,Perception,Perception: physiology,Polymorphism, Genetic,Population Dynamics,Predatory Behavior,Probability,Reproduction,Selection, Genetic},
month = apr,
number = {1420},
pages = {419--48},
pmid = {12028784},
title = {{Bayesian natural selection and the evolution of perceptual systems.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1692963\&tool=pmcentrez\&rendertype=abstract},
volume = {357},
year = {2002}
}
@article{Resnik2010,
author = {Resnik, Philip and Hardisty, Eric},
file = {:Users/jonas/Documents/Mendeley Desktop/Resnik, Hardisty - 2010 - Gibbs sampling for the uninitiated.pdf:pdf},
journal = {University of Maryland Computer Science Department; CS-TR},
keywords = {gibbs sampling,mcmc,tutorial},
mendeley-tags = {gibbs sampling,mcmc,tutorial},
pages = {1--23},
publisher = {Citeseer},
title = {{Gibbs sampling for the uninitiated}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.2875\&amp;rep=rep1\&amp;type=pdf},
volume = {4956},
year = {2010}
}
@article{VanGerven2010a,
abstract = {Recent research has shown that reconstruction of perceived images based on hemodynamic response as measured with functional magnetic resonance imaging (fMRI) is starting to become feasible. In this letter, we explore reconstruction based on a learned hierarchy of features by employing a hierarchical generative model that consists of conditional restricted Boltzmann machines. In an unsupervised phase, we learn a hierarchy of features from data, and in a supervised phase, we learn how brain activity predicts the states of those features. Reconstruction is achieved by sampling from the model, conditioned on brain activity. We show that by using the hierarchical generative model, we can obtain good-quality reconstructions of visual images of handwritten digits presented during an fMRI scanning session.},
author = {van Gerven, Marcel a J and de Lange, Floris P and Heskes, Tom},
doi = {10.1162/NECO\_a\_00047},
file = {:Users/jonas/Documents/Mendeley Desktop/van Gerven, de Lange, Heskes - 2010 - Neural decoding with hierarchical generative models.pdf:pdf},
issn = {1530-888X},
journal = {Neural computation},
keywords = {Brain,Brain: physiology,Models, Neurological,Neural Networks (Computer),Visual Perception,Visual Perception: physiology},
month = dec,
number = {12},
pages = {3127--42},
pmid = {20858128},
title = {{Neural decoding with hierarchical generative models.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20858128},
volume = {22},
year = {2010}
}
@article{Kover2010a,
abstract = {Human perception of ambiguous sensory signals is biased by prior experiences. It is not known how such prior information is encoded, retrieved and combined with sensory information by neurons. Previous authors have suggested dynamic encoding mechanisms for prior information, whereby top-down modulation of firing patterns on a trial-by-trial basis creates short-term representations of priors. Although such a mechanism may well account for perceptual bias arising in the short-term, it does not account for the often irreversible and robust changes in perception that result from long-term, developmental experience. Based on the finding that more frequently experienced stimuli gain greater representations in sensory cortices during development, we reasoned that prior information could be stored in the size of cortical sensory representations. For the case of auditory perception, we use a computational model to show that prior information about sound frequency distributions may be stored in the size of primary auditory cortex frequency representations, read-out by elevated baseline activity in all neurons and combined with sensory-evoked activity to generate a percept that conforms to Bayesian integration theory. Our results suggest an alternative neural mechanism for experience-induced long-term perceptual bias in the context of auditory perception. They make the testable prediction that the extent of such perceptual prior bias is modulated by both the degree of cortical reorganization and the magnitude of spontaneous activity in primary auditory cortex. Given that cortical over-representation of frequently experienced stimuli, as well as perceptual bias towards such stimuli is a common phenomenon across sensory modalities, our model may generalize to sensory perception, rather than being specific to auditory perception.},
author = {K\"{o}ver, Hania and Bao, Shaowen},
editor = {Soares, Daphne},
file = {:Users/jonas/Documents/Mendeley Desktop/K\"{o}ver, Bao - 2010 - Cortical Plasticity as a Mechanism for Storing Bayesian Priors in Sensory Perception.pdf:pdf},
institution = {Helen Wills Neuroscience Institute, University of California, Berkeley, California, United States of America.},
journal = {PLoS ONE},
number = {5},
pages = {7},
publisher = {Public Library of Science},
title = {{Cortical Plasticity as a Mechanism for Storing Bayesian Priors in Sensory Perception}},
url = {http://dx.plos.org/10.1371/journal.pone.0010497},
volume = {5},
year = {2010}
}
@article{Chopin2011,
abstract = {We consider the generic problem of performing sequential Bayesian inference in a state-space model with observation process y, state process x and fixed parameter theta. An idealized approach would be to apply the iterated batch importance sampling (IBIS) algorithm of Chopin (2002). This is a sequential Monte Carlo algorithm in the theta-dimension, that samples values of theta, reweights iteratively these values using the likelihood increments p(y\_t|y\_1:t-1, theta), and rejuvenates the theta-particles through a resampling step and a MCMC update step. In state-space models these likelihood increments are intractable in most cases, but they may be unbiasedly estimated by a particle filter in the x-dimension, for any fixed theta. This motivates the SMC\^{}2 algorithm proposed in this article: a sequential Monte Carlo algorithm, defined in the theta-dimension, which propagates and resamples many particle filters in the x-dimension. The filters in the x-dimension are an example of the random weight particle filter as in Fearnhead et al. (2010). On the other hand, the particle Markov chain Monte Carlo (PMCMC) framework developed in Andrieu et al. (2010) allows us to design appropriate MCMC rejuvenation steps. Thus, the theta-particles target the correct posterior distribution at each iteration t, despite the intractability of the likelihood increments. We explore the applicability of our algorithm in both sequential and non-sequential applications and consider various degrees of freedom, as for example increasing dynamically the number of x-particles. We contrast our approach to various competing methods, both conceptually and empirically through a detailed simulation study, included here and in a supplement, and based on particularly challenging examples.},
archivePrefix = {arXiv},
arxivId = {1101.1528},
author = {Chopin, Nicolas and Jacob, Pierre E. and Papaspiliopoulos, Omiros},
eprint = {1101.1528},
file = {:Users/jonas/Documents/Mendeley Desktop/SMC\^{}2- an efficient algorithm for sequential analysis of state-space models.pdf:pdf;:Users/jonas/Documents/Mendeley Desktop/SMC2Supplementpdf.pdf:pdf},
keywords = {carlo,iterated batch importance sampling,particle filtering,particle markov chain monte,sequential monte carlo,state-space models},
month = jan,
number = {1},
pages = {27},
title = {{SMC\^{}2: an efficient algorithm for sequential analysis of state-space models}},
url = {http://arxiv.org/abs/1101.1528},
volume = {0},
year = {2011}
}
@article{Urban2009,
author = {Urban, Gerald a},
doi = {10.1088/0957-0233/20/1/012001},
file = {:Users/jonas/Documents/Mendeley Desktop/Urban - 2009 - Micro- and nanobiosensors—state of the art and trends.pdf:pdf},
issn = {0957-0233},
journal = {Measurement Science and Technology},
keywords = {article are in colour,bioanalytics,biological parameters,biomedical,glucose,lactate,medical monitoring,metabolic parameters,microsystem,nanotechnology,only in the electronic,some figures in this,version},
month = jan,
number = {1},
pages = {012001},
title = {{Micro- and nanobiosensors—state of the art and trends}},
url = {http://stacks.iop.org/0957-0233/20/i=1/a=012001?key=crossref.74ada4421967e337230127f877c98fdb},
volume = {20},
year = {2009}
}
@article{Castro2008,
author = {Castro, R. M. and Haupt, J. and Nowak, R. and Raz, G. M.},
doi = {10.1109/ICASSP.2008.4518814},
file = {:Users/jonas/Documents/Mendeley Desktop/Castro et al. - 2008 - Finding needles in noisy haystacks.pdf:pdf},
isbn = {978-1-4244-1483-3},
issn = {1520-6149},
journal = {2008 IEEE International Conference on Acoustics, Speech and Signal Processing},
month = mar,
number = {1},
pages = {5133--5136},
publisher = {Ieee},
title = {{Finding needles in noisy haystacks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4518814},
year = {2008}
}
@article{Nicholls,
author = {Nicholls, Geoff K},
file = {:Users/jonas/Documents/Mendeley Desktop/Nicholls - Unknown - Bayesian Inference and Markov Chain Monte Carlo by Example.pdf:pdf},
journal = {Inverse Problems},
keywords = {and phrases,and ville kolehmainen for,bayes,bayesian inference,examples,i thank jari kaipio,inviting me to give,lecture notes,mcmc,these lectures},
mendeley-tags = {bayes,mcmc},
title = {{Bayesian Inference and Markov Chain Monte Carlo by Example}}
}
@article{Sidhu1998,
abstract = {A method of management of a dual stenoses affecting the proximal common carotid artery and the internal carotid artery, the tandem lesion, is described in two cases. The combination of a surgical endarterectomy of the internal carotid artery narrowing and percutaneous balloon dilatation of the more proximal common carotid artery narrowing, via the arteriotomy site, with clamping of the internal carotid artery was successfully employed to avoid an extrathoracic bypass procedure.},
author = {Sidhu, P S and Morgan, M B and Walters, H L and Baskerville, P a and Fraser, S C},
file = {:Users/jonas/Documents/Mendeley Desktop/Sidhu et al. - 1998 - Technical report Combined carotid bifurcation endarterectomy and intra-operative transluminal angioplasty of a proximal common carotid artery stenosis an alternative to extrathoracic bypass.pdf:pdf},
issn = {0009-9260},
journal = {Clinical radiology},
keywords = {Aged,Angioplasty, Balloon,Carotid Artery, Common,Carotid Artery, Common: radiography,Carotid Artery, Internal,Carotid Artery, Internal: radiography,Carotid Artery, Internal: surgery,Carotid Stenosis,Carotid Stenosis: radiography,Carotid Stenosis: surgery,Carotid Stenosis: therapy,Combined Modality Therapy,Endarterectomy, Carotid,Endarterectomy, Carotid: methods,Female,Humans,Male},
month = jun,
number = {6},
pages = {444--7},
pmid = {9651061},
title = {{Technical report: Combined carotid bifurcation endarterectomy and intra-operative transluminal angioplasty of a proximal common carotid artery stenosis: an alternative to extrathoracic bypass.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16234653},
volume = {53},
year = {1998}
}
@article{Johnson2009,
archivePrefix = {arXiv},
arxivId = {arXiv:0910.2090v1},
author = {Johnson, W. Evan and Liu, X. Shirley and Liu, Jun S.},
doi = {10.1214/09-AOAS248},
eprint = {arXiv:0910.2090v1},
file = {:Users/jonas/Documents/Mendeley Desktop/Johnson, Liu, Liu - 2009 - Doubly stochastic continuous-time hidden Markov approach for analyzing genome tiling arrays.pdf:pdf},
issn = {1932-6157},
journal = {The Annals of Applied Statistics},
keywords = {Bayesian hierarchical model,Expectation Conditional Maximization,Hidden Markov Model,Markov chain Monte Carlo,Tiling microarray,bayes,continuous-space Markov chain,dbn,forward--backward algorithm},
mendeley-tags = {bayes,dbn},
month = sep,
number = {3},
pages = {1183--1203},
title = {{Doubly stochastic continuous-time hidden Markov approach for analyzing genome tiling arrays}},
url = {http://projecteuclid.org/euclid.aoas/1254773284},
volume = {3},
year = {2009}
}
@article{Candes2010a,
author = {Candes, Emmanuel J and Plan, Yaniv},
doi = {10.1109/JPROC.2009.2035722},
file = {:Users/jonas/Documents/Mendeley Desktop/Candes, Plan - 2010 - Matrix Completion With Noise.pdf:pdf},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
keywords = {2009,accepted october 17,compressed sensing,date of publication,duality in optimization,low-,manuscript received march 18,matrix completion,nuclear-norm minimization,oracle inequalities,rank matrices,semidefinite programming},
month = jun,
number = {6},
pages = {925--936},
title = {{Matrix Completion With Noise}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5454406},
volume = {98},
year = {2010}
}
@article{Beck2008b,
abstract = {When making a decision, one must first accumulate evidence, often over time, and then select the appropriate action. Here, we present a neural model of decision making that can perform both evidence accumulation and action selection optimally. More specifically, we show that, given a Poisson-like distribution of spike counts, biological neural networks can accumulate evidence without loss of information through linear integration of neural activity and can select the most likely action through attractor dynamics. This holds for arbitrary correlations, any tuning curves, continuous and discrete variables, and sensory evidence whose reliability varies over time. Our model predicts that the neurons in the lateral intraparietal cortex involved in evidence accumulation encode, on every trial, a probability distribution which predicts the animal's performance. We present experimental evidence consistent with this prediction and discuss other predictions applicable to more general settings.},
author = {Beck, Jeffrey M and Ma, Wei Ji and Kiani, Roozbeh and Hanks, Tim and Churchland, Anne K and Roitman, Jamie and Shadlen, Michael N and Latham, Peter E and Pouget, Alexandre},
file = {:Users/jonas/Documents/Mendeley Desktop/Beck et al. - 2008 - Probabilistic population codes for Bayesian decision making.pdf:pdf},
institution = {Department of Brain and Cognitive Sciences, University of Rochester, Rochester, NY 14627, USA.},
journal = {Neuron},
number = {6},
pages = {1142--1152},
publisher = {Elsevier Ltd},
title = {{Probabilistic population codes for Bayesian decision making.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2742921\&tool=pmcentrez\&rendertype=abstract},
volume = {60},
year = {2008}
}
@article{Aaronson,
author = {Aaronson, Scott},
file = {:Users/jonas/Documents/Mendeley Desktop/Aaronson - Unknown - NP-complete Problems and Physical Reality.pdf:pdf},
keywords = {complexity theory,np,physics},
mendeley-tags = {complexity theory,np,physics},
pages = {1--23},
title = {{NP-complete Problems and Physical Reality}}
}
@article{Robinson2007,
abstract = {Gas chromatography-mass spectrometry (GC-MS) is a robust platform for the profiling of certain classes of small molecules in biological samples. When multiple samples are profiled, including replicates of the same sample and/or different sample states, one needs to account for retention time drifts between experiments. This can be achieved either by the alignment of chromatographic profiles prior to peak detection, or by matching signal peaks after they have been extracted from chromatogram data matrices. Automated retention time correction is particularly important in non-targeted profiling studies.},
author = {Robinson, Mark D and {De Souza}, David P and Keen, Woon Wai and Saunders, Eleanor C and McConville, Malcolm J and Speed, Terence P and Liki\'{c}, Vladimir a},
doi = {10.1186/1471-2105-8-419},
file = {:Users/jonas/Documents/Mendeley Desktop/Robinson et al. - 2007 - A dynamic programming approach for the alignment of signal peaks in multiple gas chromatography-mass spectrometry experiments.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Artificial Intelligence,Gas Chromatography-Mass Spectrometry,Gas Chromatography-Mass Spectrometry: methods,Gene Expression Profiling,Gene Expression Profiling: methods,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Peptide Mapping,Peptide Mapping: methods},
month = jan,
pages = {419},
pmid = {17963529},
title = {{A dynamic programming approach for the alignment of signal peaks in multiple gas chromatography-mass spectrometry experiments.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2194738\&tool=pmcentrez\&rendertype=abstract},
volume = {8},
year = {2007}
}
@article{Defreitas2004,
author = {{De Freitas}, N. and Dearden, Richard and Hutter, Frank and Morales-Menendez, R. and Mutch, J. and Poole, David},
doi = {10.1109/JPROC.2003.823157},
file = {:Users/jonas/Documents/Mendeley Desktop/ieee2004\_rbpf.pdf:pdf},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
keywords = {RBPF,blackwellized particle filtering,diagnosis,rao,robotics,state estimation},
mendeley-tags = {RBPF},
month = mar,
number = {3},
pages = {455--468},
title = {{Diagnosis by a Waiter and a Mars Explorer}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1271400},
volume = {92},
year = {2004}
}
@article{Steimer2009a,
abstract = {From a theoretical point of view, statistical inference is an attractive model of brain operation. However, it is unclear how to implement these inferential processes in neuronal networks. We offer a solution to this problem by showing in detailed simulations how the belief propagation algorithm on a factor graph can be embedded in a network of spiking neurons. We use pools of spiking neurons as the function nodes of the factor graph. Each pool gathers "messages" in the form of population activities from its input nodes and combines them through its network dynamics. Each of the various output messages to be transmitted over the edges of the graph is computed by a group of readout neurons that feed in their respective destination pools. We use this approach to implement two examples of factor graphs. The first example, drawn from coding theory, models the transmission of signals through an unreliable channel and demonstrates the principles and generality of our network approach. The second, more applied example is of a psychophysical mechanism in which visual cues are used to resolve hypotheses about the interpretation of an object's shape and illumination. These two examples, and also a statistical analysis, demonstrate good agreement between the performance of our networks and the direct numerical evaluation of belief propagation.},
author = {Steimer, Andreas and Maass, Wolfgang and Douglas, Rodney},
file = {:Users/jonas/Documents/Mendeley Desktop/Steimer, Maass, Douglas - 2009 - Belief propagation in networks of spiking neurons.pdf:pdf},
institution = {Institute of Neuroinformatics, University of Z\"{u}rich, and ETH Z\"{u}rich, Z\"{u}rich, 8057 Switzerland. asteimer@ini.phys.ethz.ch},
journal = {Neural Computation},
number = {9},
pages = {2502--2523},
pmid = {19548806},
publisher = {MIT Press},
title = {{Belief propagation in networks of spiking neurons.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19548806},
volume = {21},
year = {2009}
}
@article{Bandyopadhyay2009,
abstract = {In electronics, information has been traditionally stored, processed and communicated using an electron's charge. This paradigm is increasingly turning out to be energy-inefficient, because movement of charge within an information processing device invariably causes current flow and an associated dissipation. Replacing 'charge' with the 'spin' of an electron to encode information may eliminate much of this dissipation and lead to more energy-efficient 'green electronics'. This realization has spurred significant research in spintronic devices and circuits where spin either directly acts as the physical variable for hosting information or augments the role of charge. In this review article, we discuss and elucidate some of these ideas, and highlight their strengths and weaknesses. Many of them can potentially reduce energy dissipation significantly, but unfortunately are error-prone and unreliable. Moreover, there are serious obstacles to their technological implementation that may be difficult to overcome in the near term. This review addresses three constructs: (1) single devices or binary switches that can be constituents of Boolean logic gates for digital information processing, (2) complete gates that are capable of performing specific Boolean logic operations, and (3) combinational circuits or architectures (equivalent to many gates working in unison) that are capable of performing universal computation.},
author = {Bandyopadhyay, Supriyo and Cahay, Marc},
doi = {10.1088/0957-4484/20/41/412001},
file = {:Users/jonas/Documents/Mendeley Desktop/Bandyopadhyay, Cahay - 2009 - Electron spin for classical information processing a brief survey of spin-based logic devices, gates and circuits.pdf:pdf},
issn = {1361-6528},
journal = {Nanotechnology},
keywords = {Electrochemistry,Electrochemistry: instrumentation,Electrochemistry: methods,Electronic,Models,Nanotechnology,Nanotechnology: instrumentation,Nanotechnology: methods,Theoretical,Transistors,spin,unconventional architecture},
mendeley-tags = {spin,unconventional architecture},
month = oct,
number = {41},
pages = {412001},
pmid = {19755729},
title = {{Electron spin for classical information processing: a brief survey of spin-based logic devices, gates and circuits.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19755729},
volume = {20},
year = {2009}
}
@article{Fox2011,
author = {Fox, Emily and Sudderth, EB},
doi = {10.1109/TSP.2010.2102756},
file = {:Users/jonas/Documents/Mendeley Desktop/Bayesian Nonparametric Inference of Switching Dynamic Linear Models.pdf:pdf},
issn = {1053-587X},
journal = {Signal Processing, IEEE \ldots},
month = apr,
number = {4},
pages = {1569--1585},
title = {{Bayesian nonparametric inference of switching dynamic linear models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5680657 http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5680657},
volume = {59},
year = {2011}
}
@article{Khan2005,
abstract = {We describe a particle filter that effectively deals with interacting targets--targets that are influenced by the proximity and/or behavior of other targets. The particle filter includes a Markov random field (MRF) motion prior that helps maintain the identity of targets throughout an interaction, significantly reducing tracker failures. We show that this MRF prior can be easily implemented by including an additional interaction factor in the importance weights of the particle filter. However, the computational requirements of the resulting multitarget filter render it unusable for large numbers of targets. Consequently, we replace the traditional importance sampling step in the particle filter with a novel Markov chain Monte Carlo (MCMC) sampling step to obtain a more efficient MCMC-based multitarget filter. We also show how to extend this MCMC-based filter to address a variable number of interacting targets. Finally, we present both qualitative and quantitative experimental results, demonstrating that the resulting particle filters deal efficiently and effectively with complicated target interactions.},
author = {Khan, Zia and Balch, Tucker and Dellaert, Frank},
doi = {10.1109/TPAMI.2005.223},
file = {:Users/jonas/Documents/Mendeley Desktop/MCMC-Based Particle Filtering for Tracking a Variable Number of Interacting Targets.pdf:pdf;:Users/jonas/Documents/Mendeley Desktop/01512059.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Animals,Artificial Intelligence,Automated,Automated: methods,Biological,Computer Simulation,Computer-Assisted,Computer-Assisted: methods,Humans,Image Enhancement,Image Enhancement: methods,Image Interpretation,Information Storage and Retrieval,Information Storage and Retrieval: methods,Markov Chains,Models,Monte Carlo Method,Motion,Movement,Movement: physiology,Pattern Recognition,Statistical,Subtraction Technique,Video Recording,Video Recording: methods},
month = nov,
number = {11},
pages = {1805--19},
pmid = {16285378},
title = {{MCMC-based particle filtering for tracking a variable number of interacting targets.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16285378},
volume = {27},
year = {2005}
}
@article{Lefebvre2002,
author = {Lefebvre, Tine and Bruyninckx, Herman and {De Schuller}, J.},
doi = {10.1109/TAC.2002.800742},
file = {:Users/jonas/Documents/Mendeley Desktop/julier2000comment.pdf:pdf},
issn = {0018-9286},
journal = {IEEE Transactions on Automatic Control},
keywords = {UKF},
mendeley-tags = {UKF},
month = aug,
number = {8},
pages = {1406--1409},
title = {{Comment on "A new method for the nonlinear transformation of means and covariances in filters and estimators"}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1024365},
volume = {47},
year = {2002}
}
@article{Weiss2002a,
abstract = {The pattern of local image velocities on the retina encodes important environmental information. Although humans are generally able to extract this information, they can easily be deceived into seeing incorrect velocities. We show that these 'illusions' arise naturally in a system that attempts to estimate local image velocity. We formulated a model of visual motion perception using standard estimation theory, under the assumptions that (i) there is noise in the initial measurements and (ii) slower motions are more likely to occur than faster ones. We found that specific instantiation of such a velocity estimator can account for a wide variety of psychophysical phenomena.},
author = {Weiss, Yair and Simoncelli, Eero P and Adelson, Edward H},
doi = {10.1038/nn858},
file = {:Users/jonas/Documents/Mendeley Desktop/Weiss, Simoncelli, Adelson - 2002 - Motion illusions as optimal percepts.pdf:pdf},
issn = {1097-6256},
journal = {Nature neuroscience},
keywords = {Contrast Sensitivity,Contrast Sensitivity: physiology,Humans,Models, Neurological,Motion Perception,Motion Perception: physiology,Optical Illusions,Optical Illusions: physiology,Time Factors},
month = jun,
number = {6},
pages = {598--604},
pmid = {12021763},
title = {{Motion illusions as optimal percepts.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12021763},
volume = {5},
year = {2002}
}
@article{Rennie2010a,
author = {Rennie, Steven and Hershey, John and Olsen, Peder},
doi = {10.1109/MSP.2010.938081},
file = {:Users/jonas/Documents/Mendeley Desktop/Rennie, Hershey, Olsen - 2010 - Single-Channel Multitalker Speech Recognition.pdf:pdf},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
keywords = {graphical models,speech rec},
mendeley-tags = {graphical models,speech rec},
month = nov,
number = {November},
pages = {66--80},
title = {{Single-Channel Multitalker Speech Recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5563101},
year = {2010}
}
@article{Feeley1997,
author = {Feeley, Marc and Miller, JS and Rozas, GJ},
file = {:Users/jonas/Documents/Mendeley Desktop/Feeley, Miller, Rozas - 1997 - Compiling Higher-Order Languages into Fully Tail-Recursive Portable C.pdf:pdf},
journal = {Work},
keywords = {compilation,scheme},
mendeley-tags = {compilation,scheme},
pages = {1--12},
title = {{Compiling Higher-Order Languages into Fully Tail-Recursive Portable C}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.8788},
year = {1997}
}
@article{Stevenson2009a,
abstract = {Current multielectrode techniques enable the simultaneous recording of spikes from hundreds of neurons. To study neural plasticity and network structure it is desirable to infer the underlying functional connectivity between the recorded neurons. Functional connectivity is defined by a large number of parameters, which characterize how each neuron influences the other neurons. A Bayesian approach that combines information from the recorded spikes (likelihood) with prior beliefs about functional connectivity (prior) can improve inference of these parameters and reduce overfitting. Recent studies have used likelihood functions based on the statistics of point-processes and a prior that captures the sparseness of neural connections. Here we include a prior that captures the empirical finding that interactions tend to vary smoothly in time. We show that this method can successfully infer connectivity patterns in simulated data and apply the algorithm to spike data recorded from primary motor (M1) and premotor (PMd) cortices of a monkey. Finally, we present a new approach to studying structure in inferred connections based on a Bayesian clustering algorithm. Groups of neurons in M1 and PMd show common patterns of input and output that may correspond to functional assemblies.},
author = {Stevenson, Ian H and Rebesco, James M and Hatsopoulos, Nicholas G and Haga, Zach and Miller, Lee E and K\"{o}rding, Konrad P},
doi = {10.1109/TNSRE.2008.2010471},
file = {:Users/jonas/Documents/Mendeley Desktop/Stevenson et al. - 2009 - Bayesian inference of functional connectivity and network structure from spikes.pdf:pdf},
issn = {1558-0210},
journal = {IEEE transactions on neural systems and rehabilitation engineering : a publication of the IEEE Engineering in Medicine and Biology Society},
keywords = {Action Potentials,Action Potentials: physiology,Algorithms,Animals,Bayes Theorem,Brain,Brain Mapping,Brain Mapping: methods,Brain: physiology,Computer Simulation,Humans,Models, Neurological,Nerve Net,Nerve Net: physiology,Synaptic Transmission,Synaptic Transmission: physiology},
month = jun,
number = {3},
pages = {203--13},
pmid = {19273038},
title = {{Bayesian inference of functional connectivity and network structure from spikes.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19273038},
volume = {17},
year = {2009}
}
@article{Ulker2010,
author = {Ulker, Yener and Gunsel, Bilge and Cemgil, Ali Taylan},
doi = {10.1109/ICPR.2010.688},
file = {:Users/jonas/Documents/Mendeley Desktop/Ulker, Gunsel, Cemgil - 2010 - Annealed SMC Samplers for Dirichlet Process Mixture Models.pdf:pdf},
isbn = {978-1-4244-7542-1},
journal = {2010 20th International Conference on Pattern Recognition},
keywords = {-bayesian nonparametrics,annealing,dirichlet process mix-,dpmm,sequential monte carlo,smc,ture},
mendeley-tags = {annealing,dpmm,smc},
month = aug,
number = {1},
pages = {2808--2811},
publisher = {Ieee},
title = {{Annealed SMC Samplers for Dirichlet Process Mixture Models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5597024},
year = {2010}
}
@article{Ayasso2010,
author = {Ayasso, H. and Duch\^{e}ne, B. and Mohammad-Djafari, A.},
doi = {10.1080/09500340903564702},
file = {:Users/jonas/Documents/Mendeley Desktop/Ayasso, Duch\^{e}ne, Mohammad-Djafari - 2010 - Bayesian inversion for optical diffraction tomography.pdf:pdf},
issn = {0950-0340},
journal = {Journal of Modern Optics},
keywords = {and gibbs sampling,bayes,bayesian approach,gauss,hierarchical markov fields,inversion,markov,monte carlo markov chain,non-linear inverse scattering problem,optical diffraction tomography,potts prior},
mendeley-tags = {bayes,inversion},
month = may,
number = {9},
pages = {765--776},
title = {{Bayesian inversion for optical diffraction tomography}},
url = {http://www.tandfonline.com/doi/abs/10.1080/09500340903564702},
volume = {57},
year = {2010}
}
@article{Circuits2009,
author = {Circuits, Probabilistic-based Noise-tolerant Vlsi and Wey, I-chyn and Member, Student and Chen, You-gang and Yu, Chang-hong and Andywu, An-yeu and Chen, Jie and Member, Senior},
doi = {10.1109/TCSI.2009.2015648},
file = {:Users/jonas/Documents/Mendeley Desktop/Circuits et al. - 2009 - Design and Implementation of Cost-Effective Probabilistic-Based Noise-Tolerant VLSI Circuits.pdf:pdf},
issn = {1549-8328},
journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
keywords = {circuits},
mendeley-tags = {circuits},
month = nov,
number = {11},
pages = {2411--2424},
title = {{Design and Implementation of Cost-Effective Probabilistic-Based Noise-Tolerant VLSI Circuits}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4785218},
volume = {56},
year = {2009}
}
@techreport{Neal2010,
author = {Neal, Radford M.},
booktitle = {Arxiv preprint arXiv:1101.0387},
file = {:Users/jonas/Documents/Mendeley Desktop/Neal - 2010 - MCMC Using Ensembles of States for Problems with Fast and Slow Variables such as Gaussian Process Regression.pdf:pdf},
institution = {University of Toronto},
keywords = {mcmc},
mendeley-tags = {mcmc},
title = {{MCMC Using Ensembles of States for Problems with Fast and Slow Variables such as Gaussian Process Regression}},
url = {http://arxiv.org/abs/1101.0387},
year = {2010}
}
@article{Moreno-Bote2011a,
abstract = {It is well-established that some aspects of perception and action can be understood as probabilistic inferences over underlying probability distributions. In some situations, it would be advantageous for the nervous system to sample interpretations from a probability distribution rather than commit to a particular interpretation. In this study, we asked whether visual percepts correspond to samples from the probability distribution over image interpretations, a form of sampling that we refer to as Bayesian sampling. To test this idea, we manipulated pairs of sensory cues in a bistable display consisting of two superimposed moving drifting gratings, and we asked subjects to report their perceived changes in depth ordering. We report that the fractions of dominance of each percept follow the multiplicative rule predicted by Bayesian sampling. Furthermore, we show that attractor neural networks can sample probability distributions if input currents add linearly and encode probability distributions with probabilistic population codes.},
author = {Moreno-Bote, Rub\'{e}n and Knill, David C and Pouget, Alexandre},
doi = {10.1073/pnas.1101430108},
file = {:Users/jonas/Documents/Mendeley Desktop/Moreno-Bote, Knill, Pouget - 2011 - Bayesian sampling in visual perception.pdf:pdf},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
month = jul,
pages = {1--6},
pmid = {21742982},
title = {{Bayesian sampling in visual perception.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21742982},
year = {2011}
}
@article{Olveczky2003a,
abstract = {An important task in vision is to detect objects moving within a stationary scene. During normal viewing this is complicated by the presence of eye movements that continually scan the image across the retina, even during fixation. To detect moving objects, the brain must distinguish local motion within the scene from the global retinal image drift due to fixational eye movements. We have found that this process begins in the retina: a subset of retinal ganglion cells responds to motion in the receptive field centre, but only if the wider surround moves with a different trajectory. This selectivity for differential motion is independent of direction, and can be explained by a model of retinal circuitry that invokes pooling over nonlinear interneurons. The suppression by global image motion is probably mediated by polyaxonal, wide-field amacrine cells with transient responses. We show how a population of ganglion cells selective for differential motion can rapidly flag moving objects, and even segregate multiple moving objects.},
author = {Olveczky, Bence P and Baccus, Stephen a and Meister, Markus},
doi = {10.1038/nature01652},
file = {:Users/jonas/Documents/Mendeley Desktop/Olveczky, Baccus, Meister - 2003 - Segregation of object and background motion in the retina.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Action Potentials,Amacrine Cells,Amacrine Cells: physiology,Animals,Dendrites,Dendrites: physiology,Eye Movements,Eye Movements: physiology,Fixation, Ocular,Fixation, Ocular: physiology,Models, Neurological,Motion,Photic Stimulation,Rabbits,Retina,Retina: cytology,Retina: physiology,Retinal Ganglion Cells,Retinal Ganglion Cells: physiology,Urodela,Urodela: physiology},
month = may,
number = {6938},
pages = {401--8},
pmid = {12754524},
title = {{Segregation of object and background motion in the retina.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12754524},
volume = {423},
year = {2003}
}
@article{Candy2007,
annote = {Straight-up review of the classic SIR PF, but with a pretty decent "Synthetic Aperature Processing for a Towed Array" problem. 

        
Very clear example, should write-up},
author = {Candy, James},
doi = {10.1109/MSP.2007.4286566},
file = {:Users/jonas/Documents/Mendeley Desktop/Bootstrap Particle Filtering.pdf:pdf},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
keywords = {review},
mendeley-tags = {review},
month = jul,
number = {4},
pages = {73--85},
title = {{Bootstrap Particle Filtering}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4286566 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4286566},
volume = {24},
year = {2007}
}
@article{Fiser2010a,
abstract = {Human perception has recently been characterized as statistical inference based on noisy and ambiguous sensory inputs. Moreover, suitable neural representations of uncertainty have been identified that could underlie such probabilistic computations. In this review, we argue that learning an internal model of the sensory environment is another key aspect of the same statistical inference procedure and thus perception and learning need to be treated jointly. We review evidence for statistically optimal learning in humans and animals, and re-evaluate possible neural representations of uncertainty based on their potential to support statistically optimal learning. We propose that spontaneous activity can have a functional role in such representations leading to a new, sampling-based, framework of how the cortex represents information and uncertainty.},
author = {Fiser, J\'{o}zsef and Berkes, Pietro and Orb\'{a}n, Gergo and Lengyel, M\'{a}t\'{e}},
doi = {10.1016/j.tics.2010.01.003},
file = {:Users/jonas/Documents/Mendeley Desktop/Fiser et al. - 2010 - Statistically optimal perception and learning from behavior to neural representations.pdf:pdf},
issn = {1879-307X},
journal = {Trends in cognitive sciences},
keywords = {Animals,Cerebral Cortex,Humans,Learning,Models, Neurological,Models, Statistical,Perception},
month = mar,
number = {3},
pages = {119--30},
pmid = {20153683},
title = {{Statistically optimal perception and learning: from behavior to neural representations.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2939867\&tool=pmcentrez\&rendertype=abstract},
volume = {14},
year = {2010}
}
@article{Howard1992,
author = {Howard, PG},
file = {:Users/jonas/Documents/Mendeley Desktop/Howard - 1992 - Practical implementations of arithmetic coding.pdf:pdf},
journal = {Image and Text compression},
keywords = {bayes,coding},
mendeley-tags = {bayes,coding},
number = {April},
title = {{Practical implementations of arithmetic coding}},
url = {http://www.springerlink.com/index/K07112M372222795.pdf},
year = {1992}
}
@article{Cohen1995,
author = {Cohen, Jacob},
doi = {10.1037/0003-066X.50.12.1103},
file = {:Users/jonas/Documents/Mendeley Desktop/Cohen - 1995 - The earth is round (p .05) Rejoinder.pdf:pdf},
issn = {0003-066X},
journal = {American Psychologist},
number = {12},
pages = {1103--1103},
title = {{The earth is round (p < .05): Rejoinder.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0003-066X.50.12.1103},
volume = {50},
year = {1995}
}
@article{Germain2006a,
address = {New York, New York, USA},
author = {Germain, Guillaume},
doi = {10.1145/1159789.1159795},
file = {:Users/jonas/Documents/Mendeley Desktop/Germain - 2006 - Concurrency oriented programming in termite scheme(2).pdf:pdf},
isbn = {1595934901},
journal = {Proceedings of the 2006 ACM SIGPLAN workshop on Erlang - ERLANG '06},
keywords = {contin-,distributed computing,erlang,lisp,parallel,scheme},
mendeley-tags = {parallel,scheme},
pages = {20},
publisher = {ACM Press},
title = {{Concurrency oriented programming in termite scheme}},
url = {http://portal.acm.org/citation.cfm?doid=1159789.1159795},
year = {2006}
}
@article{Feng2009,
author = {Feng, Jinchao and Jia, Kebin and Qin, Chenghu and Yan, Guorui and Zhang, Xing and Liu, Junting and Tian, Jie},
file = {:Users/jonas/Documents/Mendeley Desktop/Feng et al. - 2009 - Three-dimensional bioluminescence tomographased on besian approach.pdf:pdf},
isbn = {1109511116},
journal = {Optics Express},
keywords = {bayesian,bioluminescence,tomography},
mendeley-tags = {bayesian,bioluminescence,tomography},
number = {19},
pages = {117--124},
title = {{Three-dimensional bioluminescence tomographased on besian approach}},
volume = {17},
year = {2009}
}
@article{Mallavarapu2009,
abstract = {Mathematical models are increasingly used to understand how phenotypes emerge from systems of molecular interactions. However, their current construction as monolithic sets of equations presents a fundamental barrier to progress. Overcoming this requires modularity, enabling sub-systems to be specified independently and combined incrementally, and abstraction, enabling generic properties of biological processes to be specified independently of specific instances. These, in turn, require models to be represented as programs rather than as datatypes. Programmable modularity and abstraction enables libraries of modules to be created, which can be instantiated and reused repeatedly in different contexts with different components. We have developed a computational infrastructure that accomplishes this. We show here why such capabilities are needed, what is required to implement them and what can be accomplished with them that could not be done previously.},
author = {Mallavarapu, Aneil and Thomson, Matthew and Ullian, Benjamin and Gunawardena, Jeremy},
doi = {10.1098/rsif.2008.0205},
file = {:Users/jonas/Documents/Mendeley Desktop/Mallavarapu et al. - 2009 - Programming with models modularity and abstraction provide powerful capabilities for systems biology.pdf:pdf},
issn = {1742-5689},
journal = {Journal of the Royal Society, Interface / the Royal Society},
keywords = {Animals,Biological,Drosophila,Drosophila: metabolism,Models,Phosphorylation,Software,Systems Biology,Systems Biology: methods,compbio,programming languages},
mendeley-tags = {compbio,programming languages},
month = mar,
number = {32},
pages = {257--70},
pmid = {18647734},
title = {{Programming with models: modularity and abstraction provide powerful capabilities for systems biology.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2659579\&tool=pmcentrez\&rendertype=abstract},
volume = {6},
year = {2009}
}
@article{Baraniuk2010,
author = {Baraniuk, Richard G and Cevher, Volkan and Wakin, Michael B},
doi = {10.1109/JPROC.2009.2038076},
file = {:Users/jonas/Documents/Mendeley Desktop/Baraniuk, Cevher, Wakin - 2010 - Low-Dimensional Models for Dimensionality Reduction and Signal Recovery A Geometric Perspective.pdf:pdf},
isbn = {0011081007},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
keywords = {compression,compressive sensing,dimensional-,ity reduction,low-dimensional signal models that,manifold,perspective a number of,point cloud,reduction,sparsity,stable embedding,support stable information-preserving dimensionali},
month = jun,
number = {6},
pages = {959--971},
title = {{Low-Dimensional Models for Dimensionality Reduction and Signal Recovery: A Geometric Perspective}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5456163},
volume = {98},
year = {2010}
}
@article{Bobrowski2009a,
abstract = {A key requirement facing organisms acting in uncertain dynamic environments is the real-time estimation and prediction of environmental states, based on which effective actions can be selected. While it is becoming evident that organisms employ exact or approximate Bayesian statistical calculations for these purposes, it is far less clear how these putative computations are implemented by neural networks in a strictly dynamic setting. In this work, we make use of rigorous mathematical results from the theory of continuous time point process filtering and show how optimal real-time state estimation and prediction may be implemented in a general setting using simple recurrent neural networks. The framework is applicable to many situations of common interest, including noisy observations, non-Poisson spike trains (incorporating adaptation), multisensory integration, and state prediction. The optimal network properties are shown to relate to the statistical structure of the environment, and the benefits of adaptation are studied and explicitly demonstrated. Finally, we recover several existing results as appropriate limits of our general setting.},
author = {Bobrowski, Omer and Meir, Ron and Eldar, Yonina C},
doi = {10.1162/neco.2008.01-08-692},
file = {:Users/jonas/Documents/Mendeley Desktop/Bobrowski, Meir, Eldar - 2009 - Bayesian filtering in spiking neural networks noise, adaptation, and multisensory integration.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Action Potentials,Action Potentials: physiology,Adaptation, Physiological,Adaptation, Physiological: physiology,Animals,Bayes Theorem,Computer Simulation,Markov Chains,Models, Neurological,Models, Statistical,Neural Networks (Computer),Neural Pathways,Neural Pathways: physiology,Noise,Sensory Receptor Cells,Sensory Receptor Cells: physiology},
month = may,
number = {5},
pages = {1277--320},
pmid = {19018706},
title = {{Bayesian filtering in spiking neural networks: noise, adaptation, and multisensory integration.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19018706},
volume = {21},
year = {2009}
}
@article{cython,
author = {Behnel, Stefan and Bradshaw, Robert and Citro, Craig and Dalcin, Lisandro and Seljebotn, Dag Sverre and Smith, Kurt},
doi = {10.1109/MCSE.2010.118},
issn = {1521-9615},
journal = {Computing in Science \& Engineering},
month = mar,
number = {2},
pages = {31--39},
title = {{Cython: The Best of Both Worlds}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5582062},
volume = {13},
year = {2011}
}
@misc{Launchbury1991,
author = {Launchbury, J},
booktitle = {Science of Computer Programming},
doi = {10.1016/0167-6423(91)90004-H},
file = {:Users/jonas/Documents/Mendeley Desktop/Launchbury - 1991 - An introduction to functional programming through lambda calculus Greg Michaelson (Addison-Wesley, Wokingham, United Kingdom, 1990), Price £17.95 (paperback), ISBN 0-201-17812-5.ps:ps},
issn = {01676423},
month = sep,
number = {2},
pages = {198--200},
title = {{An introduction to functional programming through lambda calculus Greg Michaelson (Addison-Wesley, Wokingham, United Kingdom, 1990), Price £17.95 (paperback), ISBN 0-201-17812-5.}},
url = {http://linkinghub.elsevier.com/retrieve/pii/016764239190004H},
volume = {16},
year = {1991}
}
@article{Gordon1993,
author = {Gordon, NJ and Salmond, DJ and Smith, AFM},
file = {:Users/jonas/Documents/Mendeley Desktop/Gordon 1993 Bootstrap Filter .pdf:pdf},
journal = {IEE Proceedings F: Radar and Signal Processing},
keywords = {smc},
mendeley-tags = {smc},
number = {2},
pages = {107--113},
title = {{Novel approach to nonlinear/non-Gaussian Bayesian state estimation}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=210672},
volume = {140},
year = {1993}
}
@inproceedings{Johansen2008,
abstract = {A talk about Auxiliary Particle Filters},
annote = {Slides},
author = {Johansen, Adam M and Whiteley, Nick},
file = {:Users/jonas/Documents/Mendeley Desktop/A Modern Perspecitve on Auxiliary Particle Filters.pdf:pdf},
keywords = {APF},
mendeley-tags = {APF},
number = {June},
pages = {1--42},
title = {{A Modern Perspective on Auxiliary Particle Filters}},
year = {2008}
}
@article{Julier2002,
author = {Julier, S. and Uhlmann, J.},
doi = {10.1109/TAC.2002.800741},
file = {:Users/jonas/Documents/Mendeley Desktop/julier2000reply.pdf:pdf},
issn = {0018-9286},
journal = {IEEE Transactions on Automatic Control},
keywords = {UKF},
mendeley-tags = {UKF},
month = aug,
number = {8},
pages = {1408--1409},
title = {{Comment on "A new method for the nonlinear transformation of means and covariances in filters and estimators" [ authors' reply]}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1024366},
volume = {47},
year = {2002}
}
@misc{scipy,
author = {Jones, Eric and Oliphant, Travis and Peterson, Pearu and Others},
title = {{SciPy: Open Source Scientific Tools for Python}},
url = {http://www.scipy.org}
}
@article{Pinto2009,
abstract = {While many models of biological object recognition share a common set of "broad-stroke" properties, the performance of any one model depends strongly on the choice of parameters in a particular instantiation of that model--e.g., the number of units per layer, the size of pooling kernels, exponents in normalization operations, etc. Since the number of such parameters (explicit or implicit) is typically large and the computational cost of evaluating one particular parameter set is high, the space of possible model instantiations goes largely unexplored. Thus, when a model fails to approach the abilities of biological visual systems, we are left uncertain whether this failure is because we are missing a fundamental idea or because the correct "parts" have not been tuned correctly, assembled at sufficient scale, or provided with enough training. Here, we present a high-throughput approach to the exploration of such parameter sets, leveraging recent advances in stream processing hardware (high-end NVIDIA graphic cards and the PlayStation 3's IBM Cell Processor). In analogy to high-throughput screening approaches in molecular biology and genetics, we explored thousands of potential network architectures and parameter instantiations, screening those that show promising object recognition performance for further analysis. We show that this approach can yield significant, reproducible gains in performance across an array of basic object recognition tasks, consistently outperforming a variety of state-of-the-art purpose-built vision systems from the literature. As the scale of available computational power continues to expand, we argue that this approach has the potential to greatly accelerate progress in both artificial vision and our understanding of the computational underpinning of biological vision.},
author = {Pinto, Nicolas and Doukhan, David and DiCarlo, James J and Cox, David D},
doi = {10.1371/journal.pcbi.1000579},
file = {:Users/jonas/Documents/Mendeley Desktop/Pinto et al. - 2009 - A high-throughput screening approach to discovering good forms of biologically inspired visual representation.pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
keywords = {Algorithms,Animals,Artificial Intelligence,Automated,Automated: methods,Biomimetics,Biomimetics: methods,Computer Simulation,Humans,Models,Neurological,Pattern Recognition,Visual,Visual Cortex,Visual Cortex: physiology,Visual: physiology,friends,gpu},
mendeley-tags = {friends,gpu},
month = nov,
number = {11},
pages = {e1000579},
pmid = {19956750},
title = {{A high-throughput screening approach to discovering good forms of biologically inspired visual representation.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2775908\&tool=pmcentrez\&rendertype=abstract},
volume = {5},
year = {2009}
}
@article{Romberg2007,
author = {Romberg, Justin and Wakin, Michael},
file = {:Users/jonas/Documents/Mendeley Desktop/Romberg, Wakin - 2007 - Compressed Sensing A Tutorial.pdf:pdf},
journal = {IEEE Statistical Signal Processing Workshop},
keywords = {Compressed Sensing},
mendeley-tags = {Compressed Sensing},
title = {{Compressed Sensing: A Tutorial}},
year = {2007}
}
@article{Ahmed2012,
abstract = {The decentralized particle filter (DPF) was proposed recently to increase the level of parallelism of particle filtering. Given a decomposition of the state space into two nested sets of variables, the DPF uses a particle filter to sample the first set and then conditions on this sample to generate a set of samples for the second set of variables. The DPF can be understood as a variant of the popular Rao-Blackwellized particle filter (RBPF), where the second step is carried out using Monte Carlo approximations instead of analytical inference. As a result, the range of applications of the DPF is broader than the one for the RBPF. In this paper, we improve the DPF in two ways. First, we derive a Monte Carlo approximation of the optimal proposal distribution and, consequently, design and implement a more efficient look-ahead DPF. Although the decentralized filters were initially designed to capitalize on parallel implementation, we show that the look-ahead DPF can outperform the standard particle filter even on a single machine. Second, we propose the use of bandit algorithms to automatically configure the state space decomposition of the DPF.},
archivePrefix = {arXiv},
arxivId = {1203.2394},
author = {Ahmed, Mohamed Osama and Bibalan, Pouyan T. and de Freitas, Nando and Fauvel, Simon},
eprint = {1203.2394},
file = {:Users/jonas/Documents/Mendeley Desktop/Decentralized, Adaptive, Look-Ahead Particle Filtering.pdf:pdf},
keywords = {DPF},
mendeley-tags = {DPF},
month = mar,
number = {i},
pages = {16},
title = {{Decentralized, Adaptive, Look-Ahead Particle Filtering}},
url = {http://adsabs.harvard.edu/abs/2012arXiv1203.2394O http://arxiv.org/abs/1203.2394},
year = {2012}
}
@article{Leonhardt2009,
abstract = {Perfect imaging has been believed to rely on negative refraction, but here we show that an ordinary positively-refracting optical medium may form perfect images as well. In particular, we establish a mathematical proof that Maxwell's fish eye in two-dimensional integrated optics makes a perfect instrument with a resolution not limited by the wavelength of light. We also show how to modify the fish eye such that perfect imaging devices can be made in practice. Our method of perfect focusing may also find applications outside of optics, in acoustics, fluid mechanics or quantum physics, wherever waves obey the two-dimensional Helmholtz equation.},
archivePrefix = {arXiv},
arxivId = {0909.5305},
author = {Leonhardt, Ulf},
doi = {10.1088/1367-2630/11/9/093040},
eprint = {0909.5305},
file = {:Users/jonas/Documents/Mendeley Desktop/Leonhardt - 2009 - Perfect imaging without negative refraction.pdf:pdf},
issn = {1367-2630},
journal = {New Journal of Physics},
keywords = {Optics,perfect imaging},
mendeley-tags = {perfect imaging},
month = sep,
number = {9},
pages = {093040},
title = {{Perfect imaging without negative refraction}},
url = {http://arxiv.org/abs/0909.5305 http://stacks.iop.org/1367-2630/11/i=9/a=093040?key=crossref.c63a62fca2ec4e072986ce40b843057b},
volume = {11},
year = {2009}
}
@article{Dahl2007,
author = {Dahl, David B},
doi = {10.1214/07-BA219B},
file = {:Users/jonas/Documents/Mendeley Desktop/Dahl - 2007 - Comment on Article by Jain and Neal.pdf:pdf},
issn = {1931-6690},
journal = {Bayesian Analysis},
keywords = {conjugacy,dpmm},
mendeley-tags = {conjugacy,dpmm},
number = {3},
pages = {479--482},
title = {{Comment on Article by Jain and Neal}},
url = {http://ba.stat.cmu.edu/journal/2007/vol02/issue03/robert.pdf},
volume = {2},
year = {2007}
}
@article{Doucet2009,
abstract = {Optimal estimation problems for non-linear non-Gaussian state-space models do not typically admit analytic solutions. Since their introduction in 1993, particle filtering methods have become a very popular class of algorithms to solve these estimation problems numerically in an online manner, i.e. recursively as obser- vations become available, and are now routinely used in fields as diverse as computer vision, econometrics, robotics and navigation. The objective of this tutorial is to provide a complete, up-to-date survey of this field as of 2008. Basic and advanced particle methods for filtering as well as smoothing are presented.},
author = {Doucet, Arnaud and Johansen, AM},
file = {:Users/jonas/Documents/Mendeley Desktop/A Tutorial on Particle Filtering and Smoothing- Fifteen years later.pdf:pdf},
journal = {Handbook of Nonlinear Filtering},
keywords = {review},
mendeley-tags = {review},
number = {December 2008},
pages = {1--39},
title = {{A tutorial on particle filtering and smoothing: fifteen years later}},
url = {http://automatica.dei.unipd.it/tl\_files/utenti/lucaschenato/Classes/PSC10\_11/Tutorial\_PF\_doucet\_johansen.pdf},
year = {2009}
}
@article{Gong2009,
abstract = {For ghost imaging, pursuing high resolution images and short acquisition times required for reconstructing images are always two main goals. We report an image reconstruction algorithm called compressive sampling (CS) reconstruction to recover ghost images. By CS reconstruction, ghost imaging with both super-resolution and a good signal-to-noise ratio can be obtained via short acquisition times. Both effect influencing and approaches further improving the resolution of ghost images via CS reconstruction, relationship between ghost imaging and CS theory are also discussed.},
archivePrefix = {arXiv},
arxivId = {0910.4823},
author = {Gong, Wenlin},
eprint = {0910.4823},
file = {:Users/jonas/Documents/Mendeley Desktop/Gong - 2009 - Super-resolution ghost imaging via compressive sampling reconstruction.pdf:pdf},
journal = {Arxiv preprint arXiv:0910.4823},
keywords = {Quantum Physics,ghost imaging,super-resolution},
mendeley-tags = {ghost imaging,super-resolution},
month = oct,
number = {3},
pages = {1--4},
title = {{Super-resolution ghost imaging via compressive sampling reconstruction}},
url = {http://arxiv.org/abs/0910.4823},
year = {2009}
}
@article{Olveczky2007a,
abstract = {Due to fixational eye movements, the image on the retina is always in motion, even when one views a stationary scene. When an object moves within the scene, the corresponding patch of retina experiences a different motion trajectory than the surrounding region. Certain retinal ganglion cells respond selectively to this condition, when the motion in the cell's receptive field center is different from that in the surround. Here we show that this response is strongest at the very onset of differential motion, followed by gradual adaptation with a time course of several seconds. Different subregions of a ganglion cell's receptive field can adapt independently. The circuitry responsible for differential motion adaptation lies in the inner retina. Several candidate mechanisms were tested, and the adaptation most likely results from synaptic depression at the synapse from bipolar to ganglion cell. Similar circuit mechanisms may act more generally to emphasize novel features of a visual stimulus.},
author = {Olveczky, Bence P and Baccus, Stephen a and Meister, Markus},
doi = {10.1016/j.neuron.2007.09.030},
file = {:Users/jonas/Documents/Mendeley Desktop/Olveczky, Baccus, Meister - 2007 - Retinal adaptation to object motion.pdf:pdf},
issn = {0896-6273},
journal = {Neuron},
keywords = {Adaptation, Physiological,Adaptation, Physiological: physiology,Animals,Membrane Potentials,Membrane Potentials: physiology,Motion Perception,Motion Perception: physiology,Neural Inhibition,Neural Inhibition: physiology,Neural Pathways,Neural Pathways: cytology,Neural Pathways: physiology,Organ Culture Techniques,Pattern Recognition, Visual,Pattern Recognition, Visual: physiology,Photic Stimulation,Retina,Retina: cytology,Retina: physiology,Retinal Bipolar Cells,Retinal Bipolar Cells: physiology,Retinal Ganglion Cells,Retinal Ganglion Cells: cytology,Retinal Ganglion Cells: physiology,Synapses,Synapses: physiology,Synaptic Transmission,Synaptic Transmission: physiology,Urodela,Visual Fields,Visual Fields: physiology},
month = nov,
number = {4},
pages = {689--700},
pmid = {18031685},
title = {{Retinal adaptation to object motion.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2117331\&tool=pmcentrez\&rendertype=abstract},
volume = {56},
year = {2007}
}
@article{Potter2010,
author = {Potter, Lee C and Ertin, Emre and Parker, Jason T and Cetin, Mujdat},
doi = {10.1109/JPROC.2009.2037526},
file = {:Users/jonas/Documents/Mendeley Desktop/Potter et al. - 2010 - Sparsity and Compressed Sensing in Radar Imaging.pdf:pdf},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
keywords = {moving target indication,penalized least squares,radar ambiguity function,random arrays,sparse reconstruc-,synthetic aperture radar,tion},
month = jun,
number = {6},
pages = {1006--1020},
title = {{Sparsity and Compressed Sensing in Radar Imaging}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5420035},
volume = {98},
year = {2010}
}
@book{Bishop2006,
author = {Bishop, C.M.},
booktitle = {Annals of Physics},
file = {:Users/jonas/Documents/Mendeley Desktop/Bishop - 2006 - Pattern Recognition and Machine Learning.pdf:pdf},
isbn = {9780387310732},
number = {2},
publisher = {Springer New York},
title = {{Pattern Recognition and Machine Learning}},
url = {http://www.mendeley.com/research/no-title-avail/ http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
volume = {4},
year = {2006}
}
@article{Sastry2010,
author = {Sastry, PS and Unnikrishnan, KP},
file = {:Users/jonas/Documents/Mendeley Desktop/Sastry, Unnikrishnan - 2010 - Conditional Probability-Based Significance Tests for Sequential Patterns in Multineuronal Spike Trains.pdf:pdf},
journal = {Neural computation},
number = {4},
pages = {1025--1059},
publisher = {MIT Press},
title = {{Conditional Probability-Based Significance Tests for Sequential Patterns in Multineuronal Spike Trains}},
url = {http://www.mitpressjournals.org/doi/pdf/10.1162/neco.2009.12-08-928},
volume = {22},
year = {2010}
}
@article{Plumbley2010,
author = {Plumbley, Mark D and Blumensath, Thomas and Daudet, Laurent and Gribonval, Remi and Davies, Mike E},
doi = {10.1109/JPROC.2009.2030345},
file = {:Users/jonas/Documents/Mendeley Desktop/Plumbley et al. - 2010 - Sparse Representations in Audio and Music From Coding to Source Separation.pdf:pdf},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
month = jun,
number = {6},
pages = {995--1005},
title = {{Sparse Representations in Audio and Music: From Coding to Source Separation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5332363},
volume = {98},
year = {2010}
}
@article{matplotlib,
author = {Hunter, John D.},
doi = {10.1109/MCSE.2007.55},
issn = {1521-9615},
journal = {Computing in Science \& Engineering},
number = {3},
pages = {90--95},
title = {{Matplotlib: A 2D Graphics Environment}},
url = {http://www.matplotlib.org},
volume = {9},
year = {2007}
}
@article{Djuric2003,
annote = {Great tutorial on using particle filtering for signal processing methods, like fading channels, etc. Includes models, etc. },
author = {Djuric, P.M. and Kotecha, J.H. and Zhang, J. and Huang, Y. and Ghirmai, T. and Bugallo, M.F. and Miguez, J.},
doi = {10.1109/MSP.2003.1236770},
file = {:Users/jonas/Documents/Mendeley Desktop/Particle Filtering, IEEE Signal Processing Magazine.pdf:pdf},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
keywords = {review},
mendeley-tags = {review},
month = sep,
number = {5},
pages = {19--38},
title = {{Particle Filtering}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1236770},
volume = {20},
year = {2003}
}
@article{Michalet2007,
abstract = {Single-molecule observation, characterization and manipulation techniques have recently come to the forefront of several research domains spanning chemistry, biology and physics. Due to the exquisite sensitivity, specificity, and unmasking of ensemble averaging, single-molecule fluorescence imaging and spectroscopy have become, in a short period of time, important tools in cell biology, biochemistry and biophysics. These methods led to new ways of thinking about biological processes such as viral infection, receptor diffusion and oligomerization, cellular signaling, protein-protein or protein-nucleic acid interactions, and molecular machines. Such achievements require a combination of several factors to be met, among which detector sensitivity and bandwidth are crucial. We examine here the needed performance of photodetectors used in these types of experiments, the current state of the art for different categories of detectors, and actual and future developments of single-photon counting detectors for single-molecule imaging and spectroscopy.},
author = {Michalet, X and Siegmund, O H W and Vallerga, J V and Jelinsky, P and Millaud, J E and Weiss, S},
doi = {10.1080/09500340600769067},
file = {:Users/jonas/Documents/Mendeley Desktop/Michalet et al. - 2007 - Detectors for single-molecule fluorescence imaging and spectroscopy.pdf:pdf},
issn = {1362-3044},
journal = {Journal of modern optics},
month = jan,
number = {2-3},
pages = {239},
pmid = {20157633},
title = {{Detectors for single-molecule fluorescence imaging and spectroscopy.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2821066\&tool=pmcentrez\&rendertype=abstract},
volume = {54},
year = {2007}
}
@article{Lee2003a,
abstract = {Traditional views of visual processing suggest that early visual neurons in areas V1 and V2 are static spatiotemporal filters that extract local features from a visual scene. The extracted information is then channeled through a feedforward chain of modules in successively higher visual areas for further analysis. Recent electrophysiological recordings from early visual neurons in awake behaving monkeys reveal that there are many levels of complexity in the information processing of the early visual cortex, as seen in the long-latency responses of its neurons. These new findings suggest that activity in the early visual cortex is tightly coupled and highly interactive with the rest of the visual system. They lead us to propose a new theoretical setting based on the mathematical framework of hierarchical Bayesian inference for reasoning about the visual system. In this framework, the recurrent feedforward/feedback loops in the cortex serve to integrate top-down contextual priors and bottom-up observations so as to implement concurrent probabilistic inference along the visual hierarchy. We suggest that the algorithms of particle filtering and Bayesian-belief propagation might model these interactive cortical computations. We review some recent neurophysiological evidences that support the plausibility of these ideas.},
author = {Lee, Tai Sing and Mumford, David},
file = {:Users/jonas/Documents/Mendeley Desktop/Lee, Mumford - 2003 - Hierarchical Bayesian inference in the visual cortex.pdf:pdf},
issn = {1084-7529},
journal = {Journal of the Optical Society of America. A, Optics, image science, and vision},
keywords = {Animals,Bayes Theorem,Haplorhini,Models, Neurological,Visual Cortex,Visual Cortex: physiology},
month = jul,
number = {7},
pages = {1434--48},
pmid = {12868647},
title = {{Hierarchical Bayesian inference in the visual cortex.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12868647},
volume = {20},
year = {2003}
}
@article{Colgin2009,
author = {Colgin, Laura Lee and Moser, Edvard I},
doi = {10.1038/nn1209-1483},
file = {:Users/jonas/Documents/Mendeley Desktop/Colgin, Moser - 2009 - Hippocampal theta rhythms follow the beat of their own drum.pdf:pdf},
issn = {1546-1726},
journal = {Nature neuroscience},
keywords = {Animals,Biological Clocks,Biological Clocks: physiology,CA1 Region, Hippocampal,CA1 Region, Hippocampal: cytology,CA1 Region, Hippocampal: physiology,Mice,Pyramidal Cells,Pyramidal Cells: physiology,Theta Rhythm},
month = dec,
number = {12},
pages = {1483--4},
pmid = {19935726},
publisher = {Nature Publishing Group},
title = {{Hippocampal theta rhythms follow the beat of their own drum.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19935726},
volume = {12},
year = {2009}
}
@article{Hinton2006,
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such "autoencoder" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
author = {Hinton, G E and Salakhutdinov, R R},
doi = {10.1126/science.1127647},
file = {:Users/jonas/Documents/Mendeley Desktop/Hinton, Salakhutdinov - 2006 - Reducing the dimensionality of data with neural networks.pdf:pdf},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
keywords = {deep belief networks},
mendeley-tags = {deep belief networks},
month = jul,
number = {5786},
pages = {504--7},
pmid = {16873662},
title = {{Reducing the dimensionality of data with neural networks.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2671482\&tool=pmcentrez\&rendertype=abstract},
volume = {313},
year = {2006}
}
@article{Knight2009,
abstract = {Tutorial of gibbs / MCMC},
author = {Knight, Kevin},
file = {:Users/jonas/Documents/Mendeley Desktop/Knight - 2009 - Bayesian Inference with Tears.pdf:pdf},
keywords = {bayes,mcmc,tutorial},
mendeley-tags = {bayes,mcmc,tutorial},
number = {September},
title = {{Bayesian Inference with Tears}},
year = {2009}
}
@article{Germain2006b,
address = {New York, New York, USA},
author = {Germain, Guillaume},
doi = {10.1145/1159789.1159795},
file = {:Users/jonas/Documents/Mendeley Desktop/Germain - 2006 - Concurrency oriented programming in termite scheme.pdf:pdf},
isbn = {1595934901},
journal = {Proceedings of the 2006 ACM SIGPLAN workshop on Erlang - ERLANG '06},
keywords = {contin-,distributed computing,erlang,lisp,parallel,scheme},
mendeley-tags = {parallel,scheme},
pages = {20},
publisher = {ACM Press},
title = {{Concurrency oriented programming in termite scheme}},
url = {http://portal.acm.org/citation.cfm?doid=1159789.1159795},
year = {2006}
}
@article{Beck2008c,
abstract = {When making a decision, one must first accumulate evidence, often over time, and then select the appropriate action. Here, we present a neural model of decision making that can perform both evidence accumulation and action selection optimally. More specifically, we show that, given a Poisson-like distribution of spike counts, biological neural networks can accumulate evidence without loss of information through linear integration of neural activity and can select the most likely action through attractor dynamics. This holds for arbitrary correlations, any tuning curves, continuous and discrete variables, and sensory evidence whose reliability varies over time. Our model predicts that the neurons in the lateral intraparietal cortex involved in evidence accumulation encode, on every trial, a probability distribution which predicts the animal's performance. We present experimental evidence consistent with this prediction and discuss other predictions applicable to more general settings.},
author = {Beck, Jeffrey M and Ma, Wei Ji and Kiani, Roozbeh and Hanks, Tim and Churchland, Anne K and Roitman, Jamie and Shadlen, Michael N and Latham, Peter E and Pouget, Alexandre},
doi = {10.1016/j.neuron.2008.09.021},
file = {:Users/jonas/Documents/Mendeley Desktop//Beck et al. - 2008 - Probabilistic population codes for Bayesian decision making.pdf:pdf},
issn = {1097-4199},
journal = {Neuron},
keywords = {Action Potentials,Action Potentials: physiology,Animals,Bayes Theorem,Computer Simulation,Decision Making,Decision Making: physiology,Haplorhini,Humans,Models, Neurological,Motion Perception,Motion Perception: physiology,Neural Networks (Computer),Neurons,Neurons: physiology,Nonlinear Dynamics,Photic Stimulation,Reaction Time,Time Factors},
month = dec,
number = {6},
pages = {1142--52},
pmid = {19109917},
publisher = {Elsevier Ltd},
title = {{Probabilistic population codes for Bayesian decision making.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2742921\&tool=pmcentrez\&rendertype=abstract},
volume = {60},
year = {2008}
}
@article{Knight1986,
abstract = {Tags for efficient parallel execution},
address = {New York, New York, USA},
author = {Knight, Tom},
doi = {10.1145/319838.319854},
file = {:Users/jonas/Documents/Mendeley Desktop/Knight - 1986 - An architecture for mostly functional languages.pdf:pdf},
isbn = {0897912004},
journal = {Proceedings of the 1986 ACM conference on LISP and functional programming - LFP '86},
keywords = {hardware,lisp,parallel},
mendeley-tags = {hardware,lisp,parallel},
pages = {105--112},
publisher = {ACM Press},
title = {{An architecture for mostly functional languages}},
url = {http://portal.acm.org/citation.cfm?doid=319838.319854},
year = {1986}
}
@article{Brockhuis2002,
abstract = {This article overviews the basic terms and methodology approaches in economic analysis in medicine: cost-benefit analysis, cost-effectiveness analysis, cost-utility analysis and costminimisation analysis. Particular emphasis is put on nuclear medicine economic evaluation, e.g. FDG - PET studies, sestamibi breast cancer imaging and radioiodine therapy of hyperthyroidism.},
author = {Brockhuis, Bogna and Lass, Piotr and Popowski, Piotr and Scheffler, Justyna},
file = {:Users/jonas/Documents/Mendeley Desktop/Brockhuis et al. - 2002 - An introduction to economic analysis in medicine--the basics of methodology and chosen terms. Examples of results of evaluation in nuclear medicine.pdf:pdf},
issn = {1506-9680},
journal = {Nuclear medicine review. Central \& Eastern Europe : journal of Bulgarian, Czech, Macedonian, Polish, Romanian, Russian, Slovak, Yugoslav Societies of Nuclear Medicine and Ukrainian Society of Radiology},
month = jan,
number = {1},
pages = {55--9},
pmid = {14600950},
title = {{An introduction to economic analysis in medicine--the basics of methodology and chosen terms. Examples of results of evaluation in nuclear medicine.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19895923},
volume = {5},
year = {2002}
}
@article{Elad2010,
author = {Elad, Michael and Figueiredo, Mario a T},
doi = {10.1109/JPROC.2009.2037655},
file = {:Users/jonas/Documents/Mendeley Desktop/Elad, Figueiredo - 2010 - On the Role of Sparse and Redundant Representations in Image Processing.pdf:pdf},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
month = jun,
number = {6},
pages = {972--982},
title = {{On the Role of Sparse and Redundant Representations in Image Processing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5420029},
volume = {98},
year = {2010}
}
@article{Chen2003,
author = {Chen, Zhe},
file = {:Users/jonas/Documents/Mendeley Desktop/Bayesian Filtering From Kalman Filters to Particle Filters, and Beyond.pdf:pdf},
journal = {Statistics},
keywords = {Adaptive Systems Lab,McMaster University,review},
mendeley-tags = {review},
title = {{Bayesian Filtering : From Kalman Filters to Particle Filters , and Beyond}},
url = {http://www.damas.ift.ulaval.ca/\_seminar/filesA11/10.1.1.107.7415.pdf},
year = {2003}
}
@article{Knuth1984,
author = {Knuth, Donald E.},
doi = {10.1145/358027.358042},
file = {:Users/jonas/Documents/Mendeley Desktop/Knuth - 1984 - The complexity of songs.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = apr,
number = {4},
pages = {344--346},
title = {{The complexity of songs}},
url = {http://portal.acm.org/citation.cfm?doid=358027.358042},
volume = {27},
year = {1984}
}
@article{Dube2005,
author = {Dub\'{e}, Danny and Feeley, Marc},
doi = {10.1007/s10990-005-4877-4},
file = {:Users/jonas/Documents/Mendeley Desktop/Dub\'{e}, Feeley - 2005 - BIT A Very Compact Scheme System for Microcontrollers.pdf:pdf},
issn = {1388-3690},
journal = {Higher-Order and Symbolic Computation},
keywords = {hardware,scheme},
mendeley-tags = {hardware,scheme},
month = dec,
number = {3-4},
pages = {271--298},
title = {{BIT: A Very Compact Scheme System for Microcontrollers}},
url = {http://www.springerlink.com/index/10.1007/s10990-005-4877-4},
volume = {18},
year = {2005}
}
@article{Ma2008a,
abstract = {Systems neuroscience traditionally conceptualizes a population of spiking neurons as merely encoding the value of a stimulus. Yet, psychophysics has revealed that people take into account stimulus uncertainty when performing sensory or motor computations and do so in a nearly Bayes-optimal way. This suggests that neural populations do not encode just a single value but an entire probability distribution over the stimulus. Several such probabilistic codes have been proposed, including one that utilizes the structure of neural variability to enable simple neural implementations of probabilistic computations such as optimal cue integration. This approach provides a quantitative link between Bayes-optimal behaviors and specific neural operations. It allows for novel ways to evaluate probabilistic codes and for predictions for physiological population recordings.},
author = {Ma, Wei Ji and Beck, Jeffrey M and Pouget, Alexandre},
doi = {10.1016/j.conb.2008.07.004},
file = {:Users/jonas/Documents/Mendeley Desktop/Ma, Beck, Pouget - 2008 - Spiking networks for Bayesian inference and choice.pdf:pdf},
issn = {0959-4388},
journal = {Current opinion in neurobiology},
keywords = {Algorithms,Bayes Theorem,Choice Behavior,Choice Behavior: physiology,Cues,Humans,Models, Neurological,Models, Statistical,Nerve Net,Nerve Net: physiology,Neurons,Neurons: physiology},
month = apr,
number = {2},
pages = {217--22},
pmid = {18678253},
title = {{Spiking networks for Bayesian inference and choice.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18678253},
volume = {18},
year = {2008}
}
@unpublished{Haug2005,
address = {McLean},
author = {Haug, A J},
file = {:Users/jonas/Documents/Mendeley Desktop/05\_0211 (1).pdf:pdf},
institution = {MITRE},
keywords = {tutorial},
mendeley-tags = {tutorial},
title = {{A Tutorial on Bayesian Estimation and Tracking Techniques Applicable to Nonlinear and Non-Gaussian Processes}},
year = {2005}
}
@article{Wu2007,
abstract = {This paper presents the top 10 data mining algorithms identified by the IEEE International Conference on Data Mining (ICDM) in December 2006: C4.5, k-Means, SVM, Apriori, EM, PageRank, AdaBoost, kNN, Naive Bayes, and CART. These top 10 algorithms are among the most influential data mining algorithms in the research community. With each algorithm, we provide a description of the algorithm, discuss the impact of the algorithm, and review current and further research on the algorithm. These 10 algorithms cover classification, clustering, statistical learning, association analysis, and link mining, which are all among the most important topics in data mining research and development.},
author = {Wu, Xindong and Kumar, Vipin and {Ross Quinlan}, J. and Ghosh, Joydeep and Yang, Qiang and Motoda, Hiroshi and McLachlan, G.J. Geoffrey J and Ng, Angus and Liu, Bing and Yu, P.S. Philip S and Zhou, Zhi-Hua and Steinbach, Michael and Hand, David J and Steinberg, Dan and Others},
doi = {10.1007/s10115-007-0114-2},
file = {:Users/jonas/Documents/Mendeley Desktop/Wu et al. - 2007 - Top 10 algorithms in data mining.pdf:pdf},
isbn = {1011500701},
issn = {02191377},
journal = {Knowledge and Information Systems},
month = dec,
number = {1},
pages = {1--37},
publisher = {Springer},
title = {{Top 10 algorithms in data mining}},
url = {http://www.springerlink.com/index/10.1007/s10115-007-0114-2 http://www.springerlink.com/index/08x148q7kj035542.pdf},
volume = {14},
year = {2007}
}
@article{Andrieu2002,
abstract = {The original RBPF paper},
author = {Andrieu, Christophe and Doucet, Arnaud},
doi = {10.1111/1467-9868.00363},
file = {:Users/jonas/Documents/Mendeley Desktop/1467-9868.00363.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {RBPF,bayesian estimation,filtering,generalized linear time series,importance,sampling,sequential monte carlo sampling,state space model},
mendeley-tags = {RBPF},
month = oct,
number = {4},
pages = {827--836},
title = {{Particle filtering for partially observed Gaussian state space models}},
url = {http://doi.wiley.com/10.1111/1467-9868.00363},
volume = {64},
year = {2002}
}
@article{Ma2010a,
abstract = {The juxtaposition of established signal detection theory models of perception and more recent claims about the encoding of uncertainty in perception is a rich source of confusion. Are the latter simply a rehash of the former? Here, we make an attempt to distinguish precisely between optimal and probabilistic computation. In optimal computation, the observer minimizes the expected cost under a posterior probability distribution. In probabilistic computation, the observer uses higher moments of the likelihood function of the stimulus on a trial-by-trial basis. Computation can be optimal without being probabilistic, and vice versa. Most signal detection theory models describe optimal computation. Behavioral data only provide evidence for a neural representation of uncertainty if they are best described by a model of probabilistic computation. We argue that single-neuron activity sometimes suffices for optimal computation, but never for probabilistic computation. A population code is needed instead. Not every population code is equally suitable, because nuisance parameters have to be marginalized out. This problem is solved by Poisson-like, but not by Gaussian variability. Finally, we build a dictionary between signal detection theory quantities and Poisson-like population quantities.},
author = {Ma, Wei Ji},
doi = {10.1016/j.visres.2010.08.035},
file = {:Users/jonas/Documents/Mendeley Desktop/Ma - 2010 - Signal detection theory, uncertainty, and Poisson-like population codes.pdf:pdf},
issn = {1878-5646},
journal = {Vision research},
keywords = {signal detection theory},
month = oct,
number = {22},
pages = {2308--19},
pmid = {20828581},
publisher = {Elsevier Ltd},
title = {{Signal detection theory, uncertainty, and Poisson-like population codes.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20828581},
volume = {50},
year = {2010}
}
@article{Fang2005,
abstract = {Recent theory has predicted a superlens that is capable of producing sub-diffraction-limited images. This superlens would allow the recovery of evanescent waves in an image via the excitation of surface plasmons. Using silver as a natural optical superlens, we demonstrated sub-diffraction-limited imaging with 60-nanometer half-pitch resolution, or one-sixth of the illumination wavelength. By proper design of the working wavelength and the thickness of silver that allows access to a broad spectrum of subwavelength features, we also showed that arbitrary nanostructures can be imaged with good fidelity. The optical superlens promises exciting avenues to nanoscale optical imaging and ultrasmall optoelectronic devices.},
author = {Fang, Nicholas and Lee, Hyesog and Sun, Cheng and Zhang, Xiang},
doi = {10.1126/science.1108759},
file = {:Users/jonas/Documents/Mendeley Desktop/Fang et al. - 2005 - Sub-diffraction-limited optical imaging with a silver superlens.pdf:pdf},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
keywords = {diffraction limit},
mendeley-tags = {diffraction limit},
month = apr,
number = {5721},
pages = {534--7},
pmid = {15845849},
title = {{Sub-diffraction-limited optical imaging with a silver superlens.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15845849},
volume = {308},
year = {2005}
}
@book{Andrew2004,
author = {Andrew, Alex M.},
booktitle = {Robotica},
doi = {10.1017/S026357470426043X},
file = {:Users/jonas/Documents/Mendeley Desktop/Andrew - 2004 - INFORMATION THEORY, INFERENCE, AND LEARNING ALGORITHMS.pdf:pdf},
isbn = {0521642981},
issn = {0263-5747},
month = jun,
number = {3},
pages = {348--349},
title = {{INFORMATION THEORY, INFERENCE, AND LEARNING ALGORITHMS}},
url = {http://www.journals.cambridge.org/abstract\_S026357470426043X},
volume = {22},
year = {2004}
}
@article{Chen2011b,
author = {Chen, Tianshi and Schon, Thomas B. and Ohlsson, Henrik and Ljung, Lennart},
doi = {10.1109/TSP.2010.2091639},
file = {:Users/jonas/Documents/Mendeley Desktop/Decentralized Particle Filter With Arbitrary State Decomposition.pdf:pdf},
issn = {1053-587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {DPF,RBPF},
mendeley-tags = {DPF,RBPF},
month = feb,
number = {2},
pages = {465--478},
title = {{Decentralized Particle Filter With Arbitrary State Decomposition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5629376},
volume = {59},
year = {2011}
}
@article{Beck2007b,
abstract = {Many experiments have shown that human behavior is nearly Bayes optimal in a variety of tasks. This implies that neural activity is capable of representing both the value and uncertainty of a stimulus, if not an entire probability distribution, and can also combine such representations in an optimal manner. Moreover, this computation can be performed optimally despite the fact that observed neural activity is highly variable (noisy) on a trial-by-trial basis. Here, we argue that this observed variability is actually expected in a neural system which represents uncertainty. Specifically, we note that Bayes' rule implies that a variable pattern of activity provides a natural representation of a probability distribution, and that the specific form of neural variability can be structured so that optimal inference can be executed using simple operations available to neural circuits.},
author = {Beck, JM and Ma, WJ and Latham, PE and Pouget, A},
file = {:Users/jonas/Documents/Mendeley Desktop/Beck et al. - 2007 - Probabilistic population codes and the exponential family of distributions.pdf:pdf},
institution = {Department of Brain and Cognitive Sciences, University of Rochester, Rochester, NY, USA.},
journal = {Progress in Brain Research},
number = {585},
pages = {509--519},
publisher = {Elsevier},
series = {Progress in Brain Research},
title = {{Probabilistic population codes and the exponential family of distributions}},
url = {http://eprints.ucl.ac.uk/48451/},
volume = {165},
year = {2007}
}
@incollection{Andrieu2008,
address = {Berlin, Heidelberg},
author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
booktitle = {Monte Carlo and Quasi-Monte Carlo Methods},
doi = {10.1007/978-3-642-04107-5\_3},
editor = {{L' Ecuyer}, Pierre and Owen, Art B.},
file = {:Users/jonas/Documents/Mendeley Desktop/Andrieu, Doucet, Holenstein - 2008 - Particle Markov Chain Monte Carlo for Efficient Numerical Simulation.pdf:pdf},
isbn = {978-3-642-04106-8},
keywords = {mcmc,smc},
number = {Mcmc},
publisher = {Springer Berlin Heidelberg},
title = {{Particle Markov Chain Monte Carlo for Efficient Numerical Simulation}},
url = {http://www.springerlink.com/index/10.1007/978-3-642-04107-5},
year = {2008}
}
@article{Pan2009,
abstract = {Despite major advances in x-ray sources, detector arrays, gantry mechanical design and especially computer performance, one component of computed tomography (CT) scanners has remained virtually constant for the past 25 years-the reconstruction algorithm. Fundamental advances have been made in the solution of inverse problems, especially tomographic reconstruction, but these works have not been translated into clinical and related practice. The reasons are not obvious and seldom discussed. This review seeks to examine the reasons for this discrepancy and provides recommendations on how it can be resolved. We take the example of field of compressive sensing (CS), summarizing this new area of research from the eyes of practical medical physicists and explaining the disconnection between theoretical and application-oriented research. Using a few issues specific to CT, which engineers have addressed in very specific ways, we try to distill the mathematical problem underlying each of these issues with the hope of demonstrating that there are interesting mathematical problems of general importance that can result from in depth analysis of specific issues. We then sketch some unconventional CT-imaging designs that have the potential to impact on CT applications, if the link between applied mathematicians and engineers/physicists were stronger. Finally, we close with some observations on how the link could be strengthened. There is, we believe, an important opportunity to rapidly improve the performance of CT and related tomographic imaging techniques by addressing these issues.},
author = {Pan, Xiaochuan and Sidky, Emil Y and Vannier, Michael},
doi = {10.1088/0266-5611/25/12/123009},
file = {:Users/jonas/Documents/Mendeley Desktop/Pan, Sidky, Vannier - 2009 - Why do commercial CT scanners still employ traditional, filtered back-projection for image reconstruction.pdf:pdf},
issn = {0266-5611},
journal = {Inverse problems},
month = jan,
number = {12},
pages = {1230009},
pmid = {20376330},
title = {{Why do commercial CT scanners still employ traditional, filtered back-projection for image reconstruction?}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2849113\&tool=pmcentrez\&rendertype=abstract},
volume = {25},
year = {2009}
}
@article{Whiteley2010,
abstract = {Switching state-space models (SSSM) are a very popular class of time series models that have found many applications in statistics, econometrics and advanced signal processing. Bayesian inference for these models typically relies on Markov chain Monte Carlo (MCMC) techniques. However, even sophisticated MCMC methods dedicated to SSSM can prove quite inefficient as they update potentially strongly correlated discrete-valued latent variables one-at-a-time (Carter and Kohn, 1996; Gerlach et al., 2000; Giordani and Kohn, 2008). Particle Markov chain Monte Carlo (PMCMC) methods are a recently developed class of MCMC algorithms which use particle filters to build efficient proposal distributions in high-dimensions (Andrieu et al., 2010). The existing PMCMC methods of Andrieu et al. (2010) are applicable to SSSM, but are restricted to employing standard particle filtering techniques. Yet, in the context of discrete-valued latent variables, specialised particle techniques have been developed which can outperform by up to an order of magnitude standard methods (Fearnhead, 1998; Fearnhead and Clifford, 2003; Fearnhead, 2004). In this paper we develop a novel class of PMCMC methods relying on these very efficient particle algorithms. We establish the theoretical validy of this new generic methodology referred to as discrete PMCMC and demonstrate it on a variety of examples including a multiple change-points model for well-log data and a model for U.S./U.K. exchange rate data. Discrete PMCMC algorithms are shown to outperform experimentally state-of-the-art MCMC techniques for a fixed computational complexity. Additionally they can be easily parallelized (Lee et al., 2010) which allows further substantial gains.},
archivePrefix = {arXiv},
arxivId = {1011.2437},
author = {Whiteley, Nick and Andrieu, Christophe and Doucet, Arnaud},
eprint = {1011.2437},
file = {:Users/jonas/Documents/Mendeley Desktop/Efficient Bayesian Inference for Switching State-Space Models using Discrete Particle Markov Chain Monte Carlo Methods.pdf:pdf},
keywords = {PMCMC,bayesian inference,markov chain monte carlo,optimal resampling,particle filters,quential monte carlo,se-,switching state-space models},
mendeley-tags = {PMCMC},
month = nov,
pages = {1--26},
title = {{Efficient Bayesian Inference for Switching State-Space Models using Discrete Particle Markov Chain Monte Carlo Methods}},
url = {http://arxiv.org/abs/1011.2437},
year = {2010}
}
@incollection{Douglas1999,
author = {Douglas, Scott C},
chapter = {18},
file = {:Users/jonas/Documents/Mendeley Desktop/Adaptive Filtering Review.pdf:pdf},
title = {{Introduction to Adaptive Filters}},
url = {http://cdsweb.cern.ch/record/105137},
year = {1999}
}
@article{Blei2010a,
author = {Blei, David and Carin, Lawrence and Dunson, David},
doi = {10.1109/MSP.2010.938079},
file = {:Users/jonas/Documents/Mendeley Desktop/Blei, Carin, Dunson - 2010 - Probabilistic Topic Models.pdf:pdf},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
keywords = {bayes,mcmc,topic models},
mendeley-tags = {bayes,mcmc,topic models},
month = nov,
number = {November},
pages = {55--65},
title = {{Probabilistic Topic Models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5563111},
year = {2010}
}
@article{Scheel2009,
author = {Scheel, Stefan},
doi = {10.1080/09500340802331849},
file = {:Users/jonas/Documents/Mendeley Desktop//Scheel - 2009 - Single-photon sources–an introduction.pdf:pdf;:Users/jonas/Documents/Mendeley Desktop//Scheel - 2009 - Single-photon sources–an introduction.htm:htm},
issn = {0950-0340},
journal = {Journal of Modern Optics},
keywords = {cavity qed,cluster states,linear optical quantum computing,photons,review,sensing,single photons},
mendeley-tags = {photons,review,sensing},
month = jan,
number = {2-3},
pages = {141--160},
title = {{Single-photon sources–an introduction}},
url = {http://www.tandfonline.com/doi/abs/10.1080/09500340802331849},
volume = {56},
year = {2009}
}
@article{Bergeron2007,
author = {Bergeron, Etienne and Feeley, Marc and David, Jean Pierre},
doi = {10.1109/NEWCAS.2007.4487978},
file = {:Users/jonas/Documents/Mendeley Desktop/Bergeron, Feeley, David - 2007 - Toward on-chip JIT synthesis on Xilinx VirtexII-Pro FPGAs.pdf:pdf},
isbn = {978-1-4244-1163-4},
journal = {2007 IEEE Northeast Workshop on Circuits and Systems},
month = aug,
pages = {642--645},
publisher = {Ieee},
title = {{Toward on-chip JIT synthesis on Xilinx VirtexII-Pro FPGAs}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4487978},
year = {2007}
}
@article{Neal2003,
author = {Neal, R.M.},
doi = {10.1214/aos/1056562461},
file = {:Users/jonas/Documents/Mendeley Desktop/Neal - 2003 - Slice sampling.pdf:pdf},
issn = {0090-5364},
journal = {Annals of Statistics},
keywords = {mcmc,slice},
mendeley-tags = {mcmc,slice},
month = jun,
number = {3},
pages = {705--741},
publisher = {JSTOR},
title = {{Slice sampling}},
url = {http://projecteuclid.org/Dienst/getRecord?id=euclid.aos/1056562461/ http://www.jstor.org/stable/10.2307/3448413},
volume = {31},
year = {2003}
}
@inproceedings{Bernardo2003,
author = {Bernardo, JM and Bayarri, MJ and Berger, JO and Dawid, AP and Heckerman, D. and Smith, AFM and West, M. and Others},
booktitle = {Bayesian statistics 7: proceedings of the seventh Valencia International Meeting, June 2-6, 2002},
file = {:Users/jonas/Documents/Mendeley Desktop/Bernardo et al. - 2003 - A Nonparametric Bayesian Approach to Inverse Problems.pdf:pdf},
keywords = {bayes,inverse problems,nonparametric bayes},
mendeley-tags = {bayes,inverse problems,nonparametric bayes},
number = {2},
pages = {403},
publisher = {Oxford University Press, USA},
title = {{A Nonparametric Bayesian Approach to Inverse Problems}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:A+Nonparametric+Bayesian+Approach+to+Inverse+Problems\#0},
year = {2003}
}
@incollection{Whiteley2010a,
annote = {Good review of the APF, although does leave a bunch of important crap out},
author = {Whiteley, Nick and Johansen, AM},
booktitle = {Inference and Learning in Dynamic \ldots},
chapter = {3},
editor = {Barber and Cemgil and Chiappa},
file = {:Users/jonas/Documents/Mendeley Desktop/Recent Developments in Auxiliary Particle Filtering.pdf:pdf},
keywords = {APF,review},
mendeley-tags = {review},
pages = {1--33},
publisher = {Cambridge University Press},
title = {{Recent Developments in Auxiliary Particle Filtering}},
url = {http://www.stats.bristol.ac.uk/~manpw/apf\_chapter.pdf},
year = {2010}
}
@article{Deneve2008b,
abstract = {In the companion letter in this issue ("Bayesian Spiking Neurons I: Inference"), we showed that the dynamics of spiking neurons can be interpreted as a form of Bayesian integration, accumulating evidence over time about events in the external world or the body. We proceed to develop a theory of Bayesian learning in spiking neural networks, where the neurons learn to recognize temporal dynamics of their synaptic inputs. Meanwhile, successive layers of neurons learn hierarchical causal models for the sensory input. The corresponding learning rule is local, spike-time dependent, and highly nonlinear. This approach provides a principled description of spiking and plasticity rules maximizing information transfer, while limiting the number of costly spikes, between successive layers of neurons.},
author = {Deneve, Sophie},
doi = {10.1162/neco.2008.20.1.118},
file = {:Users/jonas/Documents/Mendeley Desktop/Deneve - 2008 - Bayesian spiking neurons II learning.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Action Potentials,Action Potentials: physiology,Algorithms,Animals,Bayes Theorem,Central Nervous System,Central Nervous System: physiology,Computer Simulation,Humans,Learning,Learning: physiology,Markov Chains,Nerve Net,Nerve Net: physiology,Neural Networks (Computer),Neuronal Plasticity,Neuronal Plasticity: physiology,Neurons,Neurons: physiology,Nonlinear Dynamics,Poisson Distribution,Synaptic Transmission,Synaptic Transmission: physiology,Time Perception,Time Perception: physiology},
month = jan,
number = {1},
pages = {118--45},
pmid = {18045003},
title = {{Bayesian spiking neurons II: learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18045003},
volume = {20},
year = {2008}
}
@article{Maugis2009,
abstract = {This article is concerned with variable selection for cluster analysis. The problem is regarded as a model selection problem in the model-based cluster analysis context. A model generalizing the model of Raftery and Dean (2006, Journal of the American Statistical Association 101, 168-178) is proposed to specify the role of each variable. This model does not need any prior assumptions about the linear link between the selected and discarded variables. Models are compared with Bayesian information criterion. Variable role is obtained through an algorithm embedding two backward stepwise algorithms for variable selection for clustering and linear regression. The model identifiability is established and the consistency of the resulting criterion is proved under regularity conditions. Numerical experiments on simulated datasets and a genomic application highlight the interest of the procedure.},
author = {Maugis, Cathy and Celeux, Gilles and Martin-Magniette, Marie-Laure},
doi = {10.1111/j.1541-0420.2008.01160.x},
file = {:Users/jonas/Documents/Mendeley Desktop/Maugis, Celeux, Martin-Magniette - 2009 - Variable selection for clustering with Gaussian mixture models.pdf:pdf},
issn = {1541-0420},
journal = {Biometrics},
keywords = {Biometry,Biometry: methods,Clinical Trials as Topic,Cluster Analysis,Computer Simulation,Data Interpretation,Effect Modifiers (Epidemiology),Models,Normal Distribution,Proportional Hazards Models,Regression Analysis,Statistical,feature selection},
mendeley-tags = {feature selection},
month = sep,
number = {3},
pages = {701--9},
pmid = {19210744},
title = {{Variable selection for clustering with Gaussian mixture models.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19210744},
volume = {65},
year = {2009}
}
@article{Pouget2008a,
author = {Pouget, Alexandre and DeAngelis, Gregory C},
doi = {10.1038/nn1208-1371},
file = {:Users/jonas/Documents/Mendeley Desktop/Pouget, DeAngelis - 2008 - Paying attention to correlated neural activity.pdf:pdf},
issn = {1546-1726},
journal = {Nature neuroscience},
keywords = {Action Potentials,Action Potentials: physiology,Animals,Attention,Attention: physiology,Cell Communication,Cell Communication: physiology,Humans,Neurons,Neurons: physiology},
month = dec,
number = {12},
pages = {1371--2},
pmid = {19023342},
title = {{Paying attention to correlated neural activity.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19023342},
volume = {11},
year = {2008}
}
@article{Ram2006,
abstract = {Rayleigh's criterion is extensively used in optical microscopy for determining the resolution of microscopes. This criterion imposes a resolution limit that has long been held as an impediment for studying nanoscale biological phenomenon through an optical microscope. However, it is well known that Rayleigh's criterion is based on intuitive notions. For example, Rayleigh's criterion is formulated in a deterministic setting that neglects the photon statistics of the acquired data. Hence it does not take into account the number of detected photons, which, in turn, raises concern over the use of Rayleigh's criterion in photon-counting techniques such as single-molecule microscopy. Here, we re-examine the resolution problem by adopting a stochastic framework and present a resolution measure that overcomes the limitations of Rayleigh's criterion. This resolution measure predicts that the resolution of optical microscopes is not limited and that it can be improved by increasing the number of detected photons. Experimental verification of the resolution measure is carried out by imaging single-molecule pairs with different distances of separation. The resolution measure provides a quantitative tool for designing and evaluating single-molecule experiments that probe biomolecular interactions.},
author = {Ram, Sripad and Ward, E Sally and Ober, Raimund J},
doi = {10.1073/pnas.0508047103},
file = {:Users/jonas/Documents/Mendeley Desktop/Ram, Ward, Ober - 2006 - Beyond Rayleigh's criterion a resolution measure with application to single-molecule microscopy.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Calibration,Microscopy,Microscopy: standards,Photons},
month = mar,
number = {12},
pages = {4457--62},
pmid = {16537357},
title = {{Beyond Rayleigh's criterion: a resolution measure with application to single-molecule microscopy.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1450193\&tool=pmcentrez\&rendertype=abstract},
volume = {103},
year = {2006}
}
@article{Andrieu2010,
author = {Andrieu, Christophe and Doucet, A},
file = {:Users/jonas/Documents/Mendeley Desktop/Andrieu, Doucet - 2010 - Particle Markov chain Monte Carlo methods.pdf:pdf},
journal = {Journal of the Royal},
keywords = {bayesian inference,dirichlet process mixtures,markov chain monte carlo,mcmc,methods,sequential monte carlo methods,smc,state space models},
mendeley-tags = {mcmc,smc},
pages = {1--33},
title = {{Particle Markov chain Monte Carlo methods}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2009.00736.x/full},
year = {2010}
}
@article{Avillac2005a,
abstract = {The ventral intraparietal area (VIP) receives converging inputs from visual, somatosensory, auditory and vestibular systems that use diverse reference frames to encode sensory information. A key issue is how VIP combines those inputs together. We mapped the visual and tactile receptive fields of multimodal VIP neurons in macaque monkeys trained to gaze at three different stationary targets. Tactile receptive fields were found to be encoded into a single somatotopic, or head-centered, reference frame, whereas visual receptive fields were widely distributed between eye- to head-centered coordinates. These findings are inconsistent with a remapping of all sensory modalities in a common frame of reference. Instead, they support an alternative model of multisensory integration based on multidirectional sensory predictions (such as predicting the location of a visual stimulus given where it is felt on the skin and vice versa). This approach can also explain related findings in other multimodal areas.},
author = {Avillac, Marie and Den\`{e}ve, Sophie and Olivier, Etienne and Pouget, Alexandre and Duhamel, Jean-Ren\'{e}},
doi = {10.1038/nn1480},
file = {:Users/jonas/Documents/Mendeley Desktop/Avillac et al. - 2005 - Reference frames for representing visual and tactile locations in parietal cortex.pdf:pdf},
issn = {1097-6256},
journal = {Nature neuroscience},
keywords = {Animals,Brain Mapping,Brain Mapping: methods,Electrophysiology,Eye Movements,Eye Movements: physiology,Fixation, Ocular,Fixation, Ocular: physiology,Head,Head: physiology,Macaca mulatta,Models, Neurological,Neurons,Neurons: physiology,Ocular Physiological Phenomena,Parietal Lobe,Parietal Lobe: physiology,Photic Stimulation,Physical Stimulation,Reaction Time,Reaction Time: physiology,Touch,Touch: physiology,Vision, Ocular,Vision, Ocular: physiology},
month = jul,
number = {7},
pages = {941--9},
pmid = {15951810},
title = {{Reference frames for representing visual and tactile locations in parietal cortex.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15951810},
volume = {8},
year = {2005}
}
@article{Andrieu2010b,
author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
doi = {10.1111/j.1467-9868.2009.00736.x},
file = {:Users/jonas/Documents/Mendeley Desktop/Andrieu, Doucet, Holenstein - 2010 - Particle Markov chain Monte Carlo methods.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {PMCMC,bayesian inference,markov chain monte carlo,mcmc,methods,sequential monte carlo,smc,state space models},
mendeley-tags = {mcmc,PMCMC,smc},
month = jun,
number = {3},
pages = {269--342},
title = {{Particle Markov chain Monte Carlo methods}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2009.00736.x},
volume = {72},
year = {2010}
}
@article{Beck2007c,
abstract = {From first principles, we derive a quadratic nonlinear, first-order dynamical system capable of performing exact Bayes-Markov inferences for a wide class of biologically plausible stimulus-dependent patterns of activity while simultaneously providing an online estimate of model performance. This is accomplished by constructing a dynamical system that has solutions proportional to the probability distribution over the stimulus space, but with a constant of proportionality adjusted to provide a local estimate of the probability of the recent observations of stimulus-dependent activity-given model parameters. Next, we transform this exact equation to generate nonlinear equations for the exact evolution of log likelihood and log-likelihood ratios and show that when the input has low amplitude, linear rate models for both the likelihood and the log-likelihood functions follow naturally from these equations. We use these four explicit representations of the probability distribution to argue that, in contrast to the arguments of previous work, the dynamical system for the exact evolution of the likelihood (as opposed to the log likelihood or log-likelihood ratios) not only can be mapped onto a biologically plausible network but is also more consistent with physiological observations.},
author = {Beck, Jeffrey M and Pouget, Alexandre},
doi = {10.1162/neco.2007.19.5.1344},
file = {:Users/jonas/Documents/Mendeley Desktop/Beck, Pouget - 2007 - Exact inferences in a neural implementation of a hidden Markov model.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Animals,Computer Simulation,Markov Chains,Models, Neurological,Neural Networks (Computer),Neurons,Neurons: physiology,Nonlinear Dynamics},
month = may,
number = {5},
pages = {1344--61},
pmid = {17381269},
title = {{Exact inferences in a neural implementation of a hidden Markov model.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17381269},
volume = {19},
year = {2007}
}
@article{numpy,
author = {van der Walt, Stéfan and Colbert, S Chris and Varoquaux, Gaël},
doi = {10.1109/MCSE.2011.37},
issn = {1521-9615},
journal = {Computing in Science \& Engineering},
month = mar,
number = {2},
pages = {22--30},
title = {{The NumPy Array: A Structure for Efficient Numerical Computation}},
url = {http://www.numpy.org},
volume = {13},
year = {2011}
}
@article{Germain2006c,
address = {New York, New York, USA},
author = {Germain, Guillaume},
doi = {10.1145/1159789.1159795},
file = {:Users/jonas/Documents/Mendeley Desktop/Germain - 2006 - Concurrency oriented programming in termite scheme(3).pdf:pdf},
isbn = {1595934901},
journal = {Proceedings of the 2006 ACM SIGPLAN workshop on Erlang - ERLANG '06},
keywords = {contin-,distributed computing,erlang,lisp,parallel,scheme},
mendeley-tags = {parallel,scheme},
pages = {20},
publisher = {ACM Press},
title = {{Concurrency oriented programming in termite scheme}},
url = {http://portal.acm.org/citation.cfm?doid=1159789.1159795},
year = {2006}
}
@article{Wilms2009,
abstract = {Two improved genetically encoded calcium indicators-based on structure-guided sensor design or on precise subcellular targeting to presynaptic boutons-allow single spikes to be detected in genetically defined populations of neurons and synapses in vivo.},
author = {Wilms, Christian D and H\"{a}usser, Michael},
doi = {10.1038/nmeth1209-871},
file = {:Users/jonas/Documents/Mendeley Desktop/Wilms, H\"{a}usser - 2009 - Lighting up neural networks using a new generation of genetically encoded calcium sensors.pdf:pdf},
issn = {1548-7105},
journal = {Nature methods},
keywords = {Action Potentials,Animals,Calcium,Calcium: metabolism,Mice,Nerve Net,Subcellular Fractions,Subcellular Fractions: metabolism,calcium sensor},
mendeley-tags = {calcium sensor},
month = dec,
number = {12},
pages = {871--2},
pmid = {19935839},
publisher = {Nature Publishing Group},
title = {{Lighting up neural networks using a new generation of genetically encoded calcium sensors.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19935839},
volume = {6},
year = {2009}
}
@phdthesis{Andrieu2009,
author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
doi = {10.1111/j.1467-9868.2009.00736.x},
file = {:Users/jonas/Documents/Mendeley Desktop/Andrieu, Doucet, Holenstein - 2009 - Particle Markov chain Monte Carlo methods.pdf:pdf},
issn = {13697412},
keywords = {dpmm,mcmc,smc},
mendeley-tags = {dpmm,mcmc,smc},
month = jun,
number = {3},
title = {{Particle Markov chain Monte Carlo methods}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2009.00736.x},
volume = {72},
year = {2009}
}
@article{Fomitchev2004,
address = {New York, New York, USA},
author = {Fomitchev, Mikhail and Ruppert, Eric},
doi = {10.1145/1011767.1011776},
file = {:Users/jonas/Documents/Mendeley Desktop/Fomitchev, Ruppert - 2004 - Lock-free linked lists and skip lists.pdf:pdf},
isbn = {1581138024},
journal = {Proceedings of the twenty-third annual ACM symposium on Principles of distributed computing - PODC '04},
keywords = {amortized analysis,analysis,distributed,efficient,fault-tolerant,linked list,lock-free,skip list},
pages = {50},
publisher = {ACM Press},
title = {{Lock-free linked lists and skip lists}},
url = {http://portal.acm.org/citation.cfm?doid=1011767.1011776},
year = {2004}
}
@article{Pouget2003a,
abstract = {In the vertebrate nervous system, sensory stimuli are typically encoded through the concerted activity of large populations of neurons. Classically, these patterns of activity have been treated as encoding the value of the stimulus (e.g., the orientation of a contour), and computation has been formalized in terms of function approximation. More recently, there have been several suggestions that neural computation is akin to a Bayesian inference process, with population activity patterns representing uncertainty about stimuli in the form of probability distributions (e.g., the probability density function over the orientation of a contour). This paper reviews both approaches, with a particular emphasis on the latter, which we see as a very promising framework for future modeling and experimental work.},
author = {Pouget, Alexandre and Dayan, Peter and Zemel, Richard S},
doi = {10.1146/annurev.neuro.26.041002.131112},
file = {:Users/jonas/Documents/Mendeley Desktop/Pouget, Dayan, Zemel - 2003 - Inference and computation with population codes.pdf:pdf},
issn = {0147-006X},
journal = {Annual review of neuroscience},
keywords = {Animals,Bayes Theorem,Humans,Models, Neurological,Motivation,Nerve Net,Nervous System Physiological Phenomena,Neural Networks (Computer),Neurons,Neurons: classification,Neurons: physiology,Orientation,Psychophysics},
month = jan,
pages = {381--410},
pmid = {12704222},
title = {{Inference and computation with population codes.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12704222},
volume = {26},
year = {2003}
}
@article{Starck2010,
author = {Starck, J.L. and Bobin, J.},
file = {:Users/jonas/Documents/Mendeley Desktop/Starck, Bobin - 2010 - Astronomical data analysis and sparsity From wavelets to compressed sensing.pdf:pdf},
journal = {Proceedings of the IEEE},
keywords = {astronomical data analysis,astronomy,compressed sensing,curvelet,restoration,wavelet,wavelets},
mendeley-tags = {astronomy,compressed sensing,wavelets},
number = {6},
pages = {1021--1030},
publisher = {IEEE},
title = {{Astronomical data analysis and sparsity: From wavelets to compressed sensing}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5299269},
volume = {98},
year = {2010}
}
@article{Colgin2008,
abstract = {Memory interference is a common cause of forgetting. Interference is a byproduct of the need to balance the formation of well-differentiated representations against the ability to retrieve memories from cues that are not identical to the original experience. How the brain accomplishes this has remained elusive. Here we review how insights can be gained from studies of an apparently unrelated phenomenon in the rodent brain--remapping in hippocampal place cells. Remapping refers to the formation of distinct representations in populations of place cells after minor changes in inputs to the hippocampus. Remapping might reflect processes involved generally in decorrelation of overlapping signals. These processes might be crucial for storing large numbers of similar experiences with only minimal interference.},
author = {Colgin, Laura Lee and Moser, Edvard I and Moser, May-Britt},
doi = {10.1016/j.tins.2008.06.008},
file = {:Users/jonas/Documents/Mendeley Desktop/Colgin, Moser, Moser - 2008 - Understanding memory through hippocampal remapping.pdf:pdf},
issn = {0166-2236},
journal = {Trends in neurosciences},
keywords = {Animals,Association Learning,Association Learning: physiology,Brain Mapping,Evoked Potentials,Evoked Potentials: physiology,Hippocampus,Hippocampus: physiology,Humans,Memory,Memory: physiology,Neuronal Plasticity,Neuronal Plasticity: physiology,Space Perception,Space Perception: physiology,Spatial Behavior,Spatial Behavior: physiology},
month = sep,
number = {9},
pages = {469--77},
pmid = {18687478},
title = {{Understanding memory through hippocampal remapping.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18687478},
volume = {31},
year = {2008}
}
@article{Neal2012,
author = {Neal, Radford M},
file = {:Users/jonas/Documents/Mendeley Desktop/Neal - 2012 - How to View an MCMC Simulation as a Permutation, with Applications to Parallel Simulation and Improved Importance Sampling.pdf:pdf},
keywords = {inference,mcmc,theory},
mendeley-tags = {inference,mcmc,theory},
number = {1201},
pages = {1--42},
title = {{How to View an MCMC Simulation as a Permutation, with Applications to Parallel Simulation and Improved Importance Sampling}},
year = {2012}
}
@article{Colby1996a,
author = {Colby, Carol L.},
doi = {10.1016/S0926-6410(96)00046-8},
file = {:Users/jonas/Documents/Mendeley Desktop/Colby - 1996 - Spatial representations for action in parietal cortex.pdf:pdf},
journal = {Cognitive Brain Research},
month = dec,
number = {1-2},
pages = {105--115},
title = {{Spatial representations for action in parietal cortex}},
volume = {5},
year = {1996}
}
@article{Series2004a,
author = {Seri\`{e}s, Peggy and Latham, Peter E and Pouget, Alexandre},
doi = {10.1038/nn1321},
file = {:Users/jonas/Documents/Mendeley Desktop/Seri\`{e}s, Latham, Pouget - 2004 - Tuning curve sharpening for orientation selectivity coding efficiency and the impact of correlations.pdf:pdf},
issn = {1097-6256},
journal = {Nature Neuroscience},
month = sep,
number = {10},
pages = {1129--1135},
title = {{Tuning curve sharpening for orientation selectivity: coding efficiency and the impact of correlations}},
url = {http://www.nature.com/doifinder/10.1038/nn1321},
volume = {7},
year = {2004}
}
@article{Cevher2010a,
author = {Cevher, Volkan and Indyk, Piotr and Carin, Lawrence and Baraniuk, Richard},
doi = {10.1109/MSP.2010.938029},
file = {:Users/jonas/Documents/Mendeley Desktop/Cevher et al. - 2010 - Sparse Signal Recovery and Acquisition with Graphical Models.pdf:pdf},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
keywords = {compressed sensing,graphical models,sparsity},
mendeley-tags = {compressed sensing,graphical models,sparsity},
month = nov,
number = {6},
pages = {92--103},
publisher = {IEEE},
title = {{Sparse Signal Recovery and Acquisition with Graphical Models}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5563109 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5563109},
volume = {27},
year = {2010}
}
@article{Donoho2010,
author = {Donoho, David L and Tanner, Jared},
doi = {10.1109/JPROC.2010.2045630},
file = {:Users/jonas/Documents/Mendeley Desktop/Donoho, Tanner - 2010 - Precise Undersampling Theorems.pdf:pdf},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
keywords = {1 -minimization,bandlimited measurements,compressed sensing,random measurements,random polytopes,superresolution,undersampling,universality of matrix ensembles},
month = jun,
number = {6},
pages = {913--924},
title = {{Precise Undersampling Theorems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5458001},
volume = {98},
year = {2010}
}
@article{Bajwa2010,
author = {Bajwa, Waheed U and Haupt, Jarvis and Sayeed, Akbar M and Nowak, Robert},
doi = {10.1109/JPROC.2010.2042415},
file = {:Users/jonas/Documents/Mendeley Desktop/Bajwa et al. - 2010 - Compressed Channel Sensing A New Approach to Estimating Sparse Multipath Channels.pdf:pdf},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
keywords = {channel estimation,compressed sensing,dantzig,least-squares estimation,modeling,multipath,multiple-antenna channels,orthogonal frequency division multiplexing,selector,signal processing,sparse channel,spread spectrum,training-based estimation},
mendeley-tags = {compressed sensing,multipath,signal processing},
month = jun,
number = {6},
pages = {1058--1076},
title = {{Compressed Channel Sensing: A New Approach to Estimating Sparse Multipath Channels}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5454399},
volume = {98},
year = {2010}
}
@techreport{Dahl,
abstract = {The Gibbs sampler is the standard Markov chain Monte Carlo sampler for drawing samples from the posterior distribution of conjugate Dirichlet process mixture models. Researchers have noticed the Gibbs sampler’s tendency to get stuck in local modes and, thus, poorly explore the posterior distribution. Jain and Neal (2004) proposed a merge-split sampler in which a naive random split is sweetened by a series of restricted Gibbs scans, where the number of Gibbs scans is a tuning parameter that must be supplied by the user. In this work, I propose an alternative merge-split sampler borrowing ideas from sequential importance sampling. My sampler proposes splits by sequentially allocating observations to one of two split components using allocation probabilities that are conditional on previously allocated data. The algorithm does not require further sweetening and is, hence, computa- tionally efficient. In addition, no tuning parameter needs to be chosen. While the conditional allocation of observations is similar to sequential importance sampling, the output from the sampler has the correct stationary distribution due to the use of the Metropolis-Hastings ratio. The computational efficiency of my sequentially-allocated merge-split (SAMS) sampler is compared to Jain and Neal’s sampler using various values for the tuning parameter. Compar- isons are made in terms of autocorrelation times for four univariate summaries of the Markov chains taken at fixed time intervals. In four examples involving different models and datasets, I show that my merge-split sampler usually performs substantially better—in some cases, two to five times faster—than existing methods, and never performs worse.},
author = {Dahl, David B},
file = {:Users/jonas/Documents/Mendeley Desktop/Dahl - Unknown - An Improved Merge-Split Sampler for Conjugate Dirichlet Process Mixture Models.pdf:pdf},
keywords = {Conjugate Dirichlet process mixture model,Markov chain Monte Carlo,Metropolis- Hastings algorithm,conjugate,dpmm,mcmc,sequential importance sampling.,split-merge updates,splitmerge},
mendeley-tags = {conjugate,dpmm,mcmc,splitmerge},
title = {{An Improved Merge-Split Sampler for Conjugate Dirichlet Process Mixture Models}}
}
@article{Jain2007,
author = {Jain, Sonia and Neal, Radford M},
file = {:Users/jonas/Documents/Mendeley Desktop/Jain, Neal - 2007 - The Role of Conditional Conjugacy in Our Algorithm.pdf:pdf},
journal = {Bayesian Analysis},
keywords = {conjugacy,dpmm,gibbs,mcmc,splitmerge},
mendeley-tags = {conjugacy,dpmm,gibbs,mcmc,splitmerge},
number = {3},
pages = {495--500},
title = {{The Role of Conditional Conjugacy in Our Algorithm}},
year = {2007}
}
@inproceedings{Mustiere2006,
author = {Mustiere, Frederic and Bolic, Miodrag and Bouchard, Martin},
booktitle = {2006 Canadian Conference on Electrical and Computer Engineering},
doi = {10.1109/CCECE.2006.277461},
file = {:Users/jonas/Documents/Mendeley Desktop/10.1.1.90.1123.pdf:pdf},
isbn = {1-4244-0038-4},
keywords = {RBPF,particle filters,rao blackwellised,rbpf,speech en-,system identification,tutorial},
mendeley-tags = {RBPF,tutorial},
number = {3},
pages = {1196--1200},
publisher = {IEEE},
title = {{Rao-Blackwellised Particle Filters: Examples of Applications}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4054871 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4054871},
year = {2006}
}
@article{Lopes2012,
author = {Lopes, Hedilbert F and Carvalho, Carlos M},
file = {:Users/jonas/Documents/Mendeley Desktop/Online Bayesian learning in dynamic models - An illustrative introduction to particle methods.pdf:pdf},
keywords = {APF,SIR,review},
mendeley-tags = {APF,review,SIR},
title = {{Online Bayesian Learning in Dynamic Models: An illustrative Introduction to Particle Methods}},
year = {2012}
}
@article{Deneve2007a,
abstract = {Several behavioral experiments suggest that the nervous system uses an internal model of the dynamics of the body to implement a close approximation to a Kalman filter. This filter can be used to perform a variety of tasks nearly optimally, such as predicting the sensory consequence of motor action, integrating sensory and body posture signals, and computing motor commands. We propose that the neural implementation of this Kalman filter involves recurrent basis function networks with attractor dynamics, a kind of architecture that can be readily mapped onto cortical circuits. In such networks, the tuning curves to variables such as arm velocity are remarkably noninvariant in the sense that the amplitude and width of the tuning curves of a given neuron can vary greatly depending on other variables such as the position of the arm or the reliability of the sensory feedback. This property could explain some puzzling properties of tuning curves in the motor and premotor cortex, and it leads to several new predictions.},
author = {Den\`{e}ve, Sophie and Duhamel, Jean-Ren\'{e} and Pouget, Alexandre},
doi = {10.1523/JNEUROSCI.3985-06.2007},
file = {:Users/jonas/Documents/Mendeley Desktop/Den\`{e}ve, Duhamel, Pouget - 2007 - Optimal sensorimotor integration in recurrent cortical networks a neural implementation of Kalman filters.pdf:pdf},
issn = {1529-2401},
journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
keywords = {Cerebral Cortex,Cerebral Cortex: physiology,Nerve Net,Nerve Net: physiology,Neural Networks (Computer),Psychomotor Performance,Psychomotor Performance: physiology,Systems Integration},
month = may,
number = {21},
pages = {5744--56},
pmid = {17522318},
title = {{Optimal sensorimotor integration in recurrent cortical networks: a neural implementation of Kalman filters.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17522318},
volume = {27},
year = {2007}
}
@article{Merlin2007,
abstract = {Diffraction restricts the ability of most electromagnetic devices to image or selectively target objects smaller than the wavelength. We describe planar subwavelength structures capable of focusing well beyond the diffraction limit, operating at arbitrary frequencies. The structure design, related to that of Fresnel plates, forces the input field to converge to a spot on the focal plane. However, unlike the diffraction-limited zone plates, for which focusing results from the interference of traveling waves, the subwavelength plates control the near field and, as such, their superlensing properties originate from a static form of interference. Practical implementations of these plates hold promise for near-field data storage, noncontact sensing, imaging, and nanolithography applications.},
author = {Merlin, R},
doi = {10.1126/science.1143884},
file = {:Users/jonas/Documents/Mendeley Desktop/Merlin - 2007 - Radiationless electromagnetic interference evanescent-field lenses and perfect focusing.pdf:pdf},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
month = aug,
number = {5840},
pages = {927--9},
pmid = {17626847},
title = {{Radiationless electromagnetic interference: evanescent-field lenses and perfect focusing.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17626847},
volume = {317},
year = {2007}
}
@article{Shim2012,
abstract = {As a greedy algorithm to recover sparse signals from compressed measurements, orthogonal matching pursuit (OMP) algorithm has received much attention in recent years. In this paper, we introduce an extension of the OMP for pursuing efficiency in reconstructing sparse signals. Our approach, henceforth referred to as generalized OMP (gOMP), is literally a generalization of the OMP in the sense that multiple N indices are identified per iteration. Owing to the selection of multiple gcorrecth indices, the gOMP algorithm is finished with much smaller number of iterations when compared to the OMP. We show that the gOMP can perfectly reconstruct any K-sparse signals (K \&gt; 1), provided that the sensing matrix satisfies the RIP with \^{A}NK \&lt; \~{a} \~{a} N K+3 \~{a} N . We also demonstrate by empirical simulations that the gOMP has excellent recovery performance comparable to .1-minimization technique with fast processing speed and competitive computational complexity.},
author = {Shim, B. and Wang, J. and Kwon, S.},
title = {{Generalized Orthogonal Matching Pursuit}},
url = {http://ieeexplore.ieee.org.libproxy.mit.edu/xpl/articleDetails.jsp?tp=\&arnumber=6302206\&contentType=Early+Access+Articles\&sortType\%3Dasc\_p\_Sequence\%26filter\%3DAND\%28p\_IS\_Number\%3A4359509\%29},
year = {2012}
}
@article{Deneve2008c,
abstract = {We show that the dynamics of spiking neurons can be interpreted as a form of Bayesian inference in time. Neurons that optimally integrate evidence about events in the external world exhibit properties similar to leaky integrate-and-fire neurons with spike-dependent adaptation and maximally respond to fluctuations of their input. Spikes signal the occurrence of new information-what cannot be predicted from the past activity. As a result, firing statistics are close to Poisson, albeit providing a deterministic representation of probabilities.},
author = {Deneve, Sophie},
doi = {10.1162/neco.2008.20.1.91},
file = {:Users/jonas/Documents/Mendeley Desktop/Deneve - 2008 - Bayesian spiking neurons I inference.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Action Potentials,Action Potentials: physiology,Adaptation, Physiological,Adaptation, Physiological: physiology,Algorithms,Animals,Bayes Theorem,Central Nervous System,Central Nervous System: physiology,Computer Simulation,Humans,Markov Chains,Models, Statistical,Movement,Movement: physiology,Nerve Net,Nerve Net: physiology,Neural Networks (Computer),Neurons,Neurons: physiology,Perception,Perception: physiology,Synaptic Transmission,Synaptic Transmission: physiology},
month = jan,
number = {1},
pages = {91--117},
pmid = {18045002},
title = {{Bayesian spiking neurons I: inference.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18045002},
volume = {20},
year = {2008}
}
@article{Shapiro2008,
abstract = {Ghost-imaging experiments correlate the outputs from two photodetectors: a high spatial-resolution (scanning pinhole or CCD camera) detector that measures a field which has not interacted with the object to be imaged, and a bucket (single-pixel) detector that collects a field that has interacted with the object. We describe a computational ghost-imaging arrangement that uses only a single-pixel detector. This configuration affords background-free imagery in the narrowband limit and a 3D sectioning capability. It clearly indicates the classical nature of ghost-image formation.},
archivePrefix = {arXiv},
arxivId = {0807.2614},
author = {Shapiro, Jeffrey H},
doi = {10.1103/PhysRevA.78.061802},
eprint = {0807.2614},
file = {:Users/jonas/Documents/Mendeley Desktop/Shapiro - 2008 - Computational Ghost Imaging.pdf:pdf},
journal = {Physics},
keywords = {Quantum Physics},
month = jul,
pages = {4},
title = {{Computational Ghost Imaging}},
url = {http://arxiv.org/abs/0807.2614},
year = {2008}
}
@article{Ghuloum2006,
author = {Ghuloum, Abdulaziz},
file = {:Users/jonas/Documents/Mendeley Desktop/Ghuloum - 2006 - An incremental approach to compiler construction.pdf:pdf},
journal = {Proceedings of the 2006 Scheme and Functional Programming Workshop},
keywords = {compilation,scheme},
mendeley-tags = {compilation,scheme},
pages = {27--37},
title = {{An incremental approach to compiler construction}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.88.170\&amp;rep=rep1\&amp;type=pdf},
year = {2006}
}
@phdthesis{FoxThesis2009,
author = {Fox, Emily},
file = {:Users/jonas/Documents/Mendeley Desktop/Fox - 2009 - Bayesian Nonparametric Learning of Complex Dynamical Phenomena.pdf:pdf},
keywords = {nonparametric bayes},
school = {Massachusetts Institute of Technology},
title = {{Bayesian Nonparametric Learning of Complex Dynamical Phenomena}},
type = {PhD},
url = {http://people.csail.mit.edu/fisher/publications/theses/Fox\_PhDThesis09.pdf},
year = {2009}
}
@article{Gustafsson2010,
author = {Gustafsson, Fredrik},
doi = {10.1109/MAES.2010.5546308},
file = {:Users/jonas/Documents/Mendeley Desktop/05546308.pdf:pdf},
issn = {0885-8985},
journal = {IEEE Aerospace and Electronic Systems Magazine},
keywords = {tut},
mendeley-tags = {tut},
month = jul,
number = {7},
pages = {53--82},
title = {{Particle filter theory and practice with positioning applications}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5546308},
volume = {25},
year = {2010}
}
@unpublished{Dahl2005,
abstract = {This paper proposes a new efficient merge-split sampler for both conjugate and nonconju- gate Dirichlet process mixture (DPM) models. These Bayesian nonparametric models are usually fit using Markov chain Monte Carlo (MCMC) or sequential importance sampling (SIS). The latest generation of Gibbs and Gibbs-like samplers for both conjugate and nonconjugate DPM models effectively update the model parameters, but can have difficulty in updating the clustering of the data. To overcome this deficiency, merge-split samplers have been developed, but until now these have been limited to conjugate or conditionally-conjugate DPM models. This paper proposes a new MCMC sampler, called the sequentially-allocated merge-split (SAMS) sampler. The sam- pler borrows ideas from sequential importance sampling. Splits are proposed by sequentially allocating observations to one of two split components using allocation probabilities that condi- tion on previously allocated data. The SAMS sampler is applicable to general nonconjugateDPM models as well as conjugate models. Further, the proposed sampler is substantially more efficient than existing conjugate and nonconjugate samplers.},
author = {Dahl, D.B.},
booktitle = {Journal of Computational and Graphical Statistics},
file = {:Users/jonas/Documents/Mendeley Desktop/Dahl - 2005 - Sequentially-allocated merge-split sampler for conjugate and nonconjugate Dirichlet process mixture models.pdf:pdf},
keywords = {conjugate,dpmm,gibbs,mcmc,nonconjugate,splitmerge},
mendeley-tags = {conjugate,dpmm,gibbs,mcmc,nonconjugate,splitmerge},
number = {2004},
publisher = {Citeseer},
title = {{Sequentially-allocated merge-split sampler for conjugate and nonconjugate Dirichlet process mixture models}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.131.3712\&amp;rep=rep1\&amp;type=pdf},
year = {2005}
}
@inproceedings{FastSLAM2002,
author = {Montemerlo, Michael and Thrun, Sebastian and Koller, Daphne and Wegbreit, Ben},
booktitle = {Proceedings of the AAAI National Conference on Artificial Intelligence},
isbn = {978-3540463993},
keywords = {PF,SLAM,SMC},
mendeley-tags = {PF,SLAM,SMC},
pages = {593--598},
publisher = {AAAI},
title = {{FastSLAM: A Factored Solution to the Simultaneous Localization and Mapping Problem}},
year = {2002}
}
@article{Gustafsson2002,
author = {Gustafsson, F. and Gunnarsson, F. and Bergman, N. and Forssell, U. and Jansson, J. and Karlsson, R. and Nordlund, P.-J.},
doi = {10.1109/78.978396},
file = {:Users/jonas/Documents/Mendeley Desktop/Particle Filters for Positioning, Navigation, and Tracking.pdf:pdf},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
number = {2},
pages = {425--437},
title = {{Particle filters for positioning, navigation, and tracking}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=978396},
volume = {50},
year = {2002}
}
@article{Neiswanger2012,
abstract = {This paper proposes a technique for the unsupervised detection and tracking of arbitrary objects in videos. It is intended to reduce the need for detection and localization methods tailored to specific object types and serve as a general framework applicable to videos with varied objects, backgrounds, and image qualities. The technique uses a dependent Dirichlet process mixture (DDPM) known as the Generalized Polya Urn (GPUDDPM) to model image pixel data that can be easily and efficiently extracted from the regions in a video that represent objects. This paper describes a specific implementation of the model using spatial and color pixel data extracted via frame differencing and gives two algorithms for performing inference in the model to accomplish detection and tracking. This technique is demonstrated on multiple synthetic and benchmark video datasets that illustrate its ability to, without modification, detect and track objects with diverse physical characteristics moving over non-uniform backgrounds and through occlusion.},
archivePrefix = {arXiv},
arxivId = {1210.3288},
author = {Neiswanger, Willie and Wood, Frank},
eprint = {1210.3288},
keywords = {multitarget},
mendeley-tags = {multitarget},
month = oct,
pages = {21},
title = {{Unsupervised Detection and Tracking of Arbitrary Objects with Dependent Dirichlet Process Mixtures}},
url = {http://arxiv.org/abs/1210.3288},
year = {2012}
}
@article{Alexandrescu2004,
author = {Alexandrescu, Andrei},
file = {:Users/jonas/Documents/Mendeley Desktop/Alexandrescu - 2004 - Lock-free data structures with hazard pointers.pdf:pdf},
journal = {C++ User Journal},
keywords = {c++,lock-free},
mendeley-tags = {c++,lock-free},
title = {{Lock-free data structures with hazard pointers}},
url = {http://erdani.com/publications/cuj-2004-12.pdf},
year = {2004}
}
@article{Chen2011a,
abstract = {The ability to accurately infer functional connectivity between ensemble neurons using experimentally acquired spike train data is currently an important research objective in computational neuroscience. Point process generalized linear models and maximum likelihood estimation have been proposed as effective methods for the identification of spiking dependency between neurons. However, unfavorable experimental conditions occasionally results in insufficient data collection due to factors such as low neuronal firing rates or brief recording periods, and in these cases, the standard maximum likelihood estimate becomes unreliable. The present studies compares the performance of different statistical inference procedures when applied to the estimation of functional connectivity in neuronal assemblies with sparse spiking data. Four inference methods were compared: maximum likelihood estimation, penalized maximum likelihood estimation, using either l(2) or l(1) regularization, and hierarchical Bayesian estimation based on a variational Bayes algorithm. Algorithmic performances were compared using well-established goodness-of-fit measures in benchmark simulation studies, and the hierarchical Bayesian approach performed favorably when compared with the other algorithms, and this approach was then successfully applied to real spiking data recorded from the cat motor cortex. The identification of spiking dependencies in physiologically acquired data was encouraging, since their sparse nature would have previously precluded them from successful analysis using traditional methods.},
author = {Chen, Zhe and Putrino, David F and Ghosh, Soumya and Barbieri, Riccardo and Brown, Emery N},
doi = {10.1109/TNSRE.2010.2086079},
file = {:Users/jonas/Documents/Mendeley Desktop/Chen et al. - 2011 - Statistical inference for assessing functional connectivity of neuronal ensembles with sparse spiking data.pdf:pdf},
issn = {1558-0210},
journal = {IEEE transactions on neural systems and rehabilitation engineering : a publication of the IEEE Engineering in Medicine and Biology Society},
keywords = {Algorithms,Animals,Bayes Theorem,Cats,Computer Simulation,Data Interpretation, Statistical,Electrophysiological Phenomena,Likelihood Functions,Linear Models,Logistic Models,Models, Neurological,Motor Cortex,Motor Cortex: physiology,Neural Networks (Computer),Neural Pathways,Neural Pathways: cytology,Neural Pathways: physiology,Neurons,Neurons: physiology,Reproducibility of Results},
month = apr,
number = {2},
pages = {121--35},
pmid = {20937583},
title = {{Statistical inference for assessing functional connectivity of neuronal ensembles with sparse spiking data.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3044782\&tool=pmcentrez\&rendertype=abstract},
volume = {19},
year = {2011}
}
@phdthesis{Lindsten2011,
author = {Lindsten, Fredrik},
file = {:Users/jonas/Documents/Mendeley Desktop/FULLTEXT01.pdf:pdf},
isbn = {9789173931731},
keywords = {RBPF,tutorial},
mendeley-tags = {RBPF,tutorial},
number = {1480},
pages = {1--193},
school = {Linköping University},
title = {{Rao-Blackwellised particle methods for inference and identification}},
type = {PhD},
url = {http://liu.diva-portal.org/smash/record.jsf?pid=diva2:416071},
year = {2011}
}
@article{Ram2006a,
author = {Ram, Sripad and {Sally Ward}, E. and Ober, Raimund J.},
doi = {10.1007/s11045-005-6237-2},
file = {:Users/jonas/Documents/Mendeley Desktop/Ram, Sally Ward, Ober - 2006 - A Stochastic Analysis of Performance Limits for Optical Microscopes.pdf:pdf},
issn = {0923-6082},
journal = {Multidimensional Systems and Signal Processing},
keywords = {b,cramer-rao,e,fisher information matrix,fluorescence microscopy,j,localization accuracy,lower bound,molecule microscopy,ober,optical imaging,parameter estimation,r,ram,s,single,spatio-temporal stochastic processes,ward},
month = jan,
number = {1},
pages = {27--57},
title = {{A Stochastic Analysis of Performance Limits for Optical Microscopes}},
url = {http://www.springerlink.com/index/10.1007/s11045-005-6237-2},
volume = {17},
year = {2006}
}
@inproceedings{Manohar2006,
author = {Manohar, Rajit},
booktitle = {IEEE Custom Integrated Circuits Conference 2006},
doi = {10.1109/CICC.2006.320939},
file = {:Users/jonas/Documents/Mendeley Desktop/Manohar - 2006 - Reconfigurable Asynchronous Logic.pdf:pdf},
isbn = {1-4244-0076-7},
keywords = {asynchronous logic,novel architectures},
mendeley-tags = {asynchronous logic,novel architectures},
month = sep,
number = {Iii},
pages = {13--20},
publisher = {IEEE},
title = {{Reconfigurable Asynchronous Logic}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4114900},
year = {2006}
}
@article{Doucet2000,
abstract = {Particle filters (PFs) are powerful sampling- based inference/learning algorithms for dynamic Bayesian networks (DBNs). They allow us to treat, in a principled way, any type of probabil- ity distribution, nonlinearity and non-stationarity. They have appeared in several fields under such names as “condensation”, “sequential Monte Carlo” and “survival of the fittest”. In this pa- per, we show how we can exploit the structure of the DBN to increase the efficiency of parti- cle filtering, using a technique known as Rao- Blackwellisation. Essentially, this samples some of the variables, and marginalizes out the rest exactly, using the Kalman filter, HMM filter, junction tree algorithm, or any other finite di- mensional optimal filter. We show that Rao- Blackwellised particle filters (RBPFs) lead to more accurate estimates than standard PFs. We demonstrate RBPFs on two problems, namely non-stationary online regression with radial ba- sis function networks and robot localization and map building.We also discuss other potential ap- plication areas and provide references to some fi- nite dimensional optimal filters},
author = {Doucet, Arnaud and de Freitas, Nando and Murphy, Kevin P and Russell, Stuart},
file = {:Users/jonas/Documents/Mendeley Desktop/rbpf\_uai00.pdf:pdf;:Users/jonas/Documents/Mendeley Desktop/exemple.pdf:pdf},
journal = {UAI2000},
keywords = {DBN,RBPF,tutorial},
mendeley-tags = {DBN,RBPF,tutorial},
title = {{Rao-Blackwellised particle filtering for dynamic Bayesian networks}},
url = {http://dl.acm.org/citation.cfm?id=2073968},
year = {2000}
}
@article{Griffiths2011,
annote = {Still one of the best introductions to the CRP. },
author = {Griffiths, Thomas L and Ghahramani, Zoubin},
file = {:Users/jonas/Documents/Mendeley Desktop//Griffiths, Ghahramani - 2011 - The Indian Buffet Process An Introduction and Review.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {beta process,chinese,exchangeable distributions,ibp,latent variable models,markov chain monte carlo,mcmc,mustread,nonparametric bayes,restaurant processes,sparse binary matrices},
mendeley-tags = {ibp,mcmc,mustread,nonparametric bayes},
pages = {1185--1224},
title = {{The Indian Buffet Process : An Introduction and Review}},
volume = {12},
year = {2011}
}
@article{Pouget2007a,
abstract = {Traditional theories of attention rely on the idea that when we search for a target in a visual display the brain boosts the activity of neurons optimally tuned for the target features. In this issue of Neuron, Navalpakkam and Itti take a computational approach to show that this strategy is actually very inefficient when the target is surrounded by distractors with similar features. Instead, the optimal strategy is to boost the activity of neurons that best discriminate between target and distractors, while essentially ignoring the neurons that respond best to the target.},
author = {Pouget, Alexandre and Bavelier, Daphn\'{e}},
doi = {10.1016/j.neuron.2007.02.004},
file = {:Users/jonas/Documents/Mendeley Desktop/Pouget, Bavelier - 2007 - Paying attention to neurons with discriminating taste.pdf:pdf},
issn = {1097-4199},
journal = {Neuron},
keywords = {Attention,Attention: physiology,Discrimination (Psychology),Discrimination (Psychology): physiology,Humans,Models, Neurological,Neurons,Neurons: physiology,Reaction Time,Visual Perception},
month = feb,
number = {4},
pages = {473--5},
pmid = {17296547},
title = {{Paying attention to neurons with discriminating taste.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17296547},
volume = {53},
year = {2007}
}
@article{Churchland2011a,
abstract = {Traditionally, insights into neural computation have been furnished by averaged firing rates from many stimulus repetitions or trials. We pursue an analysis of neural response variance to unveil neural computations that cannot be discerned from measures of average firing rate. We analyzed single-neuron recordings from the lateral intraparietal area (LIP), during a perceptual decision-making task. Spike count variance was divided into two components using the law of total variance for doubly stochastic processes: (1) variance of counts that would be produced by a stochastic point process with a given rate, and loosely (2) the variance of the rates that would produce those counts (i.e., "conditional expectation"). The variance and correlation of the conditional expectation exposed several neural mechanisms: mixtures of firing rate states preceding the decision, accumulation of stochastic "evidence" during decision formation, and a stereotyped response at decision end. These analyses help to differentiate among several alternative decision-making models.},
author = {Churchland, Anne K and Kiani, R and Chaudhuri, R and Wang, Xiao-Jing and Pouget, Alexandre and Shadlen, M N},
doi = {10.1016/j.neuron.2010.12.037},
file = {:Users/jonas/Documents/Mendeley Desktop/Churchland et al. - 2011 - Variance as a signature of neural computations during decision making.pdf:pdf},
issn = {1097-4199},
journal = {Neuron},
keywords = {Action Potentials,Action Potentials: physiology,Animals,Computer Simulation,Decision Making,Decision Making: physiology,Haplorhini,Models, Neurological,Motion Perception,Motion Perception: physiology,Neurons,Neurons: physiology,Numerical Analysis, Computer-Assisted,Photic Stimulation,Reaction Time,Reaction Time: physiology,Statistics as Topic,Stochastic Processes,Time Factors},
month = feb,
number = {4},
pages = {818--31},
pmid = {21338889},
publisher = {Elsevier Inc.},
title = {{Variance as a signature of neural computations during decision making.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3066020\&tool=pmcentrez\&rendertype=abstract},
volume = {69},
year = {2011}
}
@article{Wang2011,
author = {Wang, Lianming},
doi = {10.1198/jcgs.2010.07081},
file = {:Users/jonas/Documents/Mendeley Desktop/Wang - 2011 - Fast Bayesian Inference in Dirichlet Process Mixture Models.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {clustering,density estimation,dpmm,efficient computation,large samples,map,nonparametric bayes,p\'{o}lya urn scheme,search,sequential analysis},
mendeley-tags = {dpmm,map,search},
month = mar,
number = {1},
pages = {196--216},
title = {{Fast Bayesian Inference in Dirichlet Process Mixture Models}},
url = {http://pubs.amstat.org/doi/abs/10.1198/jcgs.2010.07081},
volume = {20},
year = {2011}
}
@article{Michalet2006,
author = {Michalet, Xavier and Weiss, Shimon},
doi = {10.1073/pnas.0600808103},
file = {:Users/jonas/Documents/Mendeley Desktop/Michalet, Weiss - 2006 - Using photon statistics to boost microscopy resolution.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {DNA,Microscopy,Microscopy: methods,Photons,Sensitivity and Specificity},
month = mar,
number = {13},
pages = {4797--8},
pmid = {16549771},
title = {{Using photon statistics to boost microscopy resolution.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1458746\&tool=pmcentrez\&rendertype=abstract},
volume = {103},
year = {2006}
}
@article{Ma2006b,
abstract = {Recent psychophysical experiments indicate that humans perform near-optimal Bayesian inference in a wide variety of tasks, ranging from cue integration to decision making to motor control. This implies that neurons both represent probability distributions and combine those distributions according to a close approximation to Bayes' rule. At first sight, it would seem that the high variability in the responses of cortical neurons would make it difficult to implement such optimal statistical inference in cortical circuits. We argue that, in fact, this variability implies that populations of neurons automatically represent probability distributions over the stimulus, a type of code we call probabilistic population codes. Moreover, we demonstrate that the Poisson-like variability observed in cortex reduces a broad class of Bayesian inference to simple linear combinations of populations of neural activity. These results hold for arbitrary probability distributions over the stimulus, for tuning curves of arbitrary shape and for realistic neuronal variability.},
author = {Ma, WJ and Beck, JM and Latham, PE and Pouget, A},
file = {:Users/jonas/Documents/Mendeley Desktop//Ma et al. - 2006 - Bayesian inference with probabilistic population codes.pdf:pdf},
institution = {Department of Brain and Cognitive Sciences, Meliora Hall, University of Rochester, Rochester, New York 14627, USA.},
journal = {Nature Neuroscience},
number = {11},
pages = {1432--1438},
publisher = {Nature Publishing Group},
title = {{Bayesian inference with probabilistic population codes.}},
url = {http://discovery.ucl.ac.uk/7609/},
volume = {9},
year = {2006}
}
@article{Robert2007,
author = {Robert, C P},
doi = {10.1214/07-BA219B},
file = {:Users/jonas/Documents/Mendeley Desktop/Robert - 2007 - Comment on Article by Jain and Neal.pdf:pdf},
issn = {1931-6690},
journal = {Bayesian Analysis},
keywords = {dpmm,mcmc,splitmerge},
mendeley-tags = {dpmm,mcmc,splitmerge},
number = {3},
pages = {479--482},
title = {{Comment on Article by Jain and Neal}},
url = {http://ba.stat.cmu.edu/journal/2007/vol02/issue03/robert.pdf},
volume = {2},
year = {2007}
}
@article{Liu2002,
author = {Liu, J.S.},
doi = {10.1109/78.978381},
file = {:Users/jonas/Documents/Mendeley Desktop/Convergence Analyses and Comparisons of Markov Chain Monte Carlo Algorithms in Digital Communications.pdf:pdf},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
number = {2},
pages = {255--270},
title = {{Convergence analyses and comparisons of Markov chain Monte Carlo algorithms in digital communications}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=978381},
volume = {50},
year = {2002}
}
@misc{TheMendeleySupportTeam2011,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
file = {:Users/jonas/Documents/Mendeley Desktop/FAQ.pdf:pdf},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2011}
}
@article{DeAngelis2009,
author = {{De Angelis}, Alessio and Dionigi, Marco and Moschitta, Antonio and Carbone, Paolo},
doi = {10.1109/TIM.2009.2020834},
file = {:Users/jonas/Documents/Mendeley Desktop/De Angelis et al. - 2009 - A Low-Cost Ultra-Wideband Indoor Ranging System.pdf:pdf},
issn = {0018-9456},
journal = {IEEE Transactions on Instrumentation and Measurement},
month = dec,
number = {12},
pages = {3935--3942},
title = {{A Low-Cost Ultra-Wideband Indoor Ranging System}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5306095},
volume = {58},
year = {2009}
}
@article{Feeley1990,
address = {New York, New York, USA},
author = {Feeley, Marc and Miller, James S.},
doi = {10.1145/91556.91606},
file = {:Users/jonas/Documents/Mendeley Desktop/Feeley, Miller - 1990 - A parallel virtual machine for efficient scheme compilation.pdf:pdf},
isbn = {089791368X},
journal = {Proceedings of the 1990 ACM conference on LISP and functional programming - LFP '90},
keywords = {compilation,scheme},
mendeley-tags = {compilation,scheme},
pages = {119--130},
publisher = {ACM Press},
title = {{A parallel virtual machine for efficient scheme compilation}},
url = {http://portal.acm.org/citation.cfm?doid=91556.91606},
year = {1990}
}
@article{Simons2009,
archivePrefix = {arXiv},
arxivId = {arXiv:0909.5368v1},
author = {Simons, F.J.},
eprint = {arXiv:0909.5368v1},
file = {:Users/jonas/Documents/Mendeley Desktop/Simons - 2009 - Slepian functions and their use in signal estimation and spectral analysis.pdf:pdf},
journal = {Arxiv preprint arXiv:0909.5368},
keywords = {inverse theory,satellite geodesy,sparsity,spectral analysis,spherical harmonics,statistical methods},
title = {{Slepian functions and their use in signal estimation and spectral analysis}},
url = {http://arxiv.org/abs/0909.5368},
year = {2009}
}
@misc{Mansinghka2009,
author = {Mansinghka, VK and Jonas, EM},
publisher = {US Patent Office},
title = {{Combinational Stochastic Logic}},
year = {2009}
}
@article{DaumeIII2009,
author = {{Daum\'{e} III}, H.},
file = {:Users/jonas/Documents/Mendeley Desktop/Daum\'{e} III - 2009 - Fast search for Dirichlet process mixture models.pdf:pdf},
journal = {Arxiv preprint arXiv:0907.1812},
keywords = {dpmm,map,search},
mendeley-tags = {dpmm,map,search},
number = {1},
title = {{Fast search for Dirichlet process mixture models}},
url = {http://arxiv.org/abs/0907.1812},
year = {2009}
}
@article{Spielman2009a,
author = {Spielman, D.A. and Teng, S.H.},
file = {:Users/jonas/Documents/Mendeley Desktop/Spielman, Teng - 2009 - Smoothed Analysis An attempt to Explain the Behavior of Algorithms in Practice.pdf:pdf},
journal = {Communications of the ACM},
keywords = {algorithms,math,smoothed analysis},
mendeley-tags = {algorithms,math,smoothed analysis},
number = {10},
pages = {76--84},
publisher = {ACM},
title = {{Smoothed Analysis: An attempt to Explain the Behavior of Algorithms in Practice}},
url = {http://portal.acm.org/citation.cfm?id=1562785},
volume = {52},
year = {2009}
}
@article{Dubois2007,
author = {Dubois, Corentin and Davy, Manuel},
doi = {10.1109/TASL.2007.894522},
file = {:Users/jonas/Documents/Mendeley Desktop/04156193.pdf:pdf},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
month = may,
number = {4},
pages = {1283--1295},
title = {{Joint Detection and Tracking of Time-Varying Harmonic Components: A Flexible Bayesian Approach}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4156193},
volume = {15},
year = {2007}
}
@article{Cevher2008,
author = {Cevher, Volkan},
file = {:Users/jonas/Documents/Mendeley Desktop/Cevher - 2008 - Learning with compressible priors.pdf:pdf},
journal = {NIPS, Vancouver, BC, Canada},
keywords = {bayes,compressed sensing},
mendeley-tags = {bayes,compressed sensing},
number = {i},
pages = {7--12},
publisher = {Citeseer},
title = {{Learning with compressible priors}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.147.5068\&amp;rep=rep1\&amp;type=pdf},
year = {2008}
}
@article{Scheel2009a,
author = {Scheel, Stefan},
doi = {10.1080/09500340802331849},
file = {:Users/jonas/Documents/Mendeley Desktop/Scheel - 2009 - Single-photon sources–an introduction.htm:htm},
issn = {0950-0340},
journal = {Journal of Modern Optics},
keywords = {cavity qed,cluster states,linear optical quantum computing,single photons},
month = jan,
number = {2-3},
pages = {141--160},
title = {{Single-photon sources–an introduction}},
url = {http://www.tandfonline.com/doi/abs/10.1080/09500340802331849},
volume = {56},
year = {2009}
}
@article{Herlihy1991,
author = {Herlihy, Maurice},
doi = {10.1145/114005.102808},
file = {:Users/jonas/Documents/Mendeley Desktop/Herlihy - 1991 - Wait-free synchronization.pdf:pdf},
issn = {01640925},
journal = {ACM Transactions on Programming Languages and Systems},
month = jan,
number = {1},
pages = {124--149},
title = {{Wait-free synchronization}},
url = {http://portal.acm.org/citation.cfm?doid=114005.102808},
volume = {13},
year = {1991}
}
@article{Guha2010,
author = {Guha, Subharup},
doi = {10.1198/jasa.2010.tm09340},
file = {:Users/jonas/Documents/Mendeley Desktop/Guha - 2010 - Posterior Simulation in Countable Mixture Models for Large Datasets.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {data squashing,dirichlet process,dirichlet process mixture model,generalized p\'{o}lya urn process,hidden markov model,inference,markov chain monte carlo,semiparametric bayes},
mendeley-tags = {dirichlet process mixture model,inference},
month = jun,
number = {490},
pages = {775--786},
title = {{Posterior Simulation in Countable Mixture Models for Large Datasets}},
url = {http://pubs.amstat.org/doi/abs/10.1198/jasa.2010.tm09340},
volume = {105},
year = {2010}
}
@article{Walker2007,
author = {Walker, Stephen G.},
doi = {10.1080/03610910601096262},
file = {:Users/jonas/Documents/Mendeley Desktop/Walker - 2007 - Sampling the Dirichlet Mixture Model with Slices.pdf:pdf},
isbn = {0361091060},
issn = {0361-0918},
journal = {Communications in Statistics - Simulation and Computation},
month = jan,
number = {1},
pages = {45--54},
title = {{Sampling the Dirichlet Mixture Model with Slices}},
url = {http://www.tandfonline.com/doi/abs/10.1080/03610910601096262},
volume = {36},
year = {2007}
}
@article{Rubinstein2010,
author = {Rubinstein, Ron and Bruckstein, Alfred M and Elad, Michael},
doi = {10.1109/JPROC.2010.2040551},
file = {:Users/jonas/Documents/Mendeley Desktop/Rubinstein, Bruckstein, Elad - 2010 - Dictionaries for Sparse Representation Modeling.pdf:pdf},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
keywords = {approximation,dictionary learning,harmonic analysis,signal,signal representation,sparse,sparse coding},
month = jun,
number = {6},
pages = {1045--1057},
title = {{Dictionaries for Sparse Representation Modeling}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5452966},
volume = {98},
year = {2010}
}
@article{Neal2000,
author = {Neal, Radford M.},
doi = {10.2307/1390653},
file = {:Users/jonas/Documents/Mendeley Desktop/Neal - 2000 - Markov Chain Sampling Methods for Dirichlet Process Mixture Models.pdf:pdf},
issn = {10618600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {crp,mcmc,mixtures,nonparametric bayes},
mendeley-tags = {crp,mcmc,mixtures,nonparametric bayes},
month = jun,
number = {2},
pages = {249},
title = {{Markov Chain Sampling Methods for Dirichlet Process Mixture Models}},
url = {http://www.jstor.org/stable/1390653?origin=crossref},
volume = {9},
year = {2000}
}
@article{Watzenig2009,
author = {Watzenig, D and Fox, C},
doi = {10.1088/0957-0233/20/5/052002},
file = {:Users/jonas/Documents/Mendeley Desktop/Watzenig, Fox - 2009 - A review of statistical modelling and inference for electrical capacitance tomography.pdf:pdf},
issn = {0957-0233},
journal = {Measurement Science and Technology},
keywords = {bayesian inference,capacitance tomography,electrical,markov chain monte carlo,statistical inversion},
month = may,
number = {5},
pages = {052002},
title = {{A review of statistical modelling and inference for electrical capacitance tomography}},
url = {http://stacks.iop.org/0957-0233/20/i=5/a=052002?key=crossref.5b4e8a6aaa89561bdcb721c22cc92531},
volume = {20},
year = {2009}
}
@article{Litvak2009a,
abstract = {In this letter, we develop and simulate a large-scale network of spiking neurons that approximates the inference computations performed by graphical models. Unlike previous related schemes, which used sum and product operations in either the log or linear domains, the current model uses an inference scheme based on the sum and maximization operations in the log domain. Simulations show that using these operations, a large-scale circuit, which combines populations of spiking neurons as basic building blocks, is capable of finding close approximations to the full mathematical computations performed by graphical models within a few hundred milliseconds. The circuit is general in the sense that it can be wired for any graph structure, it supports multistate variables, and it uses standard leaky integrate-and-fire neuronal units. Following previous work, which proposed relations between graphical models and the large-scale cortical anatomy, we focus on the cortical microcircuitry and propose how anatomical and physiological aspects of the local circuitry may map onto elements of the graphical model implementation. We discuss in particular the roles of three major types of inhibitory neurons (small fast-spiking basket cells, large layer 2/3 basket cells, and double-bouquet neurons), subpopulations of strongly interconnected neurons with their unique connectivity patterns in different cortical layers, and the possible role of minicolumns in the realization of the population-based maximum operation.},
author = {Litvak, Shai and Ullman, Shimon},
doi = {10.1162/neco.2009.05-08-783},
file = {:Users/jonas/Documents/Mendeley Desktop/Litvak, Ullman - 2009 - Cortical circuitry implementing graphical models.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Brain Mapping,Cerebral Cortex,Cerebral Cortex: physiology,Computer Graphics,Data Interpretation, Statistical,Markov Chains,Models, Neurological,Models, Statistical,Neural Networks (Computer),Neural Pathways},
month = nov,
number = {11},
pages = {3010--56},
pmid = {19686065},
title = {{Cortical circuitry implementing graphical models.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19686065},
volume = {21},
year = {2009}
}
@article{Seung2009,
abstract = {Many theories of neural networks assume rules of connection between pairs of neurons that are based on their cell types or functional properties. It is finally becoming feasible to test such pairwise models of connectivity, due to emerging advances in neuroanatomical techniques. One method will be to measure the functional properties of connected pairs of neurons, sparsely sampling pairs from many specimens. Another method will be to find a "connectome," a dense map of all connections in a single specimen, and infer functional properties of neurons through computational analysis. For the latter method, the most exciting prospect would be to decode the memories that are hypothesized to be stored in connectomes.},
author = {Seung, H Sebastian},
doi = {10.1016/j.neuron.2009.03.020},
file = {:Users/jonas/Documents/Mendeley Desktop/Seung - 2009 - Reading the book of memory sparse sampling versus dense mapping of connectomes.pdf:pdf},
issn = {1097-4199},
journal = {Neuron},
keywords = {Animals,Brain,Brain Mapping,Brain: cytology,Brain: physiology,Caenorhabditis elegans,Caenorhabditis elegans: physiology,Humans,Memory,Memory: physiology,Models,Nerve Net,Nerve Net: physiology,Neural Networks (Computer),Neural Pathways,Neurological,Neurons,Neurons: physiology,Retina,Retina: cytology,Retina: physiology,Synaptic Transmission,compressed sensing,connectomics,neuroscience},
mendeley-tags = {compressed sensing,connectomics,neuroscience},
month = apr,
number = {1},
pages = {17--29},
pmid = {19376064},
publisher = {Elsevier Inc.},
title = {{Reading the book of memory: sparse sampling versus dense mapping of connectomes.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19376064},
volume = {62},
year = {2009}
}
@article{Kording2006a,
abstract = {Action selection is a fundamental decision process for us, and depends on the state of both our body and the environment. Because signals in our sensory and motor systems are corrupted by variability or noise, the nervous system needs to estimate these states. To select an optimal action these state estimates need to be combined with knowledge of the potential costs or rewards of different action outcomes. We review recent studies that have investigated the mechanisms used by the nervous system to solve such estimation and decision problems, which show that human behaviour is close to that predicted by Bayesian Decision Theory. This theory defines optimal behaviour in a world characterized by uncertainty, and provides a coherent way of describing sensorimotor processes.},
author = {K\"{o}rding, Konrad P and Wolpert, Daniel M},
doi = {10.1016/j.tics.2006.05.003},
file = {:Users/jonas/Documents/Mendeley Desktop/K\"{o}rding, Wolpert - 2006 - Bayesian decision theory in sensorimotor control.pdf:pdf},
issn = {1364-6613},
journal = {Trends in cognitive sciences},
keywords = {Brain,Brain: physiology,Cognition,Cognition: physiology,Decision Theory,Efficiency,Feedback,Feedback: physiology,Generalization (Psychology),Generalization (Psychology): physiology,Humans,Models, Statistical,Movement,Movement: physiology,Perception,Perception: physiology,Psychomotor Performance,Psychomotor Performance: physiology,Vision, Ocular,Vision, Ocular: physiology},
month = jul,
number = {7},
pages = {319--26},
pmid = {16807063},
title = {{Bayesian decision theory in sensorimotor control.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16807063},
volume = {10},
year = {2006}
}
@article{Whiteley2011,
author = {Whiteley, Nick and Johansen, Adam M. and Godsill, Simon},
doi = {10.1198/jcgs.2009.08052},
file = {:Users/jonas/Documents/Mendeley Desktop/jcgs\%2E2009\%2E08052.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
month = jan,
number = {1},
pages = {119--139},
title = {{Monte Carlo Filtering of Piecewise Deterministic Processes}},
url = {http://www.tandfonline.com/doi/abs/10.1198/jcgs.2009.08052},
volume = {20},
year = {2011}
}
@article{Cornebise2008,
author = {Cornebise, Julien and Moulines, \'{E}ric and Olsson, Jimmy},
doi = {10.1007/s11222-008-9089-4},
file = {:Users/jonas/Documents/Mendeley Desktop/Adaptive methods for sequential importance sampling with application to state space models.pdf:pdf},
issn = {0960-3174},
journal = {Statistics and Computing},
keywords = {adaptive monte carlo,anr,anr-05-blan-0299,auxiliary particle,coefficient of variation,filter,kullback-leibler,research agency,supported by the national,this work was partly,under the program},
month = aug,
number = {4},
pages = {461--480},
title = {{Adaptive methods for sequential importance sampling with application to state space models}},
url = {http://www.springerlink.com/index/10.1007/s11222-008-9089-4},
volume = {18},
year = {2008}
}
@book{Johansen2010,
abstract = {Lecture Notes},
author = {Johansen, Adam M and Evers, Ludger},
editor = {Whiteley, NIck},
file = {:Users/jonas/Documents/Mendeley Desktop/Monte Carlo Methods.pdf:pdf},
keywords = {review},
mendeley-tags = {review},
title = {{Monte Carlo Methods}},
year = {2010}
}
@phdthesis{Briers2007,
author = {Briers, Mark},
booktitle = {Cambridge University, PhD Thesis},
file = {:Users/jonas/Documents/Mendeley Desktop/10.1.1.112.8731.pdf:pdf},
keywords = {block sampling},
mendeley-tags = {block sampling},
number = {April},
pages = {1--195},
school = {University of Cambridge},
title = {{Improved Monte Carlo methods for state-space models}},
type = {PhD},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.112.8731\&rep=rep1\&type=pdf},
year = {2007}
}
@article{Herrmann2009,
author = {Herrmann, Felix J.},
doi = {10.1190/1.3255570},
file = {:Users/jonas/Documents/Mendeley Desktop/Herrmann - 2009 - Sub-Nyquist sampling and sparsity How to get more information from fewer samples.pdf:pdf},
journal = {SEG Technical Program Expanded Abstracts},
keywords = {compressed sensing},
mendeley-tags = {compressed sensing},
pages = {3410--3415},
publisher = {Seg},
title = {{Sub-Nyquist sampling and sparsity: How to get more information from fewer samples}},
url = {http://link.aip.org/link/SEGEAB/v28/i1/p3410/s1\&Agg=doi},
year = {2009}
}
@article{Wiechert2010a,
abstract = {Decorrelation is a fundamental computation that optimizes the format of neuronal activity patterns. Channel decorrelation by adaptive mechanisms results in efficient coding, whereas pattern decorrelation facilitates the readout and storage of information. Mechanisms achieving pattern decorrelation, however, remain unclear. We developed a theoretical framework that relates high-dimensional pattern decorrelation to neuronal and circuit properties in a mathematically stringent fashion. For a generic class of random neuronal networks, we proved that pattern decorrelation emerges from neuronal nonlinearities and is amplified by recurrent connectivity. This mechanism does not require adaptation of the network, is enhanced by sparse connectivity, depends on the baseline membrane potential and is robust. Connectivity measurements and computational modeling suggest that this mechanism is involved in pattern decorrelation in the zebrafish olfactory bulb. These results reveal a generic relationship between the structure and function of neuronal circuits that is probably relevant for pattern processing in various brain areas.},
author = {Wiechert, Martin T and Judkewitz, Benjamin and Riecke, Hermann and Friedrich, Rainer W},
doi = {10.1038/nn.2591},
file = {:Users/jonas/Documents/Mendeley Desktop/Wiechert et al. - 2010 - Mechanisms of pattern decorrelation by recurrent neuronal circuits.pdf:pdf},
issn = {1546-1726},
journal = {Nature neuroscience},
keywords = {Animals,Computer Simulation,Models, Neurological,Models, Theoretical,Neural Pathways,Neural Pathways: physiology,Neurons,Neurons: physiology,Olfactory Bulb,Olfactory Bulb: physiology,Zebrafish},
month = aug,
number = {8},
pages = {1003--10},
pmid = {20581841},
publisher = {Nature Publishing Group},
title = {{Mechanisms of pattern decorrelation by recurrent neuronal circuits.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20581841},
volume = {13},
year = {2010}
}
@article{Gray,
author = {Gray, Robert M},
file = {:Users/jonas/Documents/Mendeley Desktop/Gray - Unknown - Toeplitz and Circulant Matrices A review.pdf:pdf},
journal = {Electrical Engineering},
keywords = {linear algebra,matrix math},
mendeley-tags = {linear algebra,matrix math},
title = {{Toeplitz and Circulant Matrices: A review}}
}
@techreport{Neal1998,
author = {Neal, Radford M.},
file = {:Users/jonas/Documents/Mendeley Desktop/Neal - 1998 - Markov Chain Sampling Methods for Dirichlet Process Mixture Models.pdf:pdf},
institution = {University of Toronto},
keywords = {dpmm,gibbs,mcmc},
mendeley-tags = {dpmm,gibbs,mcmc},
title = {{Markov Chain Sampling Methods for Dirichlet Process Mixture Models}},
year = {1998}
}
@article{Johansen2008a,
author = {Johansen, Adam M. and Doucet, Arnaud},
doi = {10.1016/j.spl.2008.01.032},
file = {:Users/jonas/Documents/Mendeley Desktop/A Note on Auxiliary Particle Filters.pdf:pdf},
issn = {01677152},
journal = {Statistics \& Probability Letters},
month = sep,
number = {12},
pages = {1498--1504},
title = {{A note on auxiliary particle filters}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167715208000035},
volume = {78},
year = {2008}
}
@article{Blei2009,
author = {Blei, DM},
file = {:Users/jonas/Documents/Mendeley Desktop/Blei - 2009 - Distance dependent Chinese restaurant processes.pdf:pdf},
journal = {Arxiv preprint arXiv:0910.1022},
title = {{Distance dependent Chinese restaurant processes}},
url = {http://arxiv.org/abs/0910.1022},
year = {2009}
}
@article{Tropp2010,
author = {Tropp, Joel a and Wright, Stephen J},
doi = {10.1109/JPROC.2010.2044010},
file = {:Users/jonas/Documents/Mendeley Desktop/Tropp, Wright - 2010 - Computational Methods for Sparse Solution of Linear Inverse Problems.pdf:pdf},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
keywords = {compressed sensing,convex optimization,dimension,entries relative to its,ing pursuit,known has few nonzero,match-,solution to a linear,sparse approximation,system while requiring that,the un-},
month = jun,
number = {6},
pages = {948--958},
title = {{Computational Methods for Sparse Solution of Linear Inverse Problems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5456165},
volume = {98},
year = {2010}
}
@article{Bilmes2010a,
author = {Bilmes, Jeff},
doi = {10.1109/MSP.2010.938078},
file = {:Users/jonas/Documents/Mendeley Desktop/Bilmes - 2010 - Dynamic Graphical Models.pdf:pdf},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
keywords = {dbn,graphical models,time series},
mendeley-tags = {dbn,graphical models,time series},
month = nov,
number = {November},
title = {{Dynamic Graphical Models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5563114},
year = {2010}
}
@article{Knill2004a,
abstract = {To use sensory information efficiently to make judgments and guide action in the world, the brain must represent and use information about uncertainty in its computations for perception and action. Bayesian methods have proven successful in building computational theories for perception and sensorimotor control, and psychophysics is providing a growing body of evidence that human perceptual computations are "Bayes' optimal". This leads to the "Bayesian coding hypothesis": that the brain represents sensory information probabilistically, in the form of probability distributions. Several computational schemes have recently been proposed for how this might be achieved in populations of neurons. Neurophysiological data on the hypothesis, however, is almost non-existent. A major challenge for neuroscientists is to test these ideas experimentally, and so determine whether and how neurons code information about sensory uncertainty.},
author = {Knill, David C and Pouget, Alexandre},
doi = {10.1016/j.tins.2004.10.007},
file = {:Users/jonas/Documents/Mendeley Desktop/Knill, Pouget - 2004 - The Bayesian brain the role of uncertainty in neural coding and computation.pdf:pdf},
issn = {0166-2236},
journal = {Trends in neurosciences},
keywords = {Animals,Bayes Theorem,Brain,Brain: physiology,Humans,Models, Biological,Nerve Net,Neurons,Neurons: metabolism,Perception},
month = dec,
number = {12},
pages = {712--9},
pmid = {15541511},
title = {{The Bayesian brain: the role of uncertainty in neural coding and computation.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15541511},
volume = {27},
year = {2004}
}
@article{Manohar2000,
author = {Manohar, R},
file = {:Users/jonas/Documents/Mendeley Desktop/Manohar - 2000 - A case for asynchronous computer architecture.pdf:pdf},
title = {{A case for asynchronous computer architecture}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.949},
year = {2000}
}
@article{Kalli2009,
author = {Kalli, Maria and Griffin, Jim E. and Walker, Stephen G.},
doi = {10.1007/s11222-009-9150-y},
file = {:Users/jonas/Documents/Mendeley Desktop/Kalli, Griffin, Walker - 2009 - Slice sampling mixture models.pdf:pdf},
issn = {0960-3174},
journal = {Statistics and Computing},
keywords = {44-1227-823627,44-1227-827932,actuarial sci-,canterbury,corresponding author,dirichlet process,email,ence,fax,griffin,institute of mathematics,jim e,k,malized weights,markov chain monte carlo,mixture model,nor-,slice sampler,statistics,tel,u,university of kent},
month = sep,
number = {1},
pages = {93--105},
title = {{Slice sampling mixture models}},
url = {http://www.springerlink.com/index/10.1007/s11222-009-9150-y},
volume = {21},
year = {2009}
}
@article{Schon2005,
author = {Schon, T and Gustafsson, F. and Nordlund, P.-J.},
doi = {10.1109/TSP.2005.849151},
file = {:Users/jonas/Documents/Mendeley Desktop/Marginalized Particle Filters for Mixed Linear$\backslash$:Nonlinear State-space Models.pdf:pdf},
issn = {1053-587X},
journal = {IEEE Transactions on Signal Processing},
month = jul,
number = {7},
pages = {2279--2289},
title = {{Marginalized particle filters for mixed linear/nonlinear state-space models}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1453762 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1453762},
volume = {53},
year = {2005}
}
@misc{Prechtl2011,
author = {Prechtl, EF and Sedwick, RJ and Jonas, EM},
publisher = {US Patent Office},
title = {{Stereoscopic Wide Field of View Imaging System}},
year = {2011}
}
@article{Kording2007a,
abstract = {The purpose of our nervous system is to allow us to successfully interact with our environment. This normative idea is formalized by decision theory that defines which choices would be most beneficial. We live in an uncertain world, and each decision may have many possible outcomes; choosing the best decision is thus complicated. Bayesian decision theory formalizes these problems in the presence of uncertainty and often provides compact models that predict observed behavior. With its elegant formalization of the problems faced by the nervous system, it promises to become a major inspiration for studies in neuroscience.},
author = {K\"{o}rding, Konrad},
doi = {10.1126/science.1142998},
file = {:Users/jonas/Documents/Mendeley Desktop/K\"{o}rding - 2007 - Decision theory what should the nervous system do.pdf:pdf},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
keywords = {Bayes Theorem,Brain,Brain: physiology,Decision Making,Humans,Nervous System Physiological Phenomena,Uncertainty},
month = oct,
number = {5850},
pages = {606--10},
pmid = {17962554},
title = {{Decision theory: what "should" the nervous system do?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17962554},
volume = {318},
year = {2007}
}
@article{Goswami2007,
author = {Goswami, Gopi and Liu, Jun S and Wong, Wing H},
doi = {10.1198/106186007X255072},
file = {:Users/jonas/Documents/Mendeley Desktop/Goswami, Liu, Wong - 2007 - Evolutionary Monte Carlo Methods for Clustering.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {dirichlet process mixture model,gibbs sampling,hastings algorithm,integrated auto-correlation,k -means,metropolis,model-based clustering,parallel tem-,pering,temperature ladder,time,variable selection},
month = dec,
number = {4},
pages = {855--876},
title = {{Evolutionary Monte Carlo Methods for Clustering}},
url = {http://pubs.amstat.org/doi/abs/10.1198/106186007X255072},
volume = {16},
year = {2007}
}
@inproceedings{JohansenExact2012,
address = {Brussels},
author = {Johansen, Adam M and Whiteley, Nick and Doucet, Arnaud},
booktitle = {Proceedings of 16th IFAC Symposium on Systems Identification},
file = {:Users/jonas/Documents/Mendeley Desktop/JWD12 (1).pdf:pdf},
keywords = {RBPF,dynamic systems,monte carlo method,optimal filtering,target tracking,target tracking filters},
mendeley-tags = {RBPF},
title = {{Exact Approximation of Rao-Blackwellised Particle Filters}},
year = {2012}
}
@article{Smolyaninov2007,
abstract = {We demonstrate a magnifying superlens that can be integrated into a conventional far-field optical microscope. Our design is based on a multilayer photonic metamaterial consisting of alternating layers of positive and negative refractive index, as originally proposed by Narimanov and Engheta. We achieved a resolution on the order of 70 nanometers. The use of such a magnifying superlens should find numerous applications in imaging.},
author = {Smolyaninov, Igor I and Hung, Yu-Ju and Davis, Christopher C},
doi = {10.1126/science.1138746},
file = {:Users/jonas/Documents/Mendeley Desktop/Smolyaninov, Hung, Davis - 2007 - Magnifying superlens in the visible frequency range.pdf:pdf},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
month = mar,
number = {5819},
pages = {1699--701},
pmid = {17379804},
title = {{Magnifying superlens in the visible frequency range.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17379804},
volume = {315},
year = {2007}
}
@article{Julier2000,
author = {Julier, Simon and Uhlmann, J. and Durrant-Whyte, H.F.},
doi = {10.1109/9.847726},
file = {:Users/jonas/Documents/Mendeley Desktop/julier2000.pdf:pdf},
issn = {00189286},
journal = {IEEE Transactions on Automatic Control},
keywords = {UKF},
mendeley-tags = {UKF},
month = mar,
number = {3},
pages = {477--482},
title = {{A new method for the nonlinear transformation of means and covariances in filters and estimators}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=847726 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=847726},
volume = {45},
year = {2000}
}
@article{Chib1995,
author = {Chib, Siddhartha and Greenberg, Edward},
file = {:Users/jonas/Documents/Mendeley Desktop/Chib, Greenberg - 1995 - Understanding the Metropolis-Hastings Algorithm.pdf:pdf},
journal = {The American Statistician},
keywords = {mcmc,mh},
mendeley-tags = {mcmc,mh},
number = {4},
pages = {327},
title = {{Understanding the Metropolis-Hastings Algorithm}},
volume = {49},
year = {1995}
}
@article{Fox2010a,
author = {Fox, Emily and Sudderth, Erik and Jordan, Michael and Willsky, Alan},
doi = {10.1109/MSP.2010.937999},
file = {:Users/jonas/Documents/Mendeley Desktop/Fox et al. - 2010 - Bayesian Nonparametric Methods for Learning Markov Switching Processes.pdf:pdf},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
keywords = {beta process,dbn,dynamical systems,graphical models,nonparametric bayes},
mendeley-tags = {beta process,dbn,dynamical systems,graphical models,nonparametric bayes},
month = nov,
number = {November},
title = {{Bayesian Nonparametric Methods for Learning Markov Switching Processes}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5563110},
year = {2010}
}
@article{Drugowitsch2010a,
author = {Drugowitsch, Jan and Pouget, Alexandre},
doi = {10.1038/nn0310-279},
file = {:Users/jonas/Documents/Mendeley Desktop/Drugowitsch, Pouget - 2010 - Quick thinking perceiving in a tenth of a blink of an eye.pdf:pdf},
issn = {1546-1726},
journal = {Nature neuroscience},
keywords = {Animals,Brain,Brain: physiology,Haplorhini,Models, Neurological,Neurons,Neurons: physiology,Neuropsychological Tests,Psychomotor Performance,Psychomotor Performance: physiology,Time Factors,Visual Perception,Visual Perception: physiology},
month = mar,
number = {3},
pages = {279--80},
pmid = {20177419},
publisher = {Nature Publishing Group},
title = {{Quick thinking: perceiving in a tenth of a blink of an eye.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20177419},
volume = {13},
year = {2010}
}
@article{Teifel2004,
address = {New York, New York, USA},
author = {Teifel, John and Manohar, Rajit},
doi = {10.1145/968280.968300},
file = {:Users/jonas/Documents/Mendeley Desktop/Teifel, Manohar - 2004 - Highly pipelined asynchronous FPGAs.pdf:pdf},
isbn = {1581138296},
journal = {Proceeding of the 2004 ACM/SIGDA 12th international symposium on Field programmable gate arrays - FPGA '04},
keywords = {asynchronous circuits,asynchronous logic,concurrency,correctness by construc-,novel architectures,pipelining,programmable logic,tion},
mendeley-tags = {asynchronous logic,novel architectures},
pages = {133},
publisher = {ACM Press},
title = {{Highly pipelined asynchronous FPGAs}},
url = {http://portal.acm.org/citation.cfm?doid=968280.968300},
year = {2004}
}
@article{Marcia2010,
author = {Marcia, R.F. and Willett, R.M. and Harmany, Z.T.},
file = {:Users/jonas/Documents/Mendeley Desktop/Marcia, Willett, Harmany - 2010 - Compressive Optical Imaging Architectures and Algorithms.pdf:pdf},
journal = {Optical and Digital Image Processing},
keywords = {compressed sensing,hardware,imaging},
mendeley-tags = {compressed sensing,hardware,imaging},
pages = {485--505},
publisher = {Wiley Online Library},
title = {{Compressive Optical Imaging: Architectures and Algorithms}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/9783527635245.ch22/summary},
year = {2010}
}
@article{Ma2006c,
abstract = {Recent psychophysical experiments indicate that humans perform near-optimal Bayesian inference in a wide variety of tasks, ranging from cue integration to decision making to motor control. This implies that neurons both represent probability distributions and combine those distributions according to a close approximation to Bayes' rule. At first sight, it would seem that the high variability in the responses of cortical neurons would make it difficult to implement such optimal statistical inference in cortical circuits. We argue that, in fact, this variability implies that populations of neurons automatically represent probability distributions over the stimulus, a type of code we call probabilistic population codes. Moreover, we demonstrate that the Poisson-like variability observed in cortex reduces a broad class of Bayesian inference to simple linear combinations of populations of neural activity. These results hold for arbitrary probability distributions over the stimulus, for tuning curves of arbitrary shape and for realistic neuronal variability.},
author = {Ma, Wei Ji and Beck, Jeffrey M and Latham, Peter E and Pouget, Alexandre},
doi = {10.1038/nn1790},
file = {:Users/jonas/Documents/Mendeley Desktop/Ma et al. - 2006 - Bayesian inference with probabilistic population codes.pdf:pdf},
issn = {1097-6256},
journal = {Nature neuroscience},
keywords = {Algorithms,Bayes Theorem,Cerebral Cortex,Cerebral Cortex: physiology,Humans,Models, Neurological,Models, Statistical,Nerve Net,Nerve Net: cytology,Nerve Net: physiology,Normal Distribution,Poisson Distribution},
month = nov,
number = {11},
pages = {1432--8},
pmid = {17057707},
title = {{Bayesian inference with probabilistic population codes.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17057707},
volume = {9},
year = {2006}
}
@article{Eldawlatly2010,
author = {Eldawlatly, S. and Zhou, Y. and Jin, R. and Oweiss, K.G.},
file = {:Users/jonas/Documents/Mendeley Desktop/Eldawlatly et al. - 2010 - On the use of dynamic Bayesian networks in reconstructing functional neuronal networks from spike train ensembles.pdf:pdf},
journal = {Neural computation},
keywords = {dbn,neural networks},
mendeley-tags = {dbn,neural networks},
number = {1},
pages = {158--189},
publisher = {MIT Press},
title = {{On the use of dynamic Bayesian networks in reconstructing functional neuronal networks from spike train ensembles}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/neco.2009.11-08-900},
volume = {22},
year = {2010}
}
@inproceedings{Klaas2005,
abstract = {Sequential Monte Carlo techniques are useful for state estimation in non-linear, non-Gaussian dynamic models. These methods allow us to approximate the joint posterior distribution using sequential importance sampling. In this framework, the dimension of the target distribution grows with each time step, thus it is necessary to introduce some resampling steps to ensure that the estimates provided by the algorithm have a reasonable variance. In many applications, we are only interested in the marginal filtering distribution which is defined on a space of fixed dimension. We present a Sequential Monte Carlo algorithm called the Marginal Particle Filter which operates directly on the marginal distribution, hence avoiding having to perform importance sampling on a space of growing dimension. Using this idea, we also derive an improved version of the auxiliary particle filter. We show theoretic and empirical results which demonstrate a reduction in variance over conventional particle filtering, and present techniques for reducing the cost of the marginal particle filter with N particles from O(N2) to O(N logN).},
author = {Klaas, Mike and Freitas, N De and Doucet, A},
booktitle = {Proceedings of the Twenty-First Conference on Uncertainty and Artificial Intelligence},
file = {:Users/jonas/Documents/Mendeley Desktop/Toward Practical N\^{}2 Monte Carlo.pdf:pdf},
title = {{Toward practical N\^{}2 Monte Carlo: The marginal particle filter}},
url = {http://arxiv.org/abs/1207.1396},
year = {2005}
}
@article{Wright2010,
author = {Wright, J. and Ma, Y. and Mairal, Julien and Sapiro, Guillermo and Huang, T.S. and Yan, Shuicheng},
file = {:Users/jonas/Documents/Mendeley Desktop/Wright et al. - 2010 - Sparse representation for computer vision and pattern recognition.pdf:pdf},
journal = {Proceedings of the IEEE},
keywords = {compressed sensing,imaging,vision},
mendeley-tags = {compressed sensing,imaging,vision},
number = {6},
pages = {1031--1044},
publisher = {IEEE},
title = {{Sparse representation for computer vision and pattern recognition}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5456194},
volume = {98},
year = {2010}
}
@article{Lyons2008,
archivePrefix = {arXiv},
arxivId = {arXiv:0811.1663v1},
author = {Lyons, Louis},
doi = {10.1214/08-AOAS163},
eprint = {arXiv:0811.1663v1},
file = {:Users/jonas/Documents/Mendeley Desktop/Lyons - 2008 - Open statistical issues in Particle Physics.pdf:pdf},
issn = {1932-6157},
journal = {The Annals of Applied Statistics},
keywords = {and phrases,background separation,blind analysis,combining results,goodness of fit,hypothesis testing,nuisance parameters,p-values,parameter determination,particle physics,signal-,statistics,upper limits},
mendeley-tags = {particle physics,statistics},
month = sep,
number = {3},
pages = {887--915},
title = {{Open statistical issues in Particle Physics}},
url = {http://projecteuclid.org/euclid.aoas/1223908045},
volume = {2},
year = {2008}
}
@article{Gilks2001,
author = {Gilks, WR and Berzuini, C},
file = {:Users/jonas/Documents/Mendeley Desktop/Following a Moving Target -- Monte Carlo inference for Dynamic Bayesian Models.pdf:pdf},
journal = {Journal of the Royal Statistical Society: \ldots},
keywords = {bayesian inference,dynamic model,hidden markov model,importance resampling,importance sampling,lter,markov chain monte carlo,methods,particle,predictive model selection,resample-move,sequential imputation,simulation,tracking},
mendeley-tags = {resample-move},
pages = {127--146},
title = {{Following a moving target - Monte Carlo inference for dynamic Bayesian models}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00280/abstract},
volume = {63},
year = {2001}
}
@article{Albert2009,
address = {New York, NY},
author = {Albert, Jim},
doi = {10.1007/978-0-387-92298-0},
file = {:Users/jonas/Documents/Mendeley Desktop/Albert - 2009 - Bayesian Computation with R.pdf:pdf},
isbn = {978-0-387-92297-3},
publisher = {Springer New York},
title = {{Bayesian Computation with R}},
url = {http://www.springerlink.com/index/10.1007/978-0-387-92298-0},
volume = {1},
year = {2009}
}
@article{Dobroiu2006,
author = {Dobroiu, Adrian and Otani, Chiko and Kawase, Kodo},
doi = {10.1088/0957-0233/17/11/R01},
file = {:Users/jonas/Documents/Mendeley Desktop/Dobroiu, Otani, Kawase - 2006 - Terahertz-wave sources and imaging applications.pdf:pdf},
keywords = {1,chemical imaging,electric field imaging,electromagnetics,imaging,more and more often,or see it,parametric generation,scattering,terahertz,terahertz radiation,thz,we hear the word,what is terahertz radiation},
mendeley-tags = {electromagnetics,imaging,thz},
pages = {161--174},
title = {{Terahertz-wave sources and imaging applications}},
volume = {17},
year = {2006}
}
@article{Henningsen2009,
abstract = {Single transverse mode oscillation is realized in a conventional HeNe laser outside the stability region of the optical resonator. Depending on the mirror separation different spatial modes can be generated. The mode volume of these modes is laterally limited by the diameter of the discharge capillary rather than by the beam waist of a stable Gaussian mode. Numerical solution of the Maxwell equations with appropriate boundary conditions shows good agreement with the observations. Such modes could potentially facilitate single transverse mode operation of waveguide lasers and fiber lasers.},
author = {Henningsen, Jes},
file = {:Users/jonas/Documents/Mendeley Desktop/Henningsen - 2009 - Non-Gaussian modes in a HeNe laser.pdf:pdf},
issn = {1094-4087},
journal = {Optics express},
month = nov,
number = {24},
pages = {21427--32},
pmid = {19997382},
title = {{Non-Gaussian modes in a HeNe laser.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19997382},
volume = {17},
year = {2009}
}
@article{Hardisty2010,
author = {Hardisty, Eric and Resnik, Philip},
file = {:Users/jonas/Documents/Mendeley Desktop/Hardisty, Resnik - 2010 - Gibbs Sampling for the Uninitiated.pdf:pdf},
journal = {Bernoulli},
keywords = {bayesian inference,gibbs,gibbs sampling,markov chain monte carlo,mcmc,na,naive bayes,ıve bayes},
mendeley-tags = {gibbs,mcmc,naive bayes},
number = {June},
title = {{Gibbs Sampling for the Uninitiated}},
year = {2010}
}
@article{Eldawlatly2010c,
abstract = {Coordination among cortical neurons is believed to be a key element in mediating many high-level cortical processes such as perception, attention, learning, and memory formation. Inferring the structure of the neural circuitry underlying this coordination is important to characterize the highly nonlinear, time-varying interactions between cortical neurons in the presence of complex stimuli. In this work, we investigate the applicability of dynamic Bayesian networks (DBNs) in inferring the effective connectivity between spiking cortical neurons from their observed spike trains. We demonstrate that DBNs can infer the underlying nonlinear and time-varying causal interactions between these neurons and can discriminate between mono- and polysynaptic links between them under certain constraints governing their putative connectivity. We analyzed conditionally Poisson spike train data mimicking spiking activity of cortical networks of small and moderately large size. The performance was assessed and compared to other methods under systematic variations of the network structure to mimic a wide range of responses typically observed in the cortex. Results demonstrate the utility of DBN in inferring the effective connectivity in cortical networks.},
author = {Eldawlatly, Seif and Zhou, Yang and Jin, Rong and Oweiss, Karim G},
doi = {10.1162/neco.2009.11-08-900},
file = {:Users/jonas/Documents/Mendeley Desktop/Eldawlatly et al. - 2010 - On the use of dynamic Bayesian networks in reconstructing functional neuronal networks from spike train ensembles(2).pdf:pdf},
issn = {1530-888X},
journal = {Neural computation},
keywords = {Action Potentials,Action Potentials: physiology,Algorithms,Bayes Theorem,Cerebral Cortex,Cerebral Cortex: physiology,Computer Simulation,Mathematical Computing,Mathematical Concepts,Nerve Net,Nerve Net: physiology,Neural Networks (Computer),Neural Pathways,Neural Pathways: physiology,Neurons,Neurons: physiology,Nonlinear Dynamics,Poisson Distribution,Signal Processing, Computer-Assisted,Synapses,Synapses: physiology,Synaptic Transmission,Synaptic Transmission: physiology},
month = jan,
number = {1},
pages = {158--89},
pmid = {19852619},
title = {{On the use of dynamic Bayesian networks in reconstructing functional neuronal networks from spike train ensembles.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2794930\&tool=pmcentrez\&rendertype=abstract},
volume = {22},
year = {2010}
}
@article{Scheel2009b,
author = {Scheel, Stefan},
doi = {10.1080/09500340802331849},
file = {:Users/jonas/Documents/Mendeley Desktop/Scheel - 2009 - Single-photon sources–an introduction.pdf:pdf},
issn = {0950-0340},
journal = {Journal of Modern Optics},
keywords = {cavity qed,cluster states,linear optical quantum computing,photons,review,sensing,single photons},
mendeley-tags = {photons,review,sensing},
month = jan,
number = {2-3},
pages = {141--160},
title = {{Single-photon sources–an introduction}},
url = {http://www.tandfonline.com/doi/abs/10.1080/09500340802331849},
volume = {56},
year = {2009}
}
@article{Mamassian2010a,
abstract = {How do we estimate the duration of a temporal interval in a familiar context? A new study finds that it is appropriate, perhaps even advantageous, to tolerate a small bias in our estimate to reduce the overall temporal uncertainty.},
author = {Mamassian, Pascal and Landy, Michael S},
doi = {10.1038/nn0810-914},
file = {:Users/jonas/Documents/Mendeley Desktop/Mamassian, Landy - 2010 - It's that time again.pdf:pdf},
issn = {1546-1726},
journal = {Nature neuroscience},
keywords = {Bayes Theorem,Humans,Psychomotor Performance,Psychomotor Performance: physiology,Time Perception,Time Perception: physiology},
month = aug,
number = {8},
pages = {914--6},
pmid = {20661267},
publisher = {Nature Publishing Group},
title = {{It's that time again.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20661267},
volume = {13},
year = {2010}
}
@article{Roweis1999,
author = {Roweis, Sam and Ghahramani, Zoubin},
doi = {10.1162/089976699300016674},
file = {:Users/jonas/Documents/Mendeley Desktop/089976699300016674.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
keywords = {ICA,LGSSM,gaussian,lingear,tutorial},
mendeley-tags = {gaussian,ICA,LGSSM,lingear,tutorial},
month = feb,
number = {2},
pages = {305--345},
title = {{A Unifying Review of Linear Gaussian Models}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/089976699300016674},
volume = {11},
year = {1999}
}
@article{Doshi-Velez2009,
address = {New York, New York, USA},
author = {Doshi-Velez, Finale and Ghahramani, Zoubin},
doi = {10.1145/1553374.1553409},
file = {:Users/jonas/Documents/Mendeley Desktop/Doshi-Velez, Ghahramani - 2009 - Accelerated sampling for the Indian Buffet Process.pdf:pdf},
isbn = {9781605585161},
journal = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
pages = {1--8},
publisher = {ACM Press},
title = {{Accelerated sampling for the Indian Buffet Process}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553409},
year = {2009}
}
@techreport{Doucet2008,
abstract = {Optimal estimation problems for non-linear non-Gaussian state-space models do not typically admit analytic solutions. Since their introduction in 1993, particle filtering methods have become a very popular class of algorithms to solve these estimation problems numerically in an online manner, i.e. recursively as observations become available, and are now routinely used in fields as diverse as computer vision, econometrics, robotics and navigation. The objective of this tutorial is to provide a complete, up-to-date survey of this field as of 2008. Basic and advanced particle methods for filtering as well as smoothing are presented.},
author = {Doucet, Arnaud and Johansen, Adam M},
file = {:Users/jonas/Documents/Mendeley Desktop/Doucet, Johansen - 2008 - A Tutorial on Particle Filtering and Smoothing Fifteen years later.pdf:pdf},
keywords = {Central Limit Theorem,Filtering,Hidden Markov Models,Markov chain Monte Carlo,Par- ticle methods,Resampling,SMC,Sequential Monte Carlo,Smoothing,State-Space models.,particle fitlering},
mendeley-tags = {particle fitlering,SMC},
pages = {4--6},
title = {{A Tutorial on Particle Filtering and Smoothing : Fifteen years later}},
year = {2008}
}
@article{Yau2011,
abstract = {We propose a hierarchical Bayesian nonparametric mixture model for clustering when some of the covariates are assumed to be of varying relevance to the clustering problem. This can be thought of as an issue in variable selection for unsupervised learning. We demonstrate that by defining a hierarchical population based nonparametric prior on the cluster locations scaled by the inverse covariance matrices of the likelihood we arrive at a 'sparsity prior' representation which admits a conditionally conjugate prior. This allows us to perform full Gibbs sampling to obtain posterior distributions over parameters of interest including an explicit measure of each covariate's relevance and a distribution over the number of potential clusters present in the data. This also allows for individual cluster specific variable selection. We demonstrate improved inference on a number of canonical problems.},
author = {Yau, Christopher and Holmes, Chris},
doi = {10.1214/11-BA612},
file = {:Users/jonas/Documents/Mendeley Desktop/Yau, Holmes - 2011 - Hierarchical Bayesian nonparametric mixture models for clustering with variable relevance determination.pdf:pdf},
issn = {1931-6690},
journal = {Bayesian analysis (Online)},
keywords = {bayesian mixture models,bayesian nonparametric priors,feature selection,lection,unsupervised learning,variable se-},
mendeley-tags = {feature selection},
month = jul,
number = {2},
pages = {329--352},
pmid = {21709771},
title = {{Hierarchical Bayesian nonparametric mixture models for clustering with variable relevance determination.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3121559\&tool=pmcentrez\&rendertype=abstract},
volume = {6},
year = {2011}
}
@misc{Jonas2011,
author = {Jonas, EM and Mansinghka, VK},
publisher = {US Patent Office},
title = {{Configurable Circuitry for Solving Stochastic Problems}},
year = {2011}
}
@article{Pitt2012,
author = {Pitt, Michael K. and Silva, Ralph Dos Santos and Giordani, Paolo and Kohn, Robert},
doi = {10.1016/j.jeconom.2012.06.004},
file = {:Users/jonas/Documents/Mendeley Desktop/1-s2.0-S0304407612001510-main.pdf:pdf},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {PMCMC},
mendeley-tags = {PMCMC},
month = jul,
publisher = {Elsevier B.V.},
title = {{On some properties of Markov chain Monte Carlo simulation methods based on the particle filter}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0304407612001510},
year = {2012}
}
@unpublished{DeSchutter1999,
author = {{De Schutter}, Joris and {De Geeter}, Jan and Lefebvre, Tine and Bruyninckx, Herman},
file = {:Users/jonas/Documents/Mendeley Desktop/kalman-filters-a-tutorial.pdf:pdf},
keywords = {kalman filter,tutorial},
mendeley-tags = {kalman filter,tutorial},
title = {{Kalman Filters: A Tutorial}},
url = {http://www.cs.ucf.edu/~mikel/Research/tutorials/kalman-filters-a-tutorial.pdf},
year = {1999}
}
@article{Fredkin1982,
author = {Fredkin, Edward and Toffoli, Tommaso},
doi = {10.1007/BF01857727},
file = {:Users/jonas/Documents/Mendeley Desktop/Fredkin, Toffoli - 1982 - Conservative logic.pdf:pdf},
issn = {0020-7748},
journal = {International Journal of Theoretical Physics},
keywords = {fredkin gate,novel architectures},
mendeley-tags = {fredkin gate,novel architectures},
month = apr,
number = {3-4},
pages = {219--253},
title = {{Conservative logic}},
url = {http://www.springerlink.com/index/10.1007/BF01857727},
volume = {21},
year = {1982}
}
@article{Jackson1999,
author = {Jackson, J. D.},
doi = {10.1119/1.19136},
file = {:Users/jonas/Documents/Mendeley Desktop/Jackson - 1999 - Classical Electrodynamics, 3rd ed.pdf:pdf},
issn = {00029505},
journal = {American Journal of Physics},
month = sep,
number = {9},
pages = {841},
title = {{Classical Electrodynamics, 3rd ed.}},
url = {http://link.aip.org/link/?AJP/67/841/2\&Agg=doi},
volume = {67},
year = {1999}
}
@article{Cooper2004,
author = {Cooper, Gregory},
file = {:Users/jonas/Documents/Mendeley Desktop/Cooper - 2004 - FrTime Functional reactive programming in PLT Scheme.pdf:pdf},
journal = {Computer science technical report. Brown University.},
keywords = {functional reactive programming,programming languages,scheme},
mendeley-tags = {functional reactive programming,programming languages,scheme},
title = {{FrTime: Functional reactive programming in PLT Scheme}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:FrTime+:+Functional+Reactive+Programming+in+PLT+Scheme\#0},
year = {2004}
}
@article{Jordan2010a,
author = {Jordan, Michael and Sudderth, Erik and Wainwright, Martin and Willsky, Alan},
doi = {10.1109/MSP.2010.938115},
file = {:Users/jonas/Documents/Mendeley Desktop/Jordan et al. - 2010 - Major Advances and Emerging Developments of Graphical Models From the Guest Editors.pdf:pdf},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
keywords = {graphical models},
mendeley-tags = {graphical models},
month = nov,
number = {6},
pages = {17--138},
title = {{Major Advances and Emerging Developments of Graphical Models [From the Guest Editors]}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5563108},
volume = {27},
year = {2010}
}
@article{Kalli2009a,
author = {Kalli, Maria and Griffin, Jim E. and Walker, Stephen G.},
doi = {10.1007/s11222-009-9150-y},
file = {::},
issn = {0960-3174},
journal = {Statistics and Computing},
keywords = {44-1227-823627,44-1227-827932,actuarial sci-,canterbury,corresponding author,dirichlet process,email,ence,fax,griffin,institute of mathematics,jim e,k,malized weights,markov chain monte carlo,mixture model,nor-,slice sampler,statistics,tel,u,university of kent},
month = sep,
number = {1},
pages = {93--105},
title = {{Slice sampling mixture models}},
url = {http://www.springerlink.com/index/10.1007/s11222-009-9150-y},
volume = {21},
year = {2009}
}
@book{Bishop2006a,
annote = {The Chapter on sampling methods is a great introduction},
author = {Bishop, C.M.},
booktitle = {Annals of Physics},
file = {::},
isbn = {9780387310732},
keywords = {mustread},
mendeley-tags = {mustread},
number = {2},
publisher = {Springer New York},
title = {{Pattern Recognition and Machine Learning}},
url = {http://www.mendeley.com/research/no-title-avail/ http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
volume = {4},
year = {2006}
}
@article{Andrieu2010d,
annote = {What: Particle methods for making improved proposal distributions in high dimensional spaces

        
Conjugate: N/A (general MCMC method; it would have to be adapted for CRP use)

        
Real?:

        
Valid?:

        
Tests:

        
Particle methods are often used to approximate various distributions, so here the idea is to use a particle method to approximate a conditional distribution for use in a Gibbs step.  In the HMM case (their running example throughout the paper) the naive SMC algorithm does not produce an exact sample from that distribution.  But in sectoin 2.4.3 they describe a "particle Gibbs" sampler which has the conditional distribution as an invariant density and "is ergodic under mild assumptions" which are hopefully laid out in section 4.

        

        
Section 2: describes the algorithm
Section 3: demonstrates the algorithm on 2 models
Section 4: Theoretical justification},
author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
doi = {10.1111/j.1467-9868.2009.00736.x},
file = {:Users/jonas/Documents/Mendeley Desktop/Particle Markov chain Monte Carlo methods.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {bayesian inference,markov chain monte carlo,mcmc,methods,sequential monte carlo,smc,state space models},
mendeley-tags = {mcmc,smc},
month = jun,
number = {3},
pages = {269--342},
title = {{Particle Markov chain Monte Carlo methods}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2009.00736.x},
volume = {72},
year = {2010}
}
@phdthesis{Bonawitz2008,
annote = {Don't worry about the details of the Blaise VM, but it's an excellent overview of MCMC, the SDK representation that inspires how we think about a lot of things; and state of the art models in this space},
author = {Bonawitz, Keith Allen},
booktitle = {Development},
file = {::},
keywords = {mcmc,mustread,sdk},
mendeley-tags = {mcmc,mustread,sdk},
school = {MIT},
title = {{Composable Probabilistic Inference with Blaise}},
url = {http://people.csail.mit.edu/bonawitz/Composable Probabilistic Inference with Blaise - Keith Bonawitz PhD Thesis.pdf},
year = {2008}
}
@article{Teh2006,
address = {Morristown, NJ, USA},
author = {Teh, Yee Whye},
doi = {10.3115/1220175.1220299},
file = {::},
journal = {Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL - ACL '06},
pages = {985--992},
publisher = {Association for Computational Linguistics},
title = {{A hierarchical Bayesian language model based on Pitman-Yor processes}},
url = {http://portal.acm.org/citation.cfm?doid=1220175.1220299},
year = {2006}
}
@article{Hardisty2010a,
author = {Hardisty, Eric and Resnik, Philip},
file = {::},
journal = {Bernoulli},
keywords = {bayesian inference,gibbs,gibbs sampling,markov chain monte carlo,mcmc,na,naive bayes,ıve bayes},
mendeley-tags = {gibbs,mcmc,naive bayes},
number = {June},
title = {{Gibbs Sampling for the Uninitiated}},
year = {2010}
}
@article{Jain2007b,
author = {Jain, Sonia and Neal, Radford M},
file = {::},
journal = {Bayesian Analysis},
keywords = {conjugacy,dpmm,gibbs,mcmc,splitmerge},
mendeley-tags = {conjugacy,dpmm,gibbs,mcmc,splitmerge},
number = {3},
pages = {495--500},
title = {{The Role of Conditional Conjugacy in Our Algorithm}},
year = {2007}
}
@article{Guha2010b,
annote = {What: Generalized Polya urn process, a generalization of many different mixture model types.

        
Conugate?:

        
Real?:

        
Valid?:

        
Tests:

        

        

        
The abstract has no specific claims about the gains of this conceptualization other than mentioning that they are "versatile" and "impressive".

        
"Many important mixture models cannot be expressed as [definition of GPU process] ... for instance Pitman-Yor processes"

        
The paper has no conclusion.},
author = {Guha, Subharup},
doi = {10.1198/jasa.2010.tm09340},
file = {::},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {data squashing,dirichlet process,dirichlet process mixture model,generalized p\'{o}lya urn process,hidden markov model,inference,markov chain monte carlo,semiparametric bayes},
mendeley-tags = {dirichlet process mixture model,inference},
month = jun,
number = {490},
pages = {775--786},
title = {{Posterior Simulation in Countable Mixture Models for Large Datasets}},
url = {http://pubs.amstat.org/doi/abs/10.1198/jasa.2010.tm09340},
volume = {105},
year = {2010}
}
@techreport{Doucet2008a,
abstract = {Optimal estimation problems for non-linear non-Gaussian state-space models do not typically admit analytic solutions. Since their introduction in 1993, particle filtering methods have become a very popular class of algorithms to solve these estimation problems numerically in an online manner, i.e. recursively as observations become available, and are now routinely used in fields as diverse as computer vision, econometrics, robotics and navigation. The objective of this tutorial is to provide a complete, up-to-date survey of this field as of 2008. Basic and advanced particle methods for filtering as well as smoothing are presented.},
author = {Doucet, Arnaud and Johansen, Adam M},
file = {::},
keywords = {Central Limit Theorem,Filtering,Hidden Markov Models,Markov chain Monte Carlo,Par- ticle methods,Resampling,SMC,Sequential Monte Carlo,Smoothing,State-Space models.,particle fitlering},
mendeley-tags = {particle fitlering,SMC},
pages = {4--6},
title = {{A Tutorial on Particle Filtering and Smoothing : Fifteen years later}},
year = {2008}
}
@article{Sawyer2007,
author = {Sawyer, Stanley and April, Vs},
file = {::},
pages = {1--12},
title = {{Wishart Distributions and Inverse-Wishart Sampling}},
volume = {2},
year = {2007}
}
@article{Jainb,
abstract = {The inferential problem of associating data to mixture components is dif- ficult when components are nearby or overlapping. We introduce a new split-merge Markov chain Monte Carlo technique that efficiently classifies observations by splitting and merging mixture components of a nonconjugate Dirichlet process mixture model. Our method, which is a Metropolis-Hastings procedure with split-merge proposals, sam- ples clusters of observations simultaneously rather than incrementally assigning observa- tions to mixture components. Split-merge moves are produced by exploiting properties of a restricted Gibbs sampling scan. A simulation study compares the new split-merge technique to a nonconjugate version of Gibbs sampling and an incremental Metropolis- Hastings technique. The results demonstrate the improved performance of the new sampler.},
annote = {What: Splitmerge sampler for nonconjugate models where conditional conjugacy is satsified

        
Conjugate: Conditional conjugacy

        
Real?:

        
Valid?:

        
Tests:},
author = {Jain, Sonia and Neal, R.M.},
file = {::},
journal = {Bayesian Analysis},
keywords = {Bayesian model,Markov chain Monte Carlo,dpmm,mcmc,nonconjugate,nonconjugate prior,split-merge moves,splitmerge},
mendeley-tags = {dpmm,mcmc,nonconjugate,splitmerge},
number = {3},
pages = {445--472},
title = {{Splitting and merging components of a nonconjugate Dirichlet process mixture model}},
url = {http://ba.stat.cmu.edu/journal/2007/vol02/issue03/jain.pdf},
volume = {2}
}
@article{Goswami2007b,
annote = {What: A parallel tempering evolutionary MCMC sampler

        
Conjugate?:

        
Real?:

        
Valid?:

        
Extend the state space to include N 'individuals' which exist at different temperatures.  Mutate the individuals according to their own temperatur-dependent distributions, or occasionally perform crossovers between them.

        
Tests: Average Integrated Autocorrelation Time (AIAT), integrated autocorrelation time (IAT), Average Maximum Log density, rejection rate,

        

        
They test their sampler on the same beta-bernoulli dataset as Jain\&Neal 2004, so useful comparisons can be made there.

        
What is integrated autocorrelation time?},
author = {Goswami, Gopi and Liu, Jun S and Wong, Wing H},
doi = {10.1198/106186007X255072},
file = {::},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {dirichlet process mixture model,gibbs sampling,hastings algorithm,integrated auto-correlation,k -means,metropolis,model-based clustering,parallel tem-,pering,temperature ladder,time,variable selection},
month = dec,
number = {4},
pages = {855--876},
title = {{Evolutionary Monte Carlo Methods for Clustering}},
url = {http://pubs.amstat.org/doi/abs/10.1198/106186007X255072},
volume = {16},
year = {2007}
}
@techreport{Neal1998b,
annote = {What: A collection of nonconjugate algorithms

        
Conjugate?:  Non-conjugate

        
Real?: They are compared to each other, with Algorithms 5 and 8 performing best there; it's hard to benchmark since so little seems to work in this space at all.

        
Valid?: Yes

        
Tests: Autocorrelation time, compute time per iteration.

        
Various algorithms for proposing moves in nonconjugate DPMM space.

      },
author = {Neal, Radford M.},
file = {::},
institution = {University of Toronto},
keywords = {dpmm,gibbs,mcmc},
mendeley-tags = {dpmm,gibbs,mcmc},
title = {{Markov Chain Sampling Methods for Dirichlet Process Mixture Models}},
year = {1998}
}
@article{VanGael2008b,
address = {New York, New York, USA},
author = {{Van Gael}, Jurgen and Saatci, Yunus and Teh, Yee Whye and Ghahramani, Zoubin},
doi = {10.1145/1390156.1390293},
file = {::},
isbn = {9781605582054},
journal = {Proceedings of the 25th international conference on Machine learning - ICML '08},
keywords = {beam,hdphmm,mcmc},
mendeley-tags = {beam,hdphmm,mcmc},
pages = {1088--1095},
publisher = {ACM Press},
title = {{Beam sampling for the infinite hidden Markov model}},
url = {http://portal.acm.org/citation.cfm?doid=1390156.1390293},
year = {2008}
}
@article{Neal2000a,
author = {Neal, Radford M.},
doi = {10.2307/1390653},
file = {::},
issn = {10618600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {crp,mcmc,mixtures,nonparametric bayes},
mendeley-tags = {crp,mcmc,mixtures,nonparametric bayes},
month = jun,
number = {2},
pages = {249},
title = {{Markov Chain Sampling Methods for Dirichlet Process Mixture Models}},
url = {http://www.jstor.org/stable/1390653?origin=crossref},
volume = {9},
year = {2000}
}
@article{Albert2009a,
address = {New York, NY},
author = {Albert, Jim},
doi = {10.1007/978-0-387-92298-0},
file = {::},
isbn = {978-0-387-92297-3},
publisher = {Springer New York},
title = {{Bayesian Computation with R}},
url = {http://www.springerlink.com/index/10.1007/978-0-387-92298-0},
volume = {1},
year = {2009}
}
@article{DaumeIII2009b,
author = {{Daum\'{e} III}, H.},
file = {::},
journal = {Arxiv preprint arXiv:0907.1812},
keywords = {dpmm,map,search},
mendeley-tags = {dpmm,map,search},
number = {1},
title = {{Fast search for Dirichlet process mixture models}},
url = {http://arxiv.org/abs/0907.1812},
year = {2009}
}
@article{Jain2004b,
author = {Jain, Sonia and Neal, R.M.},
file = {::},
institution = {Department Of Statistics, University of Toronto},
journal = {Journal of Computational and Graphical Statistics},
keywords = {conjugacy,dpmm,mcmc,splitmerge},
mendeley-tags = {conjugacy,dpmm,mcmc,splitmerge},
number = {1},
pages = {158--182},
publisher = {ASA},
title = {{A split-merge Markov chain Monte Carlo procedure for the Dirichlet process mixture model}},
url = {http://pubs.amstat.org/doi/pdf/10.1198/1061860043001},
volume = {13},
year = {2004}
}
@article{Dahl2007b,
author = {Dahl, David B},
doi = {10.1214/07-BA219B},
file = {::},
issn = {1931-6690},
journal = {Bayesian Analysis},
keywords = {conjugacy,dpmm},
mendeley-tags = {conjugacy,dpmm},
number = {3},
pages = {479--482},
title = {{Comment on Article by Jain and Neal}},
url = {http://ba.stat.cmu.edu/journal/2007/vol02/issue03/robert.pdf},
volume = {2},
year = {2007}
}
@article{Maceachern2007b,
author = {Maceachern, Steven N},
doi = {10.1214/07-BA219B},
file = {::},
issn = {1931-6690},
journal = {Bayesian Analysis},
keywords = {conjugate,dpmm,mcmc,nonconjugate,splitmerge},
mendeley-tags = {conjugate,dpmm,mcmc,nonconjugate,splitmerge},
number = {3},
pages = {479--482},
title = {{Comment on Article by Jain and Neal}},
url = {http://ba.stat.cmu.edu/journal/2007/vol02/issue03/robert.pdf},
volume = {2},
year = {2007}
}
@article{Liang2007a,
address = {New York, New York, USA},
author = {Liang, Percy and Jordan, Michael I. and Taskar, Ben},
doi = {10.1145/1273496.1273565},
file = {::},
isbn = {9781595937933},
journal = {Proceedings of the 24th international conference on Machine learning - ICML '07},
keywords = {augmentation,dpmm,mcmc,splitmerge},
mendeley-tags = {augmentation,dpmm,mcmc,splitmerge},
pages = {545--552},
publisher = {ACM Press},
title = {{A permutation-augmented sampler for DP mixture models}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273565},
year = {2007}
}
@article{Vinyals2007,
author = {Vinyals, O and Friedland, G and Mirghafori, N},
file = {::},
number = {510},
title = {{Revisiting a basic function on current CPUs : A fast logarithm implementation with adjustable accuracy}},
year = {2007}
}
@article{Walker2007c,
annote = {What: A slice sampler for DPMM 

        
Conjugate?: Both (theta is sampled as part of the complete likelihood)

        
Real?: I'm not super convinced by their demonstration of the method.  Their posterior predictive distribution looks fine but the only other evaluation metric they provide is a running average number of clusters as a function of time, and the running average is 4, whereas visually there are clearly 3 clusters.  More explanation of that, and a better picture of the chain in general, would be needed to help convince me that mixing is happening.  Also, the variable-at-a-time indicator variable sampling is known to be problematic (thus the proliferation of split-merge samplers).  Perhaps that could be modified?

        
Valid?: Looks good to me

        
Tests: running average number of cluster as a function of time; data prediction from the posterior

        
Unusually, this sampler maintains a representation of the usually-integrated-out measure P (sometimes called G).  

        
Auxiliary variables u, theta, and v/w are introduced and gibbsed over.

        
It seems like this method could likely generalize to include the pitman-yor process -- Does PY have a stick-breaking construction?  If so, then it should generalize.

        
Their test dataset had 3 clusters but their sampler converges soundly to 4.  Why?  No explanation of this. Also why number of clusters takes on non-integer values?  Ah, it is a running average.  That is much less informative to me than a time-series plot.},
author = {Walker, Stephen G.},
doi = {10.1080/03610910601096262},
file = {::},
isbn = {0361091060},
issn = {0361-0918},
journal = {Communications in Statistics - Simulation and Computation},
month = jan,
number = {1},
pages = {45--54},
title = {{Sampling the Dirichlet Mixture Model with Slices}},
url = {http://www.tandfonline.com/doi/abs/10.1080/03610910601096262},
volume = {36},
year = {2007}
}
@techreport{Neal2010a,
author = {Neal, Radford M.},
booktitle = {Arxiv preprint arXiv:1101.0387},
file = {::},
institution = {University of Toronto},
keywords = {mcmc},
mendeley-tags = {mcmc},
title = {{MCMC Using Ensembles of States for Problems with Fast and Slow Variables such as Gaussian Process Regression}},
url = {http://arxiv.org/abs/1101.0387},
year = {2010}
}
@article{Chib1995a,
author = {Chib, Siddhartha and Greenberg, Edward},
file = {::},
journal = {The American Statistician},
keywords = {mcmc,mh},
mendeley-tags = {mcmc,mh},
number = {4},
pages = {327},
title = {{Understanding the Metropolis-Hastings Algorithm}},
volume = {49},
year = {1995}
}
@article{Kalli2009c,
author = {Kalli, Maria and Griffin, Jim E. and Walker, Stephen G.},
doi = {10.1007/s11222-009-9150-y},
file = {::},
issn = {0960-3174},
journal = {Statistics and Computing},
keywords = {44-1227-823627,44-1227-827932,actuarial sci-,canterbury,corresponding author,dirichlet process,email,ence,fax,griffin,institute of mathematics,jim e,k,malized weights,markov chain monte carlo,mixture model,nor-,slice sampler,statistics,tel,u,university of kent},
month = sep,
number = {1},
pages = {93--105},
title = {{Slice sampling mixture models}},
url = {http://www.springerlink.com/index/10.1007/s11222-009-9150-y},
volume = {21},
year = {2009}
}
@article{Jain2004c,
author = {Jain, Sonia and Neal, R.M.},
file = {::},
institution = {Department Of Statistics, University of Toronto},
journal = {Journal of Computational and Graphical Statistics},
keywords = {conjugacy,dpmm,mcmc,splitmerge},
mendeley-tags = {conjugacy,dpmm,mcmc,splitmerge},
number = {1},
pages = {158--182},
publisher = {ASA},
title = {{A split-merge Markov chain Monte Carlo procedure for the Dirichlet process mixture model}},
url = {http://pubs.amstat.org/doi/pdf/10.1198/1061860043001},
volume = {13},
year = {2004}
}
@article{Liang2007d,
address = {New York, New York, USA},
annote = {What: A cluster-group-at-a-time DPMM sampler with an auxiliary variable that reduces the reclusterings considered.

        
Conjugate? Yes

        
Real?  Results only show improvement during burn-in, at the expense of a quite complicated algorithm

        
Valid?  Unclear.  There are many variations on the main idea discussed, some of which are described as heuristic, but at least for the version he tests, he claims it is valid MCMC

        
Tests: Separate testing of the burnin phase and the mixing phase

        

        

        

        
Claims to reduce burnin time while being competitive in mixing},
author = {Liang, Percy and Jordan, Michael I. and Taskar, Ben},
doi = {10.1145/1273496.1273565},
file = {::},
isbn = {9781595937933},
journal = {Proceedings of the 24th international conference on Machine learning - ICML '07},
keywords = {augmentation,dpmm,mcmc,splitmerge},
mendeley-tags = {augmentation,dpmm,mcmc,splitmerge},
pages = {545--552},
publisher = {ACM Press},
title = {{A permutation-augmented sampler for DP mixture models}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273565},
year = {2007}
}
@techreport{Dahlb,
abstract = {The Gibbs sampler is the standard Markov chain Monte Carlo sampler for drawing samples from the posterior distribution of conjugate Dirichlet process mixture models. Researchers have noticed the Gibbs sampler’s tendency to get stuck in local modes and, thus, poorly explore the posterior distribution. Jain and Neal (2004) proposed a merge-split sampler in which a naive random split is sweetened by a series of restricted Gibbs scans, where the number of Gibbs scans is a tuning parameter that must be supplied by the user. In this work, I propose an alternative merge-split sampler borrowing ideas from sequential importance sampling. My sampler proposes splits by sequentially allocating observations to one of two split components using allocation probabilities that are conditional on previously allocated data. The algorithm does not require further sweetening and is, hence, computa- tionally efficient. In addition, no tuning parameter needs to be chosen. While the conditional allocation of observations is similar to sequential importance sampling, the output from the sampler has the correct stationary distribution due to the use of the Metropolis-Hastings ratio. The computational efficiency of my sequentially-allocated merge-split (SAMS) sampler is compared to Jain and Neal’s sampler using various values for the tuning parameter. Compar- isons are made in terms of autocorrelation times for four univariate summaries of the Markov chains taken at fixed time intervals. In four examples involving different models and datasets, I show that my merge-split sampler usually performs substantially better—in some cases, two to five times faster—than existing methods, and never performs worse.},
annote = {What: Merge-split sampler for DPMM with proposals inspired by sequential importance sampling

        
Conjugate?:  Conjugate

        
Real?:

        
Valid?:

        
Tests: autocorrelation time of univariate summaries: number of components, size of largest component, log posterior pdf, entropy

        
Propose splits by randomly permuting the observations and then sequentially assigning observations to the two sides of the split according to a conditional posterior.

        
Autocorrelation time is the only test.    What is the significance of Monte Carlo Standard Error?},
author = {Dahl, David B},
file = {::},
keywords = {Conjugate Dirichlet process mixture model,Markov chain Monte Carlo,Metropolis- Hastings algorithm,conjugate,dpmm,mcmc,sequential importance sampling.,split-merge updates,splitmerge},
mendeley-tags = {conjugate,dpmm,mcmc,splitmerge},
title = {{An Improved Merge-Split Sampler for Conjugate Dirichlet Process Mixture Models}}
}
@article{Blei2009b,
annote = {What: Distance dependent CRP is a generalization of CRP in which customers are not in general exchangeable.  Resulting Gibbs sampler which also works well for normal CRP

        
Conjugate?: Conjugate

        
Real?: Hard to tell from the tests

        
Valid?: yes

        
Tests: Compare log probability of MAP estimates

        

        
CRP is re-envisioned as: have each customer link itself to some specific previous customer instead of to a table.  Gibbs over customer linkages instead of over table assignments.  A single customer linkage change can split/merge groups, for example.

        
I don't really understand the point of the fully observed (word-level) language learning task.  The only unobserved variable seems to be the clustering, but each table can only have one word at it, so the clustering is already almost determined -- just a fragmentation of the word-level clustering.

        
I believe that the test "plot the log probability of the MAP estimate of the partition structure under the CRP for each sampler [after it has run for 1000 iterations]" means that for each sampler run, the experimenter took the maximum posterior likelihood sample from the chain and called that the "MAP estimate of the partition structure under the CRP [for that sampler]".  (In the other direction it doesn't make much sense -- starting with the true MAP estimate for the posterior (who knows if that can even be analytically computed) and figuring out what probability it is assigned according to each sampler run?  Unclear how to do this, and a bad test even if actually implemented).  But finding a high probability estimate doesn't mean a chain is mixing.  We learned that from ADD testing.},
author = {Blei, DM},
file = {::},
journal = {Arxiv preprint arXiv:0910.1022},
title = {{Distance dependent Chinese restaurant processes}},
url = {http://arxiv.org/abs/0910.1022},
year = {2009}
}
@article{Jain2004e,
abstract = {This article proposes a split-mergeMarkov chain algorithmto address the problemof inef cient sampling for conjugate Dirichlet process mixture models. Traditional Markov chain Monte Carlo methods for Bayesian mixture models, such as Gibbs sampling, can become trapped in isolated modes corresponding to an inappropriate clustering of data points. This article describes a Metropolis-Hastingsprocedure that can escape such local modesby splittingormergingmixturecomponents.Our algorithmemploysa newtechnique in which an appropriate proposal for splitting or merging components is obtained by using a restrictedGibbs sampling scan.We demonstrateempirically that ourmethod outperforms the Gibbs sampler in situationswhere two or more components are similar in structure.},
annote = {What: A split-merg algorithm family which they claim substantially outperforms component-at-a-time Gibbs

        
Conjugate?: Yes

        
Real?: Likely.  the paper is highly cited

        
Valid?: Yes.  see below

        
TEsts: Trace plots, compute time per iteration, burn-in time, autocorrelation times.

        
Pick two components at random.  If they are the same component, propose a split (cleverly, to maximise its chances of being accepted).  If they are different components, propose a merge.

        
The split proposal uses some MH ninja positioning to make itself simultaneously tractable and a valid MH move.  Basically, they have a proposal kernel that they want to use, but the transition probabilities are intractable.  So instead they split the work of their proposal kernel into two parts, the "launch state determining" part and the "proposal" part.  

        
They consider their move to be a mixture kernel which is a mix of unknown proportions between the different possible launch states.   A launch state (elementary kernel) consists of a pair of observations (i,j) and a deterministic mapping from subsets of observations S (where i,j not in S) to splits of S (where a split of S is an assigment of each observation in S to either the i-cluster or the j-cluster).  We do not have explicit weights with which these elementary kernels are mixed, but we do have a generative process for selecting a launch state.  Once a launch state is selected, the kernel which corresponds to that launch state proceeds as follows.  1) If in the current state, i and j are in different components, propose to merge 2) If in the current state, i and j are in the same component, propose a split.  There will be nonzero probability of going back and forth between the merged and proposed states in both 1 and 2.

        
They test on two beta-bernoulli example datasets where they feel they have a good sense for the posterior.  The algorithm has certain parameters which need tuning and they test those.  Plots where the x-axis is "iterations" look promising for understanding something about the mixing properties of the chain.},
author = {Jain, Sonia and Neal, Radford M},
doi = {10.1198/1061860043001},
file = {::},
isbn = {1061860043001},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {conjugate,crp,dpmm,gibbs,gibbs sampler,latent class analysis,mcmc,metropolis,metropolis-hastings algorithm},
mendeley-tags = {conjugate,crp,dpmm,gibbs,mcmc},
month = mar,
number = {1},
pages = {158--182},
title = {{A Split-Merge Markov chain Monte Carlo Procedure for the Dirichlet Process Mixture Model}},
url = {http://pubs.amstat.org/doi/abs/10.1198/1061860043001},
volume = {13},
year = {2004}
}
@article{Griffiths2011a,
annote = {Still one of the best introductions to the CRP. 

      },
author = {Griffiths, Thomas L and Ghahramani, Zoubin},
file = {::},
journal = {Journal of Machine Learning Research},
keywords = {beta process,chinese,exchangeable distributions,ibp,latent variable models,markov chain monte carlo,mcmc,mustread,nonparametric bayes,restaurant processes,sparse binary matrices},
mendeley-tags = {ibp,mcmc,mustread,nonparametric bayes},
pages = {1185--1224},
title = {{The Indian Buffet Process : An Introduction and Review}},
volume = {12},
year = {2011}
}
@article{Ulker2010b,
author = {Ulker, Yener and Gunsel, Bilge and Cemgil, Ali Taylan},
doi = {10.1109/ICPR.2010.688},
file = {::},
isbn = {978-1-4244-7542-1},
journal = {2010 20th International Conference on Pattern Recognition},
keywords = {-bayesian nonparametrics,annealing,dirichlet process mix-,dpmm,sequential monte carlo,smc,ture},
mendeley-tags = {annealing,dpmm,smc},
month = aug,
number = {1},
pages = {2808--2811},
publisher = {Ieee},
title = {{Annealed SMC Samplers for Dirichlet Process Mixture Models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5597024},
year = {2010}
}
@article{Liang2007c,
address = {New York, New York, USA},
author = {Liang, Percy and Jordan, Michael I. and Taskar, Ben},
doi = {10.1145/1273496.1273565},
file = {::},
isbn = {9781595937933},
journal = {Proceedings of the 24th international conference on Machine learning - ICML '07},
keywords = {augmentation,dpmm,mcmc,splitmerge},
mendeley-tags = {augmentation,dpmm,mcmc,splitmerge},
pages = {545--552},
publisher = {ACM Press},
title = {{A permutation-augmented sampler for DP mixture models}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273565},
year = {2007}
}
@techreport{Walsh2004a,
abstract = {Lecture notes for EEB 581},
author = {Walsh, B},
booktitle = {Notes},
file = {::},
keywords = {mcmc,tutorial},
mendeley-tags = {mcmc,tutorial},
number = {April},
title = {{Markov Chain Monte Carlo and Gibbs Sampling}},
year = {2004}
}
@unpublished{Dahl2005b,
abstract = {This paper proposes a new efficient merge-split sampler for both conjugate and nonconju- gate Dirichlet process mixture (DPM) models. These Bayesian nonparametric models are usually fit using Markov chain Monte Carlo (MCMC) or sequential importance sampling (SIS). The latest generation of Gibbs and Gibbs-like samplers for both conjugate and nonconjugate DPM models effectively update the model parameters, but can have difficulty in updating the clustering of the data. To overcome this deficiency, merge-split samplers have been developed, but until now these have been limited to conjugate or conditionally-conjugate DPM models. This paper proposes a new MCMC sampler, called the sequentially-allocated merge-split (SAMS) sampler. The sam- pler borrows ideas from sequential importance sampling. Splits are proposed by sequentially allocating observations to one of two split components using allocation probabilities that condi- tion on previously allocated data. The SAMS sampler is applicable to general nonconjugateDPM models as well as conjugate models. Further, the proposed sampler is substantially more efficient than existing conjugate and nonconjugate samplers.},
author = {Dahl, D.B.},
booktitle = {Journal of Computational and Graphical Statistics},
file = {::},
keywords = {conjugate,dpmm,gibbs,mcmc,nonconjugate,splitmerge},
mendeley-tags = {conjugate,dpmm,gibbs,mcmc,nonconjugate,splitmerge},
number = {2004},
publisher = {Citeseer},
title = {{Sequentially-allocated merge-split sampler for conjugate and nonconjugate Dirichlet process mixture models}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.131.3712\&amp;rep=rep1\&amp;type=pdf},
year = {2005}
}
@article{Walker2007b,
author = {Walker, Stephen G.},
doi = {10.1080/03610910601096262},
file = {::},
isbn = {0361091060},
issn = {0361-0918},
journal = {Communications in Statistics - Simulation and Computation},
month = jan,
number = {1},
pages = {45--54},
title = {{Sampling the Dirichlet Mixture Model with Slices}},
url = {http://www.tandfonline.com/doi/abs/10.1080/03610910601096262},
volume = {36},
year = {2007}
}
@phdthesis{Andrieu2009a,
author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
doi = {10.1111/j.1467-9868.2009.00736.x},
file = {::},
issn = {13697412},
keywords = {dpmm,mcmc,smc},
mendeley-tags = {dpmm,mcmc,smc},
month = jun,
number = {3},
title = {{Particle Markov chain Monte Carlo methods}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2009.00736.x},
volume = {72},
year = {2009}
}
@article{Hardisty2010b,
author = {Hardisty, Eric and Resnik, Philip},
file = {::},
journal = {Bernoulli},
keywords = {bayesian inference,gibbs,gibbs sampling,markov chain monte carlo,mcmc,na,naive bayes,ıve bayes},
mendeley-tags = {gibbs,mcmc,naive bayes},
number = {June},
title = {{Gibbs Sampling for the Uninitiated}},
year = {2010}
}
@article{Neal2012b,
author = {Neal, Radford M},
file = {::},
keywords = {inference,mcmc,theory},
mendeley-tags = {inference,mcmc,theory},
number = {1201},
pages = {1--42},
title = {{How to View an MCMC Simulation as a Permutation, with Applications to Parallel Simulation and Improved Importance Sampling}},
year = {2012}
}
@article{Wang2011b,
author = {Wang, Lianming},
doi = {10.1198/jcgs.2010.07081},
file = {::},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {clustering,density estimation,dpmm,efficient computation,large samples,map,nonparametric bayes,p\'{o}lya urn scheme,search,sequential analysis},
mendeley-tags = {dpmm,map,search},
month = mar,
number = {1},
pages = {196--216},
title = {{Fast Bayesian Inference in Dirichlet Process Mixture Models}},
url = {http://pubs.amstat.org/doi/abs/10.1198/jcgs.2010.07081},
volume = {20},
year = {2011}
}
@article{Neal2003b,
author = {Neal, R.M.},
doi = {10.1214/aos/1056562461},
file = {::},
issn = {0090-5364},
journal = {Annals of Statistics},
keywords = {mcmc,slice},
mendeley-tags = {mcmc,slice},
month = jun,
number = {3},
pages = {705--741},
publisher = {JSTOR},
title = {{Slice sampling}},
url = {http://projecteuclid.org/Dienst/getRecord?id=euclid.aos/1056562461/ http://www.jstor.org/stable/10.2307/3448413},
volume = {31},
year = {2003}
}
@article{Banerjee1979,
abstract = {Some Bayesian results are derived for the inverse Gaussian family of distributions with non- informative reference prior as well as the natural conjugate prior. With a particular parameter- ization, the posterior distributions are found to have remarkable similarities with the corre- sponding results for the normal model. Finally, an application of the Bayesian results is given toward analyzing some equipment failure data.},
author = {Banerjee, Asit K and Bhattacharyya, G. K.},
file = {::},
issn = {1600-5740},
journal = {Technometrics},
keywords = {bayesian inference,inverse gaussian distribution},
month = aug,
number = {2},
pages = {247--251},
title = {{Bayesian Results for the Inverse Gaussian Distribution with an Application}},
volume = {21},
year = {1979}
}
@incollection{Andrieu2008a,
address = {Berlin, Heidelberg},
author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
booktitle = {Monte Carlo and Quasi-Monte Carlo Methods},
doi = {10.1007/978-3-642-04107-5\_3},
editor = {{L' Ecuyer}, Pierre and Owen, Art B.},
file = {::},
isbn = {978-3-642-04106-8},
keywords = {mcmc,smc},
number = {Mcmc},
publisher = {Springer Berlin Heidelberg},
title = {{Particle Markov Chain Monte Carlo for Efficient Numerical Simulation}},
url = {http://www.springerlink.com/index/10.1007/978-3-642-04107-5},
year = {2008}
}
@article{Robert2007b,
author = {Robert, C P},
doi = {10.1214/07-BA219B},
file = {::},
issn = {1931-6690},
journal = {Bayesian Analysis},
keywords = {dpmm,mcmc,splitmerge},
mendeley-tags = {dpmm,mcmc,splitmerge},
number = {3},
pages = {479--482},
title = {{Comment on Article by Jain and Neal}},
url = {http://ba.stat.cmu.edu/journal/2007/vol02/issue03/robert.pdf},
volume = {2},
year = {2007}
}
@article{Murphy2007,
annote = {Kevin Murphy is the BOMB for doing all this work. Great introduction to conjugate models. },
author = {Murphy, Kevin P},
file = {::},
journal = {Work},
keywords = {bayes,conjugacy,gaussian},
mendeley-tags = {bayes,conjugacy,gaussian},
number = {7},
title = {{Conjugate Bayesian analysis of the Gaussian distribution}},
volume = {0},
year = {2007}
}
